# ElBruno.AI.Evaluation

[![NuGet](https://img.shields.io/badge/NuGet-ElBruno.AI.Evaluation-blue?logo=nuget)](https://www.nuget.org/packages/ElBruno.AI.Evaluation)
[![.NET 8+](https://img.shields.io/badge/.NET-8.0-blue?logo=.net)](https://dotnet.microsoft.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Build Status](https://github.com/elbruno/elbruno-ai-evaluation/actions/workflows/ci.yml/badge.svg)](https://github.com/elbruno/elbruno-ai-evaluation/actions)

**AI Testing & Observability Toolkit for .NET** ‚Äî the xUnit for AI applications

> **.NET already has [Microsoft.Extensions.AI.Evaluation](https://learn.microsoft.com/en-us/dotnet/ai/evaluation/libraries)** ‚Äî a comprehensive, LLM-powered evaluation framework. **ElBruno.AI.Evaluation complements it** with offline deterministic evaluators, synthetic test data generation, golden dataset management, regression detection, and xUnit-native assertions ‚Äî scenarios the official libraries don't cover. [See the full comparison ‚Üí](docs/comparison-with-microsoft-evaluation.md)

## Why ElBruno.AI.Evaluation?

Microsoft's official evaluation libraries are excellent for LLM-powered quality analysis and Azure-based safety checks. But they require external LLM calls, Azure credentials, and don't provide test data generation or dataset lifecycle management. ElBruno.AI.Evaluation fills those gaps: **fast, offline, deterministic evaluators** that run in CI without any external dependencies, **synthetic data generation** to bootstrap your test suites, and **regression detection** to catch quality drops before they ship. Use them together for the best of both worlds.

## Quick Start

### Install

```bash
dotnet add package ElBruno.AI.Evaluation
dotnet add package ElBruno.AI.Evaluation.Xunit       # For xUnit tests
dotnet add package ElBruno.AI.Evaluation.Reporting   # For SQLite & exports
```

### 10-Line Example

```csharp
using ElBruno.AI.Evaluation.Datasets;
using ElBruno.AI.Evaluation.Evaluators;
using ElBruno.AI.Evaluation.Extensions;
using Microsoft.Extensions.AI;

// Create a golden dataset
var dataset = new GoldenDataset 
{
    Name = "customer-support",
    Examples = new()
    {
        new() { 
            Input = "How do I reset my password?",
            ExpectedOutput = "Visit Settings > Security > Reset Password"
        }
    }
};

// Configure evaluators
var evaluators = new IEvaluator[] 
{
    new RelevanceEvaluator(),      // 0.6+ similarity to expected output
    new HallucinationEvaluator(),  // No hallucinated facts
    new SafetyEvaluator()          // No PII or harmful content
};

// Run evaluation pipeline against your AI model
var results = await chatClient.EvaluateAsync(dataset, evaluators);

Console.WriteLine($"Pass Rate: {results.PassRate:P0}");
Console.WriteLine($"Aggregate Score: {results.AggregateScore:F2}");
```

## Packages

| Package | Description | Status |
|---------|-------------|--------|
| **ElBruno.AI.Evaluation** | Core evaluators, metrics, golden datasets, and extension methods for `IChatClient` | ‚úÖ Available |
| **ElBruno.AI.Evaluation.Xunit** | xUnit attribute-based testing integration (`AIEvaluationTest`, `AIAssert`) | ‚úÖ Available |
| **ElBruno.AI.Evaluation.Reporting** | SQLite result persistence, baseline snapshots, and export formats (JSON, CSV, Console) | ‚úÖ Available |
| **ElBruno.AI.Evaluation.SyntheticData** | Synthetic test data generation (template-based and LLM-powered) for evaluation datasets | ‚úÖ Available |

## Features

‚ú® **5 Built-in Evaluators**
- **RelevanceEvaluator** ‚Äî Cosine similarity + term overlap; checks if output matches expected content
- **FactualityEvaluator** ‚Äî Claim-level verification against reference text  
- **CoherenceEvaluator** ‚Äî Sentence completeness, contradiction detection, repetition penalties
- **HallucinationEvaluator** ‚Äî Token grounding and fact hallucination detection
- **SafetyEvaluator** ‚Äî PII masking, profanity detection, harmful content blocklists

üéØ **Golden Datasets**
- Fluent API for building test data programmatically or from JSON/CSV
- Tag-based filtering and subset selection
- Semantic versioning for regression tracking
- Metadata and context support for RAG scenarios

üß¨ **Synthetic Data Generation**
- Template-based (deterministic) generation for fast, reproducible test data
- LLM-powered generation via IChatClient for diverse edge cases and adversarial examples
- Composite generation combining deterministic + LLM for cost-effective diversity
- Built-in templates: Q&A, RAG, Adversarial, Domain-specific
- Validation and deduplication utilities for data quality

üîÑ **Regression Testing**
- Baseline snapshots for tracking metric trends
- Automatic regression detection with configurable tolerance
- Integration with CI/CD pipelines (GitHub Actions, Azure Pipelines)

üóÑÔ∏è **Production-Ready Persistence**
- SQLite storage for evaluation runs with queryable metrics
- Export to JSON, CSV, or console
- Cost and token tracking for monitoring LLM expenses
- Batch operations for large-scale evaluations

üß™ **xUnit Integration**
- `AIEvaluationTest` attribute for data-driven tests
- `AIAssert` fluent assertions (`PassesThreshold`, `AllMetricsPass`)
- Native Test Explorer support in Visual Studio
- Dataset-driven test generation

## Documentation

Comprehensive guides for all use cases:

- **[Quick Start](docs/quickstart.md)** ‚Äî Get up and running in 5 minutes
- **[Evaluation Metrics](docs/evaluation-metrics.md)** ‚Äî Deep dive into each evaluator with code examples and threshold guidance
- **[Golden Datasets](docs/golden-datasets.md)** ‚Äî Dataset design, JSON schema, CSV import, versioning, and best practices
- **[Synthetic Data Generation](docs/synthetic-data.md)** ‚Äî Template-based and LLM-powered test data generation for evaluation
- **[Best Practices](docs/best-practices.md)** ‚Äî Production patterns, threshold tuning, CI/CD integration, and monitoring
- **[Publishing to NuGet](docs/publishing.md)** ‚Äî How to publish new versions using GitHub Actions + Trusted Publishing

## Blog Series

A complete developer journey through AI testing in .NET, covering both ElBruno.AI.Evaluation and Microsoft.Extensions.AI.Evaluation:

1. **[Testing AI in .NET: The Landscape](blog/01-introducing-elbruno-ai-evaluation.md)** ‚Äî Understanding both official and complementary toolkits
2. **[Building Your Test Foundation: Golden Datasets & Synthetic Data](blog/02-golden-datasets-for-ai-testing.md)** ‚Äî Creating and managing test data with versioning
3. **[Evaluators: From Quick Checks to Deep Analysis](blog/03-ai-evaluators-deep-dive.md)** ‚Äî Layering deterministic and LLM-powered evaluation
4. **[AI Testing in Your CI Pipeline](blog/04-ai-testing-with-xunit.md)** ‚Äî xUnit integration and automation for quality gates
5. **[Production AI Evaluation: Combining Both Toolkits](blog/05-from-demo-to-production.md)** ‚Äî End-to-end hybrid pipeline with monitoring and cost tracking
6. **[Generating Synthetic Test Data for AI Evaluation](blog/06-synthetic-data-generation.md)** ‚Äî Deep dive into template-based, LLM-powered, and composite data generation
7. **[A Guide to Choosing the Right Evaluators for Your AI App](blog/07-choosing-the-right-evaluators.md)** ‚Äî Evaluator selection by scenario (chatbots, RAG, agents, etc.)

## Samples

Real-world examples showing the toolkit in action:

- **[ChatbotEvaluation](samples/ChatbotEvaluation/)** ‚Äî Customer support chatbot evaluation with multi-evaluator testing
- **[RagEvaluation](samples/RagEvaluation/)** ‚Äî RAG (Retrieval-Augmented Generation) system evaluation with context-aware metrics
- **[SyntheticDataGeneration](samples/SyntheticDataGeneration/)** ‚Äî Generate Q&A, RAG, and adversarial datasets with round-trip JSON persistence
- **[EvaluationJourney](samples/EvaluationJourney/)** ‚Äî Complete pipeline from synthetic data ‚Üí evaluators ‚Üí regression detection ‚Üí JSON export
- **[HybridEvaluation](samples/HybridEvaluation/)** ‚Äî Fast deterministic first pass with guidance on plugging in Microsoft's LLM-based evaluators

## Roadmap

### v1.0 (Current)
- ‚úÖ 5 evaluators (Relevance, Factuality, Coherence, Hallucination, Safety)
- ‚úÖ Golden datasets with JSON/CSV support
- ‚úÖ xUnit integration
- ‚úÖ SQLite persistence and exports
- ‚úÖ Synthetic data generation (template-based and LLM-powered)

### v1.5 (Planned)
- GitHub Copilot integration for copilot-assisted test generation
- Enhanced evaluator pipeline with streaming support
- Custom evaluator marketplace

### v2.0 (Future)
- Visual debugging dashboard with metric drill-down
- Real-time monitoring and alerting
- Distributed evaluation for large-scale datasets
- Model-specific evaluator profiles (GPT, Claude, Llama, etc.)

## Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines on:
- Submitting bug reports and feature requests
- Creating pull requests
- Writing evaluator plugins
- Adding new dataset formats

## License

MIT License ‚Äî see [LICENSE](LICENSE) for details.

---

**Author:** Bruno Capuano  
**Repository:** [elbruno/elbruno-ai-evaluation](https://github.com/elbruno/elbruno-ai-evaluation)  
**Discussions:** [GitHub Discussions](https://github.com/elbruno/elbruno-ai-evaluation/discussions)
