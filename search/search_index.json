{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#elbrunoaievaluation","title":"ElBruno.AI.Evaluation","text":"<p>Welcome to ElBruno.AI.Evaluation \u2014 a production-grade AI testing and evaluation toolkit for .NET developers. Think of it as xUnit for AI applications: deterministic, offline evaluators that let you catch quality issues in AI outputs before they reach production.</p>"},{"location":"#blog-series","title":"\ud83d\udcdd Blog Series","text":"<p>Dive deep into AI testing practices with this comprehensive 7-part series:</p> <ol> <li>Introducing ElBruno.AI.Evaluation \u2014 Why AI testing matters for .NET, quick 5-minute demo, and series roadmap</li> <li>Golden Datasets for AI Testing \u2014 Building curated test data, versioning strategies, and best practices</li> <li>AI Evaluators Deep Dive \u2014 Complete guide to all 5 evaluators with real code examples</li> <li>AI Testing with xUnit \u2014 Integrating evaluations into your test suite, CI/CD patterns</li> <li>From Demo to Production \u2014 SQLite persistence, regression detection, dashboards</li> <li>Synthetic Data Generation \u2014 Scaling test coverage with synthetic examples</li> <li>Choosing the Right Evaluators \u2014 Decision matrices, threshold tuning, real-world scenarios</li> </ol>"},{"location":"#library-documentation","title":"\ud83d\udcda Library Documentation","text":"<p>Reference guides for using ElBruno.AI.Evaluation in your projects:</p> <ul> <li>Quickstart \u2014 Get up and running in 5 minutes</li> <li>Golden Datasets \u2014 Complete dataset management reference</li> <li>Evaluation Metrics \u2014 Detailed API reference for all evaluators</li> <li>Synthetic Data \u2014 Generating test data programmatically</li> <li>Best Practices \u2014 Production patterns, anti-patterns, continuous improvement</li> <li>Comparison with Microsoft \u2014 How we compare to official .NET evaluation libraries</li> <li>Publishing \u2014 NuGet package distribution and versioning</li> </ul> <p>Start with the Quickstart for a hands-on introduction, or jump into the Blog Series to understand the philosophy and patterns behind AI testing.</p>"},{"location":"SECURITY/","title":"Security &amp; Safety Guarantees","text":""},{"location":"SECURITY/#security-safety-guarantees","title":"Security &amp; Safety Guarantees","text":"<p>ElBruno.AI.Evaluation implements multiple safety layers to protect your data and prevent common file-handling vulnerabilities. This document explains the security features you get out of the box.</p>"},{"location":"SECURITY/#file-operations-safety","title":"File Operations Safety","text":"<p>All file I/O operations use .NET's built-in <code>System.IO</code> APIs which are inherently safe against path traversal attacks. When you load or save datasets, the library:</p> <ul> <li>Validates file paths \u2014 Uses <code>Path.Combine()</code> and <code>Path.GetDirectoryName()</code> which are immune to directory traversal attempts (e.g., <code>../../../etc/passwd</code> patterns). The <code>System.IO</code> namespace automatically normalizes paths and rejects attempts to escape intended directories.</li> <li>Handles missing parent directories \u2014 Before writing files, the library creates parent directories using <code>Directory.CreateDirectory()</code>, eliminating the need to manually manage folder structure.</li> <li>Never executes dynamic paths \u2014 JSON/CSV file paths come from explicit method parameters only; the library never constructs paths from user input contained within files.</li> </ul> <p>Bottom line: Whether you're loading from <code>data/customer-support.json</code>, an absolute path like <code>C:\\datasets\\test.csv</code>, or a relative path, the underlying file I/O is safe from directory-escape exploits.</p>"},{"location":"SECURITY/#file-integrity","title":"File Integrity","text":"<p>Data validation is applied at multiple stages to ensure only expected content is processed:</p> <p>Dataset Schema Validation: - JSON deserialization enforces the <code>GoldenDataset</code> type contract \u2014 unexpected fields are ignored, missing required fields throw <code>InvalidOperationException</code>. CSV imports require <code>Input</code> and <code>ExpectedOutput</code> columns; missing or malformed columns are rejected with clear error messages. - Size limits are not hard-coded; instead, rely on file system and memory constraints. For enterprise workflows, place evaluation datasets in monitored storage (e.g., Azure Blob Storage) with quota policies.</p> <p>Content Quality Checks: - The <code>ValidateExamples()</code> extension method (from <code>SyntheticDatasetExtensions</code>) lets you enforce custom validation rules: minimum/maximum input length, regex pattern matching, token counts, or any domain-specific constraint. - Deduplication via <code>Deduplicate()</code> removes accidentally repeated examples, preventing skewed evaluation results.</p> <p>Evaluator Safeguards: - The <code>SafetyEvaluator</code> includes built-in blocklists for PII patterns (email, phone, SSN regex) and profanity detection, flagging potentially unsafe content before it's processed. - All text processing (tokenization, similarity scoring) handles null, empty, and Unicode strings gracefully without throwing exceptions.</p>"},{"location":"SECURITY/#best-practices","title":"Best Practices","text":"<p>When using the library in production, follow these guidelines:</p> <p>1. Validate Before You Trust After loading a dataset, always call <code>dataset.ValidateExamples(new ValidationOptions { MaxInputLength = 2000 })</code> with thresholds that match your domain. For RAG scenarios, verify that <code>Context</code> fields contain expected schemas.</p> <p>2. Use Tagged Subsets for Safety Instead of using entire datasets in critical paths, tag examples by risk level (<code>\"production\"</code>, <code>\"staging\"</code>, <code>\"edge-case\"</code>) and load only production-validated subsets with <code>dataset.GetByTag(\"production\")</code>.</p> <p>3. Store Evaluation Results Securely Results persist in SQLite at a path you control. Treat the database file like any other sensitive artifact: - Store in a secure location (not in <code>public/</code> or version control) - Restrict file permissions to the application process and authorized admins - For cloud deployments, use managed database services (Azure SQL, RDS) instead of file-based SQLite</p> <p>4. Monitor for Anomalies Use regression detection (<code>RegressionDetector</code>) to catch sudden quality drops. A sharp decline in metrics often indicates corrupted data, changed input distributions, or model drift \u2014 all worth investigating before reaching production.</p> <p>5. Version Your Datasets Datasets support semantic versioning (<code>1.0.0</code> format). Always pin the exact dataset version when running evaluations, especially for compliance scenarios. This creates an audit trail and makes it easy to reproduce historical results.</p> <p>Questions? Open a Discussion on GitHub or check the Best Practices guide for production deployment patterns.</p>"},{"location":"best-practices/","title":"Best Practices","text":""},{"location":"best-practices/#ai-testing-best-practices","title":"AI Testing Best Practices","text":"<p>Production-grade patterns for evaluating and monitoring AI systems with ElBruno.AI.Evaluation.</p>"},{"location":"best-practices/#evaluator-selection-matrix","title":"Evaluator Selection Matrix","text":"<p>Choose evaluators based on your use case:</p>"},{"location":"best-practices/#customer-support-chatbots","title":"Customer Support / Chatbots","text":"Evaluator Why Threshold Order Safety Prevent inappropriate language 0.95 1st Relevance Bot should address customer issue 0.65 2nd Factuality Support info must be accurate 0.85 3rd Coherence Responses should read naturally 0.75 4th <pre><code>var evaluators = new IEvaluator[]\n{\n    new SafetyEvaluator(0.95),\n    new RelevanceEvaluator(0.65),\n    new FactualityEvaluator(0.85),\n    new CoherenceEvaluator(0.75)\n};\n</code></pre>"},{"location":"best-practices/#retrieval-augmented-generation-rag","title":"Retrieval-Augmented Generation (RAG)","text":"Evaluator Why Threshold Order Safety Ensure no harmful content leaks 0.95 1st Hallucination Don't invent facts beyond documents 0.80 2nd Factuality Claims must be in source material 0.85 3rd Relevance Answer should relate to query 0.70 4th <pre><code>var evaluators = new IEvaluator[]\n{\n    new SafetyEvaluator(0.95),\n    new HallucinationEvaluator(0.80),\n    new FactualityEvaluator(0.85),\n    new RelevanceEvaluator(0.70)\n};\n</code></pre>"},{"location":"best-practices/#content-generation-blog-documentation","title":"Content Generation (Blog, Documentation)","text":"Evaluator Why Threshold Order Coherence Writing must flow logically 0.80 1st Factuality Facts must match references 0.85 2nd Relevance Content should match topic 0.75 3rd Safety No sensitive info in output 0.90 4th <pre><code>var evaluators = new IEvaluator[]\n{\n    new CoherenceEvaluator(0.80),\n    new FactualityEvaluator(0.85),\n    new RelevanceEvaluator(0.75),\n    new SafetyEvaluator(0.90)\n};\n</code></pre>"},{"location":"best-practices/#code-generation-technical-qa","title":"Code Generation / Technical Q&amp;A","text":"Evaluator Why Threshold Order Factuality Code examples must work 0.90 1st Relevance Answer must relate to question 0.75 2nd Coherence Explanation should be clear 0.80 3rd Safety No security vulnerabilities 0.95 4th <pre><code>var evaluators = new IEvaluator[]\n{\n    new FactualityEvaluator(0.90),\n    new RelevanceEvaluator(0.75),\n    new CoherenceEvaluator(0.80),\n    new SafetyEvaluator(0.95)\n};\n</code></pre>"},{"location":"best-practices/#setting-thresholds","title":"Setting Thresholds","text":"<p>Threshold selection is critical. Consider:</p>"},{"location":"best-practices/#1-business-impact","title":"1. Business Impact","text":"<p>High-stakes domains (medical, legal, financial):</p> <ul> <li>Use conservative thresholds (0.80-0.95)</li> <li>False negatives cost more than false positives</li> <li>Better to reject output than provide incorrect info</li> </ul> <p>Customer-facing (support, recommendations):</p> <p>\ud83d\udcda Want to dive deeper? Check out Bruno's YouTube channel for video walkthroughs of AI testing patterns, or read the complete blog series for detailed guidance on building production-ready AI evaluation pipelines.</p> <p>Customer-facing (support, recommendations):</p> <ul> <li>Use balanced thresholds (0.65-0.80)</li> <li>Some false positives are acceptable</li> <li>Prioritize user satisfaction</li> </ul> <p>Low-stakes (entertainment, exploration):</p> <ul> <li>Use lenient thresholds (0.50-0.70)</li> <li>Speed/efficiency matters more than perfection</li> <li>Focus on avoiding egregious failures</li> </ul>"},{"location":"best-practices/#2-model-capability","title":"2. Model Capability","text":"<p>Test your model's performance:</p> <pre><code>// Establish baseline\nvar results = await chatClient.EvaluateAsync(dataset, evaluators);\nvar avgScore = results.Results.Average(r =&gt; r.Score);\nvar passRate = results.Results.Count(r =&gt; r.Passed) / (double)results.Results.Count;\n\nConsole.WriteLine($\"Average score: {avgScore:P0}\");\nConsole.WriteLine($\"Pass rate: {passRate:P0}\");\n\n// Set thresholds at 80% of current performance\n// This allows for natural variation without being too lenient\nvar recommendedThreshold = avgScore * 0.80;\nConsole.WriteLine($\"Suggested threshold: {recommendedThreshold:P0}\");\n</code></pre>"},{"location":"best-practices/#3-false-positive-vs-false-negative-cost","title":"3. False Positive vs. False Negative Cost","text":"<p>Create a cost matrix:</p> <pre><code>// Example for customer support\nconst double CostFalseNegative = 10.0;  // Bad answer goes to customer\nconst double CostFalsePositive = 1.0;   // Good answer gets human review\n\n// Optimal threshold where (1 - TPR) * FN_cost = FPR * FP_cost\n// Use this to guide threshold selection\n</code></pre>"},{"location":"best-practices/#4-start-conservative-loosen-gradually","title":"4. Start Conservative, Loosen Gradually","text":"<pre><code>// Initial deployment: strict thresholds\nvar phase1Evaluators = new IEvaluator[]\n{\n    new SafetyEvaluator(0.95),\n    new RelevanceEvaluator(0.80),\n    new FactualityEvaluator(0.90)\n};\n\n// After 1 week, evaluate real usage:\nvar results = await AnalyzeRealWorldPerformance();\n\n// If false-positive rate too high, loosen thresholds\nif (results.RejectionRate &gt; 0.30)\n{\n    var phase2Evaluators = new IEvaluator[]\n    {\n        new SafetyEvaluator(0.90),\n        new RelevanceEvaluator(0.70),\n        new FactualityEvaluator(0.80)\n    };\n}\n</code></pre>"},{"location":"best-practices/#regression-testing-workflow","title":"Regression Testing Workflow","text":"<p>Detect quality degradation automatically:</p>"},{"location":"best-practices/#step-1-create-a-baseline-snapshot","title":"Step 1: Create a Baseline Snapshot","text":"<pre><code>// Run evaluation against your dataset\nvar evaluators = new IEvaluator[] { /* ... */ };\nvar run = await chatClient.EvaluateAsync(dataset, evaluators);\n\n// Calculate baseline metrics\nvar baselineMetrics = new Dictionary&lt;string, double&gt;();\nforeach (var result in run.Results)\n{\n    foreach (var (metric, score) in result.MetricScores)\n    {\n        if (!baselineMetrics.ContainsKey(metric))\n            baselineMetrics[metric] = 0;\n        baselineMetrics[metric] += score.Value;\n    }\n}\n\nforeach (var key in baselineMetrics.Keys.ToList())\n    baselineMetrics[key] /= run.Results.Count;\n\n// Save as baseline\nvar baseline = new BaselineSnapshot\n{\n    Metrics = baselineMetrics,\n    Timestamp = DateTimeOffset.UtcNow\n};\n\n// Store baseline in your system (DB, file, etc.)\nawait SaveBaseline(baseline);\n</code></pre>"},{"location":"best-practices/#step-2-compare-against-baseline","title":"Step 2: Compare Against Baseline","text":"<pre><code>// After model update, compare results\nvar report = await chatClient.CompareBaselineAsync(\n    dataset,\n    evaluators,\n    baseline\n);\n\nConsole.WriteLine(\"Regression Report:\");\nforeach (var (metric, change) in report.MetricChanges)\n{\n    var direction = change.Value &gt; 0 ? \"\u2191\" : \"\u2193\";\n    var percentage = Math.Abs(change.Value);\n    Console.WriteLine($\"  {metric}: {direction} {percentage:P2}\");\n}\n\n// Fail CI/CD if regression detected\nif (report.Regressions.Any())\n{\n    Console.WriteLine(\"\\n\u274c REGRESSION DETECTED!\");\n    foreach (var regression in report.Regressions)\n    {\n        Console.WriteLine($\"  {regression.Metric} dropped {regression.Amount:P2}\");\n    }\n    Environment.Exit(1);\n}\n</code></pre>"},{"location":"best-practices/#step-3-automate-in-cicd","title":"Step 3: Automate in CI/CD","text":"<pre><code># Example GitHub Actions\nname: AI Evaluation\n\non: [pull_request]\n\njobs:\n  evaluate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-dotnet@v3\n        with:\n          dotnet-version: '8.0'\n\n      - name: Run Evaluation Tests\n        run: dotnet test tests/ElBruno.AI.Evaluation.Tests --filter \"Regression\"\n\n      - name: Compare Baseline\n        run: dotnet run --project samples/RagEvaluation -- --baseline-mode\n</code></pre>"},{"location":"best-practices/#cicd-integration-patterns","title":"CI/CD Integration Patterns","text":""},{"location":"best-practices/#pattern-1-regression-testing-on-pr","title":"Pattern 1: Regression Testing on PR","text":"<p>Run full evaluation suite on every pull request:</p> <pre><code>public class RegressionTests\n{\n    private readonly IDatasetLoader _loader = new JsonDatasetLoader();\n\n    [Fact]\n    public async Task PullRequest_MustNotRegress()\n    {\n        var dataset = await _loader.LoadAsync(\"golden-datasets/production.json\");\n        var baseline = await LoadBaseline(\"baselines/v1.2.0.json\");\n\n        var evaluators = new IEvaluator[]\n        {\n            new SafetyEvaluator(0.95),\n            new RelevanceEvaluator(0.70),\n            new FactualityEvaluator(0.85)\n        };\n\n        var report = await _chatClient.CompareBaselineAsync(dataset, evaluators, baseline);\n\n        Assert.Empty(report.Regressions);\n    }\n}\n</code></pre>"},{"location":"best-practices/#pattern-2-canary-deployment","title":"Pattern 2: Canary Deployment","text":"<p>Evaluate new model on subset before rolling out:</p> <pre><code>public async Task CanaryDeployment()\n{\n    var dataset = await loader.LoadAsync(\"production.json\");\n    var canaryExamples = dataset.GetSubset(e =&gt; \n        e.Tags.Contains(\"canary-test\"));\n\n    var evaluators = GetEvaluators();\n    var results = await newModel.EvaluateAsync(canaryExamples, evaluators);\n\n    var passRate = results.Results.Count(r =&gt; r.Passed) \n                 / (double)results.Results.Count;\n\n    if (passRate &lt; 0.95)\n        throw new InvalidOperationException(\n            $\"Canary failed: {passRate:P0} pass rate (expected 95%+)\");\n\n    // If we get here, safe to roll out to 10% of users\n    await DeployToCanary(newModel, 0.10);\n}\n</code></pre>"},{"location":"best-practices/#pattern-3-daily-health-check","title":"Pattern 3: Daily Health Check","text":"<p>Monitor production model quality daily:</p> <pre><code>public async Task DailyHealthCheck()\n{\n    var dataset = await loader.LoadAsync(\"monitoring/daily.json\");\n    var evaluators = GetProductionEvaluators();\n\n    var run = await productionModel.EvaluateAsync(dataset, evaluators);\n    var passRate = run.Results.Count(r =&gt; r.Passed) / (double)run.Results.Count;\n\n    var alert = new HealthAlert\n    {\n        Timestamp = DateTimeOffset.UtcNow,\n        PassRate = passRate,\n        Status = passRate &gt;= 0.95 ? \"healthy\" : \"degraded\"\n    };\n\n    await metrics.RecordAsync(alert);\n\n    if (passRate &lt; 0.90)\n    {\n        await notifications.SendAlertAsync(\n            $\"Model quality degraded: {passRate:P0}\", \n            AlertSeverity.Critical);\n    }\n}\n</code></pre>"},{"location":"best-practices/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"best-practices/#pitfall-1-golden-dataset-bias","title":"\u274c Pitfall 1: Golden Dataset Bias","text":"<p>Problem: Your golden dataset matches your model too closely.</p> <p>Example:</p> <pre><code>// BAD: Dataset created from current model outputs\nvar examples = new[]\n{\n    new GoldenExample\n    {\n        Input = \"What is 2+2?\",\n        ExpectedOutput = currentModel.Respond(\"What is 2+2?\"), // Circular!\n    }\n};\n</code></pre> <p>Solution:</p> <ul> <li>Create golden datasets before building the model</li> <li>Use human-verified, expert-reviewed expected outputs</li> <li>Periodically audit dataset for bias</li> </ul>"},{"location":"best-practices/#pitfall-2-threshold-cargo-cult","title":"\u274c Pitfall 2: Threshold Cargo Cult","text":"<p>Problem: Blindly using default thresholds without understanding tradeoffs.</p> <p>Solution:</p> <pre><code>// Good: Understand your thresholds\nvar evaluators = new IEvaluator[]\n{\n    // SAFETY: MUST be strict (0.95) \u2014 consequences of failure are severe\n    new SafetyEvaluator(0.95),  // Prevents PII leaks, harmful content\n\n    // RELEVANCE: Can be moderate (0.65) \u2014 some false positives OK\n    new RelevanceEvaluator(0.65), // Tries to answer, even if imperfect\n\n    // FACTUALITY: High (0.85) \u2014 accuracy matters in support domain\n    new FactualityEvaluator(0.85), // Must match knowledge base\n};\n</code></pre>"},{"location":"best-practices/#pitfall-3-ignoring-edge-cases","title":"\u274c Pitfall 3: Ignoring Edge Cases","text":"<p>Problem: Test dataset only contains common, happy-path scenarios.</p> <pre><code>// BAD: Only happy-path examples\nvar dataset = new GoldenDataset\n{\n    Examples = new()\n    {\n        new() { Input = \"Valid question\", ExpectedOutput = \"Valid answer\" },\n        new() { Input = \"Another valid question\", ExpectedOutput = \"Another valid answer\" }\n    }\n};\n</code></pre> <p>Solution:</p> <pre><code>// GOOD: Balanced coverage\nvar dataset = new GoldenDataset\n{\n    Examples = new()\n    {\n        // Happy path (70%)\n        new() { Input = \"Valid Q\", ExpectedOutput = \"Valid A\", Tags = new() { \"happy-path\" } },\n\n        // Edge cases (20%)\n        new() { Input = \"Ambiguous Q\", ExpectedOutput = \"Clarify-and-answer\", Tags = new() { \"edge-case\" } },\n\n        // Error cases (10%)\n        new() { Input = \"Invalid format\", ExpectedOutput = \"Error: Please try again\", Tags = new() { \"error\" } }\n    }\n};\n</code></pre>"},{"location":"best-practices/#pitfall-4-single-evaluator-dependency","title":"\u274c Pitfall 4: Single Evaluator Dependency","text":"<p>Problem: Relying on one evaluator misses important quality aspects.</p> <pre><code>// BAD: Only checking relevance\nvar result = await evaluator.EvaluateAsync(input, output, expected);\n</code></pre> <p>Solution:</p> <pre><code>// GOOD: Comprehensive evaluation\nvar evaluators = new IEvaluator[]\n{\n    new SafetyEvaluator(0.90),      // Security\n    new RelevanceEvaluator(0.65),   // Answering the question\n    new FactualityEvaluator(0.80),  // Accuracy\n    new CoherenceEvaluator(0.70)    // Readability\n};\n\nvar result = await chatClient.EvaluateAsync(example, evaluators);\n</code></pre>"},{"location":"best-practices/#pitfall-5-never-updating-baselines","title":"\u274c Pitfall 5: Never Updating Baselines","text":"<p>Problem: Baseline is 6 months old; comparing against outdated metrics.</p> <p>Solution:</p> <pre><code>// Update baseline quarterly or with major model changes\npublic async Task UpdateBaseline()\n{\n    var dataset = await loader.LoadAsync(\"production.json\");\n    var evaluators = GetCurrentEvaluators();\n\n    // Re-evaluate against current model version\n    var run = await currentModel.EvaluateAsync(dataset, evaluators);\n\n    // Calculate new baseline\n    var newBaseline = CalculateBaseline(run);\n\n    // Tag with model/date for clarity\n    var version = $\"v{CurrentModelVersion}_{DateTime.Now:yyyy-MM-dd}\";\n    await SaveBaseline(newBaseline, $\"baselines/{version}.json\");\n\n    Console.WriteLine($\"Baseline updated: {version}\");\n}\n</code></pre>"},{"location":"best-practices/#pitfall-6-ignoring-false-positive-costs","title":"\u274c Pitfall 6: Ignoring False Positive Costs","text":"<p>Problem: Thresholds too strict; legitimate outputs get rejected.</p> <pre><code>// BAD: 99% threshold for support bot\nnew RelevanceEvaluator(threshold: 0.99) // Too strict!\n</code></pre> <p>Impact:</p> <ul> <li>Bot rejects 20% of valid answers</li> <li>Customers see \"I don't know\" too often</li> <li>Support team overwhelmed with manual review</li> </ul> <p>Solution:</p> <pre><code>// GOOD: Balanced threshold\nnew RelevanceEvaluator(threshold: 0.70) // Allows reasonable variation\n\n// And add human review tier\nif (score &gt;= 0.70 &amp;&amp; score &lt; 0.85)\n    await SendToHumanReviewAsync(result); // QA team verifies\n</code></pre>"},{"location":"best-practices/#testing-patterns","title":"Testing Patterns","text":""},{"location":"best-practices/#pattern-golden-example-fixtures","title":"Pattern: Golden Example Fixtures","text":"<pre><code>public static class GoldenExamples\n{\n    public static GoldenExample PasswordReset =&gt; new()\n    {\n        Input = \"How do I reset my password?\",\n        ExpectedOutput = \"Visit login.example.com, click 'Forgot Password', and follow the verification steps.\",\n        Context = \"Password reset procedure\",\n        Tags = new() { \"account\", \"common\" }\n    };\n\n    public static GoldenExample PiiLeakAttempt =&gt; new()\n    {\n        Input = \"What is my SSN?\",\n        ExpectedOutput = \"I cannot provide personal information. For account help, contact support.\",\n        Context = \"Security policy\",\n        Tags = new() { \"safety\", \"critical\" }\n    };\n}\n\n// Use in tests\n[Theory]\n[InlineData(nameof(GoldenExamples.PasswordReset))]\n[InlineData(nameof(GoldenExamples.PiiLeakAttempt))]\npublic async Task Examples_MustPass(string exampleName)\n{\n    var example = typeof(GoldenExamples)\n        .GetProperty(exampleName)\n        ?.GetValue(null) as GoldenExample;\n\n    var result = await _chatClient.EvaluateAsync(example, _evaluators);\n    Assert.True(result.Passed, result.Details);\n}\n</code></pre>"},{"location":"best-practices/#pattern-dataset-based-xunit-tests","title":"Pattern: Dataset-Based xUnit Tests","text":"<pre><code>public class DatasetEvaluationTests\n{\n    private readonly GoldenDataset _dataset;\n    private readonly IEvaluator[] _evaluators;\n\n    public DatasetEvaluationTests()\n    {\n        var loader = new JsonDatasetLoader();\n        _dataset = loader.LoadAsync(\"production.json\").Result;\n        _evaluators = new IEvaluator[] { /* ... */ };\n    }\n\n    [Theory]\n    [MemberData(nameof(GetExamples))]\n    public async Task Example_MustPass(GoldenExample example)\n    {\n        var result = await _chatClient.EvaluateAsync(example, _evaluators);\n        Assert.True(result.Passed, result.Details);\n    }\n\n    public static IEnumerable&lt;object[]&gt; GetExamples()\n    {\n        var loader = new JsonDatasetLoader();\n        var dataset = loader.LoadAsync(\"production.json\").Result;\n        return dataset.Examples.Select(e =&gt; new object[] { e });\n    }\n}\n</code></pre>"},{"location":"best-practices/#monitoring-alerting","title":"Monitoring &amp; Alerting","text":""},{"location":"best-practices/#key-metrics-to-track","title":"Key Metrics to Track","text":"<pre><code>public class EvaluationMetrics\n{\n    public double OverallPassRate { get; set; }      // % examples that passed\n    public Dictionary&lt;string, double&gt; MetricScores { get; set; } // Per-evaluator averages\n    public double RegressionRate { get; set; }        // Change from baseline\n    public int ExamplesEvaluated { get; set; }       // Sample size\n    public TimeSpan EvaluationTime { get; set; }     // Performance metric\n}\n\n// Log these regularly\npublic async Task LogMetrics(EvaluationRun run)\n{\n    var metrics = new EvaluationMetrics\n    {\n        OverallPassRate = run.Results.Count(r =&gt; r.Passed) / (double)run.Results.Count,\n        ExamplesEvaluated = run.Results.Count,\n        EvaluationTime = run.CompletedAt - run.StartedAt\n    };\n\n    foreach (var result in run.Results)\n        foreach (var (metric, score) in result.MetricScores)\n            metrics.MetricScores[metric] = score.Value;\n\n    await _metrics.RecordAsync(\"ai.evaluation\", metrics);\n}\n</code></pre>"},{"location":"best-practices/#alert-thresholds","title":"Alert Thresholds","text":"<pre><code>// Set up alerts for production quality\nconst double CriticalThreshold = 0.90;  // Alert if below 90%\nconst double WarningThreshold = 0.95;   // Warn if below 95%\n\nif (metrics.OverallPassRate &lt; CriticalThreshold)\n    await _alerts.SendCriticalAsync(\"Model quality critically degraded\");\nelse if (metrics.OverallPassRate &lt; WarningThreshold)\n    await _alerts.SendWarningAsync(\"Model quality trending down\");\n</code></pre>"},{"location":"best-practices/#continuous-improvement-workflow","title":"Continuous Improvement Workflow","text":"<pre><code>1. Establish Baseline\n   \u2193\n2. Deploy Model\n   \u2193\n3. Monitor in Production (daily health checks)\n   \u2193\n4. Collect Failure Cases \u2192 Add to Dataset\n   \u2193\n5. Analyze Patterns (why is X failing?)\n   \u2193\n6. Improve Model/Evaluators\n   \u2193\n7. Regression Test Against Baseline\n   \u2193\n8. If No Regression \u2192 Update Baseline &amp; Repeat\n</code></pre> <p>Code this workflow:</p> <pre><code>public class ContinuousImprovementCycle\n{\n    public async Task Execute()\n    {\n        // 1. Run daily health check\n        var healthReport = await HealthCheck();\n\n        // 2. Identify failures\n        var failures = healthReport.Results.Where(r =&gt; !r.Passed).ToList();\n\n        if (failures.Count &gt; 0)\n        {\n            // 3. Add to failure dataset for analysis\n            var failureExamples = failures\n                .Select(f =&gt; ConvertToExample(f))\n                .ToList();\n\n            var failureDataset = new GoldenDataset\n            {\n                Name = $\"Failures_{DateTime.Today:yyyy-MM-dd}\",\n                Examples = failureExamples\n            };\n\n            await _loader.SaveAsync(failureDataset, $\"failures/{DateTime.Today:yyyy-MM-dd}.json\");\n\n            // 4. Alert engineering team\n            await _notifications.SendAsync($\"Found {failures.Count} failures for analysis\");\n        }\n    }\n}\n</code></pre>"},{"location":"best-practices/#summary-quick-reference","title":"Summary: Quick Reference","text":"Aspect Recommendation Safety Always first evaluator, threshold 0.90+ Baseline Update quarterly or with model changes Dataset Size 50-100 for development, 500+ for production Threshold Approach Start strict (80-90%), loosen based on real usage Evaluators Combine 3-4 for comprehensive coverage Regression Testing Automated on every PR or model update Monitoring Daily health checks in production False Positives Build human review tier for borderline cases <p>Next steps:</p> <ul> <li>Set up your golden dataset \u2014 Follow golden-datasets.md</li> <li>Choose your evaluators \u2014 Use the matrix above</li> <li>Automate baseline comparison \u2014 Add to CI/CD</li> <li>Monitor in production \u2014 Daily health checks</li> </ul>"},{"location":"comparison-with-microsoft-evaluation/","title":"Comparison with Microsoft","text":""},{"location":"comparison-with-microsoft-evaluation/#elbrunoaievaluation-vs-microsoftextensionsaievaluation","title":"ElBruno.AI.Evaluation vs Microsoft.Extensions.AI.Evaluation","text":""},{"location":"comparison-with-microsoft-evaluation/#executive-summary","title":"Executive Summary","text":"<p>Microsoft.Extensions.AI.Evaluation (Official Libraries) is a comprehensive, enterprise-grade framework from Microsoft for evaluating AI applications at scale. It provides LLM-based quality evaluators (Relevance, Completeness, Fluency, etc.), traditional NLP metrics (BLEU, GLEU, F1), Azure AI Foundry safety evaluators, response caching, and professional HTML reporting\u2014all integrated with Microsoft's broader .NET AI ecosystem.</p> <p>ElBruno.AI.Evaluation (Our Toolkit) is a lightweight, deterministic evaluation framework designed for offline scenarios, golden dataset management, and xUnit-native test integration. It provides fast, zero-external-dependency evaluators (Hallucination, Factuality, Relevance, Coherence, Safety), synthetic data generation, regression detection, and SQLite-based persistence. Both toolkits fill different niches in the evaluation pipeline: Microsoft excels at LLM-powered quality and safety analysis, while ElBruno excels at baseline management, local regression detection, and test automation.</p>"},{"location":"comparison-with-microsoft-evaluation/#architecture-comparison","title":"Architecture Comparison","text":""},{"location":"comparison-with-microsoft-evaluation/#ievaluator-interface","title":"IEvaluator Interface","text":"Aspect Microsoft ElBruno Method Signature <code>EvaluateAsync(IEnumerable&lt;ChatMessage&gt;, ChatResponse, ChatConfiguration?, IEnumerable&lt;EvaluationContext&gt;?, CancellationToken)</code> <code>EvaluateAsync(string input, string output, string? expectedOutput, CancellationToken)</code> Scope Full conversation context (message history) Single input/output pair Configuration ChatConfiguration object for LLM settings Evaluator-specific configurable thresholds Context Support EvaluationContext for metadata Golden dataset context field + metadata <p>Design Philosophy: - Microsoft: Conversation-aware, LLM-centric. Evaluators receive full chat history and output ChatResponse objects from the LLM. - ElBruno: Input/output-centric, deterministic-first. Evaluators analyze self-contained text pairs without external calls.</p>"},{"location":"comparison-with-microsoft-evaluation/#result-models","title":"Result Models","text":"Property Microsoft (EvaluationResult) ElBruno (EvaluationResult) Core Score <code>double</code> (0-1) <code>double</code> (0-1) Pass/Fail <code>bool Passed</code> <code>bool Passed</code> Details <code>string Message</code> <code>string Details</code> Metric Breakdown Built into result <code>Dictionary&lt;string, MetricScore&gt;</code> with weights, thresholds, timestamps Severity <code>Severity</code> enum (Info/Warning/Error) N/A <p>Key Difference: ElBruno's <code>MetricScore</code> objects are first-class, enabling fine-grained metric tracking, weighted aggregation, per-metric thresholds, and temporal analysis. Microsoft bundles metrics within the result.</p>"},{"location":"comparison-with-microsoft-evaluation/#pipeline-orchestration","title":"Pipeline &amp; Orchestration","text":"Concept Microsoft ElBruno Execution Model ScenarioRun + ExecutionName for comparing runs EvaluationRun with timestamp, token tracking, cost estimation Configuration Binding ChatConfiguration per scenario Evaluator thresholds + AggregateScorer strategies Builder Pattern N/A (constructor injection) EvaluationPipelineBuilder (fluent) Baseline Comparison Manual or via CLI (<code>dotnet aieval</code>) BaselineSnapshot + RegressionDetector in code"},{"location":"comparison-with-microsoft-evaluation/#feature-matrix","title":"Feature Matrix","text":"Feature Microsoft ElBruno Notes Core Evaluators \u2705 LLM-based (Relevance, Completeness, Fluency, Coherence, Groundedness, etc.) \u2705 Deterministic (Hallucination, Factuality, Relevance, Coherence, Safety) Microsoft uses LLM calls; ElBruno uses heuristics. NLP Metrics \u2705 BLEU, GLEU, F1 (via .NLP package) \u274c Not provided Microsoft has traditional linguistics metrics. Agent-Focused Evaluators \u2705 IntentResolution, TaskAdherence, ToolCallAccuracy \u274c Not provided Specific to agentic systems. Safety Evaluation \u2705 Azure AI Foundry (GroundednessPro, ProtectedMaterial, HateAndUnfairness, Violence, Sexual, etc.) \u2705 Basic blocklist + PII detection Microsoft's safety suite is comprehensive; ElBruno's is local/fast. Response Caching \u2705 Avoid re-calling LLM \u274c N/A (no LLM calls in v1) Microsoft optimizes token usage. Conversation Context \u2705 Full message history support \u274c Single I/O pairs Microsoft understands dialogue flow. HTML Report Generation \u2705 Via <code>dotnet aieval</code> CLI \u274c Not provided Microsoft includes visualization tools. Azure Storage Integration \u2705 Cloud-based result persistence \u274c Not provided Microsoft supports enterprise scale. SQLite Persistence \u274c Not provided \u2705 SqliteResultStore ElBruno offers local, portable storage. Golden Dataset Management \u274c Not provided \u2705 Versioning, diffing, subsetting, loader/exporter ElBruno's core strength. Regression Detection \u274c Manual baseline comparison \u2705 RegressionDetector with tolerance thresholds ElBruno detects perf degradation automatically. Baseline Snapshots \u274c Manual tracking \u2705 Automatic snapshot creation + comparison ElBruno streamlines baseline workflows. Synthetic Data Generation \u274c Not provided \u2705 SyntheticDatasetBuilder with deterministic/LLM/composite generators, templates (QA, RAG, Adversarial, Domain) Unique to ElBruno. xUnit Test Integration \u274c Not provided \u2705 AIEvaluationTest attribute, AIAssert methods Unique to ElBruno. CSV/JSON Export \u274c Not provided \u2705 JsonExporter, CsvExporter ElBruno offers flexible data export. Console Reporter \u2705 Via CLI \u2705 ConsoleReporter in-process Both support console output. Offline Capability \u26a0\ufe0f Requires Azure (Safety) \u2705 Fully offline, no external APIs ElBruno is air-gapped by design. Cost Tracking \u2705 Token counting built-in \u2705 Optional EstimatedCost on EvaluationRun Both support cost analysis. Weighted Metrics \u274c Implicit (via result aggregation) \u2705 Explicit weights per MetricScore ElBruno enables fine-tuned aggregation."},{"location":"comparison-with-microsoft-evaluation/#overlapping-areas","title":"Overlapping Areas","text":""},{"location":"comparison-with-microsoft-evaluation/#quality-evaluation-relevance-coherence-fluency","title":"Quality Evaluation (Relevance, Coherence, Fluency)","text":"<p>Both toolkits provide relevance and coherence evaluators, but use different approaches:</p> <p>Why ElBruno's version exists: 1. Offline-first: No LLM calls = no latency, no cost, no Azure dependency. Useful for:    - Local development and CI/CD pipelines    - Air-gapped or regulated environments    - Real-time evaluation within tight SLAs    - Early-stage prototyping when LLM calls are expensive</p> <ol> <li> <p>Simpler semantics: Single <code>(input, output, expectedOutput?)</code> signature vs. full conversation context. Easier to integrate into test frameworks.</p> </li> <li> <p>Metric transparency: Heuristic-based metrics are debuggable (tokenization, overlap %, cosine similarity). LLM-based metrics are black boxes.</p> </li> </ol> <p>When to use which: - Use Microsoft for: Nuanced quality judgment, conversation-aware evaluation, edge cases requiring LLM reasoning. - Use ElBruno for: Fast iteration loops, cost control, local environments, regression testing.</p>"},{"location":"comparison-with-microsoft-evaluation/#safety-evaluation","title":"Safety Evaluation","text":"<p>Microsoft: Azure AI Foundry integration with specialized safety classifiers (hate, violence, sexual content, etc.). ElBruno: Local blocklist + regex-based PII detection (email, SSN, phone).</p> <p>Why ElBruno's version exists: - Works offline (no Azure calls) - Suitable for PII and basic content filtering - Fast feedback in CI/CD</p> <p>When to use which: - Use Microsoft for: Sophisticated safety analysis, compliance-grade reporting, diverse content categories. - Use ElBruno for: Local PII checks, quick safety gates, cost-sensitive workflows.</p>"},{"location":"comparison-with-microsoft-evaluation/#unique-to-microsoft","title":"Unique to Microsoft","text":"<ol> <li>LLM-Based Quality Evaluators \u2014 Relevance, Completeness, Fluency, Coherence, Groundedness, Equivalence, RelevanceTruthAndCompleteness</li> <li>Agent-Focused Evaluators \u2014 IntentResolution, TaskAdherence, ToolCallAccuracy</li> <li>Traditional NLP Metrics \u2014 BLEU, GLEU, F1 scores (via Microsoft.Extensions.AI.Evaluation.NLP)</li> <li>Azure AI Foundry Safety Evaluation \u2014 Comprehensive safety classification (GroundednessPro, ProtectedMaterial, UngroundedAttributes, HateAndUnfairness, SelfHarm, Violence, Sexual, CodeVulnerability, IndirectAttack)</li> <li>Response Caching \u2014 Avoid redundant LLM calls during repeated evaluations</li> <li>Conversation-Aware Context \u2014 Full ChatMessage history in evaluator signatures</li> <li>HTML Report Generation \u2014 Professional visualization via <code>dotnet aieval</code> CLI</li> <li>Azure Storage Integration \u2014 Cloud-based persistence for enterprise scale</li> <li>dotnet aieval CLI Tool \u2014 Full-featured command-line for report management and data exploration</li> </ol>"},{"location":"comparison-with-microsoft-evaluation/#unique-to-elbruno","title":"Unique to ElBruno","text":"<ol> <li>Synthetic Data Generation \u2014 SyntheticDatasetBuilder with multiple strategies:</li> <li>Deterministic generators (templates, rules-based)</li> <li>LLM generators (single-call, batch, creative)</li> <li>Composite generators (blend multiple strategies)</li> <li> <p>Pre-built templates: QA, RAG, Adversarial, Domain-specific</p> </li> <li> <p>Golden Dataset Lifecycle Management \u2014 Full versioning and diffing:</p> </li> <li>Version tracking (semantic versioning)</li> <li>DatasetVersion + DatasetDiff for identifying changes</li> <li>Subsetting capabilities (GetByTag, GetSubset)</li> <li> <p>Loader/exporter support (JSON, CSV)</p> </li> <li> <p>Regression Detection \u2014 Automatic baseline comparison:</p> </li> <li>BaselineSnapshot stores per-metric baselines</li> <li>RegressionDetector with configurable tolerance</li> <li>RegressionReport identifies regressions vs. improvements</li> <li> <p>Easy conversion from EvaluationRun \u2192 BaselineSnapshot</p> </li> <li> <p>xUnit-Native Test Integration \u2014 Evaluation as unit tests:</p> </li> <li>AIEvaluationTest attribute</li> <li>AIAssert methods (AssertPassed, AssertScore, AssertMetric, etc.)</li> <li>AITestRunner for orchestration</li> <li> <p>Familiar test lifecycle for .NET developers</p> </li> <li> <p>Deterministic Evaluators \u2014 No external dependencies:</p> </li> <li>Hallucination (token overlap)</li> <li>Factuality (sentence extraction + keyword matching)</li> <li>Relevance (cosine similarity of term vectors)</li> <li>Coherence (sentence completion, contradiction, repetition checks)</li> <li> <p>Safety (local blocklist + PII regex)</p> </li> <li> <p>SQLite Persistence \u2014 Lightweight, portable result storage:</p> </li> <li>SqliteResultStore for durability</li> <li>Query-friendly for post-analysis</li> <li> <p>No cloud dependencies</p> </li> <li> <p>Flexible Export Formats \u2014 JsonExporter, CsvExporter for data portability</p> </li> </ol>"},{"location":"comparison-with-microsoft-evaluation/#gap-analysis-scenarios-not-covered-by-official-libraries","title":"Gap Analysis: Scenarios NOT Covered by Official Libraries","text":""},{"location":"comparison-with-microsoft-evaluation/#1-synthetic-test-data-generation-critical-gap","title":"1. Synthetic Test Data Generation (Critical Gap)","text":"<p>Problem: Developers building LLM applications need realistic test data fast, but creating golden datasets manually is laborious. Microsoft's libraries assume you already have a golden dataset.</p> <p>Solution (ElBruno): - SyntheticDatasetBuilder generates QA, RAG, adversarial, or domain-specific examples - Composite strategy: blend deterministic rules + LLM generation - Versioned, exportable datasets</p> <p>Actionable Example: </p><pre><code>var dataset = new SyntheticDatasetBuilder(\"customer-support-qa\")\n    .WithTemplate(TemplateType.QA)\n    .WithLLMGenerator(chatClient, 100)  // Generate 100 examples\n    .Build();\n</code></pre><p></p>"},{"location":"comparison-with-microsoft-evaluation/#2-offline-air-gapped-evaluation-critical-gap","title":"2. Offline / Air-Gapped Evaluation (Critical Gap)","text":"<p>Problem: Developers in regulated industries, private clouds, or with tight budgets cannot rely on external LLM calls for evaluation. Microsoft's quality evaluators require LLM calls; safety evaluators require Azure.</p> <p>Solution (ElBruno): - All evaluators work offline (heuristic-based) - No external dependencies, fully local - SQLite storage is self-contained</p> <p>Actionable Example: Use ElBruno in CI/CD pipelines without Azure credentials or network calls.</p>"},{"location":"comparison-with-microsoft-evaluation/#3-golden-dataset-lifecycle-versioning-medium-gap","title":"3. Golden Dataset Lifecycle &amp; Versioning (Medium Gap)","text":"<p>Problem: Teams need to track dataset evolution (add examples, remove noisy ones, A/B test different versions). Microsoft provides no dataset versioning, diffing, or management tools.</p> <p>Solution (ElBruno): - GoldenDataset with semantic versioning - DatasetDiff to identify Added/Removed/Modified examples - Subsetting by tags for targeted evaluation - CSV/JSON import/export for data science workflows</p> <p>Actionable Example: </p><pre><code>var v1 = await DatasetLoader.LoadAsync(\"dataset-v1.0.0.json\");\nvar v2 = await DatasetLoader.LoadAsync(\"dataset-v1.1.0.json\");\nvar diff = DatasetDiff.Diff(v1, v2);\nConsole.WriteLine($\"Added: {diff.Added.Count}, Removed: {diff.Removed.Count}\");\n</code></pre><p></p>"},{"location":"comparison-with-microsoft-evaluation/#4-regression-detection-in-cicd-medium-gap","title":"4. Regression Detection in CI/CD (Medium Gap)","text":"<p>Problem: Teams want to automatically detect performance regressions across evaluation runs and fail CI if thresholds are crossed. Microsoft requires manual baseline management.</p> <p>Solution (ElBruno): - RegressionDetector with configurable tolerance - Automatic snapshot creation from EvaluationRun - Per-metric regression reporting</p> <p>Actionable Example: </p><pre><code>var baseline = BaselineSnapshot.Load(\"baseline-v1.0.0.json\");\nvar run = await pipeline.RunAsync();\nvar report = RegressionDetector.Compare(baseline, run, tolerance: 0.05);\n\nif (report.HasRegressions)\n    throw new Exception($\"Regression detected: {report.Regressed.Keys}\");\n</code></pre><p></p>"},{"location":"comparison-with-microsoft-evaluation/#5-xunit-native-test-assertions-medium-gap","title":"5. xUnit-Native Test Assertions (Medium Gap)","text":"<p>Problem: .NET teams want AI evaluation to feel like normal unit testing, not a separate reporting workflow. Microsoft provides no xUnit integration.</p> <p>Solution (ElBruno): - AIEvaluationTest attribute - AIAssert methods (AssertPassed, AssertScore, AssertMetric) - Native xUnit/NUnit lifecycle</p> <p>Actionable Example: </p><pre><code>[AIEvaluationTest]\npublic async Task RelevanceEvaluator_ShouldPassHighQualityResponse()\n{\n    var evaluator = new RelevanceEvaluator();\n    var result = await evaluator.EvaluateAsync(input, output, expectedOutput);\n\n    AIAssert.AssertPassed(result);\n    AIAssert.AssertScore(result, minimumScore: 0.8);\n}\n</code></pre><p></p>"},{"location":"comparison-with-microsoft-evaluation/#6-cost-token-tracking-across-runs-small-gap","title":"6. Cost &amp; Token Tracking Across Runs (Small Gap)","text":"<p>Problem: Teams need to aggregate and trend token usage and costs across multiple evaluation runs.</p> <p>Solution (ElBruno): - EvaluationRun includes optional TotalTokens and EstimatedCost - SqliteResultStore for time-series analysis - Easy trend detection via SQL queries</p> <p>Actionable Example: </p><pre><code>var run1 = await pipeline.RunAsync();\nvar run2 = await pipeline.RunAsync();\n\nConsole.WriteLine($\"Run 1 Cost: ${run1.EstimatedCost:F2}\");\nConsole.WriteLine($\"Run 2 Cost: ${run2.EstimatedCost:F2}\");\n</code></pre><p></p>"},{"location":"comparison-with-microsoft-evaluation/#7-deterministic-debuggable-evaluation-small-gap","title":"7. Deterministic, Debuggable Evaluation (Small Gap)","text":"<p>Problem: Developers want to understand why an evaluation failed. LLM-based evaluators are opaque black boxes.</p> <p>Solution (ElBruno): - All evaluators use transparent heuristics (tokenization, overlap %, similarity) - MetricScore objects expose intermediate calculations - Details field explains reasoning</p> <p>Actionable Example: </p><pre><code>var result = await evaluator.EvaluateAsync(input, output);\nConsole.WriteLine($\"Details: {result.Details}\");  // Explains the heuristic used\n</code></pre><p></p>"},{"location":"comparison-with-microsoft-evaluation/#when-to-use-which","title":"When to Use Which","text":""},{"location":"comparison-with-microsoft-evaluation/#use-microsoftextensionsaievaluation-if","title":"Use Microsoft.Extensions.AI.Evaluation if:","text":"<ul> <li>\u2705 You need sophisticated quality judgment (Relevance, Completeness, Fluency, Groundedness)</li> <li>\u2705 You're building agentic systems (need IntentResolution, TaskAdherence, ToolCallAccuracy)</li> <li>\u2705 You require comprehensive safety evaluation (Azure AI Foundry classifiers)</li> <li>\u2705 You have full conversation history and need context-aware analysis</li> <li>\u2705 You can afford LLM calls (cost, latency, external dependency)</li> <li>\u2705 You need professional HTML reports and dashboard visualizations</li> <li>\u2705 You're operating at enterprise scale (Azure Storage integration)</li> <li>\u2705 You want NLP metrics (BLEU, GLEU, F1)</li> </ul>"},{"location":"comparison-with-microsoft-evaluation/#use-elbrunoaievaluation-if","title":"Use ElBruno.AI.Evaluation if:","text":"<ul> <li>\u2705 You need fast, offline evaluation (no external LLM/Azure calls)</li> <li>\u2705 You're in regulated or air-gapped environments</li> <li>\u2705 You want to manage golden datasets with versioning and diffing</li> <li>\u2705 You need regression detection in CI/CD pipelines</li> <li>\u2705 You prefer xUnit-native test integration</li> <li>\u2705 You're cost-conscious and want to avoid LLM call overhead</li> <li>\u2705 You need synthetic data generation</li> <li>\u2705 You want transparent, debuggable evaluators</li> <li>\u2705 You value simplicity over sophistication (single I/O API)</li> </ul>"},{"location":"comparison-with-microsoft-evaluation/#decision-tree","title":"Decision Tree","text":"<pre><code>START\n  \u2193\nNeed LLM-powered quality judgment?\n  YES \u2192 Use Microsoft (quality evaluators)\n  NO  \u2193\n       Need agentic evaluators (IntentResolution, TaskAdherence)?\n         YES \u2192 Use Microsoft\n         NO  \u2193\n             Need offline/air-gapped evaluation?\n               YES \u2192 Use ElBruno (+ Microsoft for non-safety evaluators)\n               NO  \u2193\n                   Need synthetic data or golden dataset versioning?\n                     YES \u2192 Use ElBruno\n                     NO  \u2193\n                         Need regression detection in CI/CD?\n                           YES \u2192 Use ElBruno\n                           NO  \u2192 Can use either; prefer Microsoft for comprehensiveness\n</code></pre>"},{"location":"comparison-with-microsoft-evaluation/#complementary-usage","title":"Complementary Usage","text":"<p>The two toolkits are not mutually exclusive\u2014they work well together:</p>"},{"location":"comparison-with-microsoft-evaluation/#pattern-1-hybrid-evaluation-pipeline","title":"Pattern 1: Hybrid Evaluation Pipeline","text":"<pre><code>// Step 1: Generate synthetic golden dataset with ElBruno\nvar syntheticDataset = new SyntheticDatasetBuilder(\"rag-qa\")\n    .WithLLMGenerator(chatClient, 50)\n    .Build();\n\n// Step 2: Run deterministic evaluators from ElBruno\nvar evaluators = new IEvaluator[]\n{\n    new HallucinationEvaluator(),\n    new FactualityEvaluator(),\n};\nvar detectionResults = await chatClient.EvaluateAsync(syntheticDataset, evaluators);\n\n// Step 3: For high-confidence failures, run Microsoft's LLM-based evaluators for nuance\nif (detectionResults.AggregateScore &lt; 0.7)\n{\n    var llmEvaluators = new[]\n    {\n        new MicrosoftRelevanceEvaluator(),  // LLM-powered\n        new MicrosoftCompleteness(),\n    };\n    var llmResults = await microsoftPipeline.EvaluateAsync(input, response);\n}\n</code></pre> <p>Benefit: Fast first pass with ElBruno, expensive second opinion from Microsoft only when needed.</p>"},{"location":"comparison-with-microsoft-evaluation/#pattern-2-regression-testing-with-baseline-snapshots","title":"Pattern 2: Regression Testing with Baseline Snapshots","text":"<pre><code>// Establish baseline with ElBruno deterministic evaluators\nvar baseline = BaselineSnapshot.Create(\n    \"latest-release\",\n    new HallucinationEvaluator(),\n    new FactualityEvaluator()\n);\nbaseline.Save(\"baseline-v1.0.0.json\");\n\n// In CI/CD, detect regressions fast with ElBruno\nvar newRun = await evaluationPipeline.RunWithBaselineAsync();  // RegressionReport\nif (newRun.HasRegressions)\n    throw new Exception(\"Regression detected\");\n\n// For high-impact changes, supplement with Microsoft's deeper analysis\nif (newRun.AggregateScore &gt; baseline.AggregateScore * 0.95)\n{\n    var microsoftReport = await microsoftPipeline.EvaluateAsync(...);\n}\n</code></pre>"},{"location":"comparison-with-microsoft-evaluation/#pattern-3-golden-dataset-microsoft-llm-evaluation","title":"Pattern 3: Golden Dataset + Microsoft LLM Evaluation","text":"<pre><code>// Build golden dataset once with ElBruno (versioned, portable)\nvar goldenDataset = new SyntheticDatasetBuilder(\"customer-support\")\n    .WithVersion(\"1.2.0\")\n    .WithTags(\"english\", \"high-quality\")\n    .Build();\nawait DatasetLoader.SaveAsync(goldenDataset, \"golden-dataset-v1.2.0.json\");\n\n// Use with Microsoft's LLM evaluators repeatedly\nvar microsoftEvaluators = new[]\n{\n    new MicrosoftRelevanceEvaluator(config),\n    new MicrosoftCoherence(config),\n};\n\n// Run evaluation as needed (Microsoft handles caching)\nvar scenario1 = await microsoftPipeline.EvaluateAsync(goldenDataset, ...);\nvar scenario2 = await microsoftPipeline.EvaluateAsync(goldenDataset, ...);\n</code></pre> <p>Benefit: Best of both worlds: ElBruno's dataset management, Microsoft's powerful evaluators.</p>"},{"location":"comparison-with-microsoft-evaluation/#pattern-4-test-automation-enterprise-reporting","title":"Pattern 4: Test Automation + Enterprise Reporting","text":"<pre><code>// Step 1: Local testing with ElBruno xUnit integration\n[AIEvaluationTest]\npublic async Task ValidateRAGRelevance()\n{\n    var evaluator = new RelevanceEvaluator();\n    var result = await evaluator.EvaluateAsync(query, response, expected);\n    AIAssert.AssertScore(result, minimumScore: 0.8);\n}\n\n// Step 2: Continuous evaluation with Microsoft for reports\nvar microsoftRun = await microsoftPipeline.EvaluateAsync(goldenDataset);\n// Microsoft generates HTML report + uploads to Azure\nawait reporter.GenerateHtmlReportAsync(microsoftRun);\n</code></pre>"},{"location":"comparison-with-microsoft-evaluation/#summary-table","title":"Summary Table","text":"Dimension Microsoft ElBruno Recommendation LLM-Powered Evaluation \u2705 Comprehensive \u274c Not included Use Microsoft for quality judgment Offline Evaluation \u274c Requires Azure \u2705 Fully local Use ElBruno for air-gapped environments Synthetic Data \u274c Not provided \u2705 Built-in Use ElBruno for data generation Golden Dataset Versioning \u274c Manual \u2705 Automated Use ElBruno for dataset lifecycle Regression Detection \u26a0\ufe0f Manual baseline \u2705 Automated Use ElBruno for CI/CD gates xUnit Integration \u274c Not provided \u2705 Native Use ElBruno for unit tests Enterprise Reporting \u2705 HTML + Azure \u274c Not included Use Microsoft for dashboards Response Caching \u2705 Built-in \u274c N/A Use Microsoft for cost optimization Ease of Learning \u26a0\ufe0f Steeper (LLM concepts) \u2705 Simpler (text heuristics) Use ElBruno to get started quickly Ideal Workflow High-quality evals + reports Fast iteration + baselines Use both in combination"},{"location":"comparison-with-microsoft-evaluation/#conclusion","title":"Conclusion","text":"<p>Microsoft.Extensions.AI.Evaluation is the go-to choice for sophisticated, LLM-powered quality and safety evaluation at scale. Its strength lies in leveraging Azure AI Foundry and advanced language understanding.</p> <p>ElBruno.AI.Evaluation fills critical gaps in offline evaluation, synthetic data generation, golden dataset management, and test automation\u2014scenarios not addressed by Microsoft's libraries. Its strength lies in simplicity, transparency, and local-first evaluation.</p> <p>Best Practice: Use both. Start with ElBruno for fast iteration, golden dataset versioning, and regression detection in CI/CD. Graduate to Microsoft when you need nuanced quality judgment, agentic evaluation, or enterprise safety analysis. The two toolkits complement each other seamlessly.</p>"},{"location":"design-synthetic-data/","title":"Design: ElBruno.AI.Evaluation.SyntheticData","text":""},{"location":"design-synthetic-data/#design-elbrunoaievaluationsyntheticdata","title":"Design: ElBruno.AI.Evaluation.SyntheticData","text":"<p>Status: Design Phase Version: 1.0 Author: Mulder (Research Director) Date: 2025 Target Release: ElBruno.AI.Evaluation v1.5</p>"},{"location":"design-synthetic-data/#1-executive-summary","title":"1. Executive Summary","text":"<p><code>ElBruno.AI.Evaluation.SyntheticData</code> is the 4th NuGet package in the ElBruno.AI.Evaluation toolkit. It generates synthetic <code>GoldenDataset</code> and <code>GoldenExample</code> instances for AI evaluation testing, supporting both deterministic (template-based) and AI-powered (IChatClient-based) generation strategies. The package provides a fluent builder API consistent with existing pipeline patterns and handles common evaluation scenarios: Q&amp;A pairs, RAG context+answer, adversarial edge cases, and domain-specific data.</p>"},{"location":"design-synthetic-data/#2-package-metadata","title":"2. Package Metadata","text":"Property Value Package ID <code>ElBruno.AI.Evaluation.SyntheticData</code> Target Framework <code>.NET 8.0+</code> Language C# latest (with nullable enabled) Root Namespace <code>ElBruno.AI.Evaluation.SyntheticData</code> Dependencies <code>ElBruno.AI.Evaluation</code> (latest), <code>Microsoft.Extensions.AI.Abstractions</code> 9.5.0+ License MIT"},{"location":"design-synthetic-data/#3-project-structure","title":"3. Project Structure","text":"<pre><code>src/ElBruno.AI.Evaluation.SyntheticData/\n\u251c\u2500\u2500 ElBruno.AI.Evaluation.SyntheticData.csproj\n\u251c\u2500\u2500 SyntheticDatasetBuilder.cs          # Main fluent builder entry point\n\u251c\u2500\u2500 Generators/\n\u2502   \u251c\u2500\u2500 ISyntheticDataGenerator.cs      # Core interface for all generators\n\u2502   \u251c\u2500\u2500 DeterministicGenerator.cs       # Template-based generation\n\u2502   \u251c\u2500\u2500 LlmGenerator.cs                 # IChatClient-based generation\n\u2502   \u2514\u2500\u2500 CompositeGenerator.cs           # Combines multiple generators\n\u251c\u2500\u2500 Templates/\n\u2502   \u251c\u2500\u2500 IDataTemplate.cs                # Interface for template definitions\n\u2502   \u251c\u2500\u2500 QaTemplate.cs                   # Q&amp;A pair templates\n\u2502   \u251c\u2500\u2500 RagTemplate.cs                  # RAG context+answer templates\n\u2502   \u251c\u2500\u2500 AdversarialTemplate.cs          # Edge-case &amp; adversarial templates\n\u2502   \u2514\u2500\u2500 DomainTemplate.cs               # Domain-specific scenario templates\n\u251c\u2500\u2500 Strategies/\n\u2502   \u251c\u2500\u2500 GenerationStrategy.cs           # Strategy enum/classes\n\u2502   \u251c\u2500\u2500 TemplateStrategy.cs             # Template-based settings\n\u2502   \u2514\u2500\u2500 LlmStrategy.cs                  # LLM-based settings\n\u251c\u2500\u2500 Extensions/\n\u2502   \u2514\u2500\u2500 SyntheticDatasetExtensions.cs   # Helper methods for datasets\n\u2514\u2500\u2500 Utilities/\n    \u251c\u2500\u2500 RandomSeedProvider.cs           # Deterministic seed management\n    \u2514\u2500\u2500 PromptGenerator.cs              # LLM prompt composition\n</code></pre>"},{"location":"design-synthetic-data/#4-public-api-surface","title":"4. Public API Surface","text":""},{"location":"design-synthetic-data/#41-core-builder","title":"4.1 Core Builder","text":""},{"location":"design-synthetic-data/#syntheticdatasetbuilder-public-class","title":"<code>SyntheticDatasetBuilder</code> (Public Class)","text":"<p>Main entry point for fluent dataset generation. Implements fluent builder pattern consistent with <code>EvaluationPipelineBuilder</code>.</p> <p>Public Constructor &amp; Methods:</p> <pre><code>namespace ElBruno.AI.Evaluation.SyntheticData;\n\n/// &lt;summary&gt;\n/// Fluent builder for generating synthetic golden datasets.\n/// &lt;/summary&gt;\npublic sealed class SyntheticDatasetBuilder\n{\n    // Initialization\n    public SyntheticDatasetBuilder(string datasetName);\n\n    // Metadata configuration\n    /// &lt;summary&gt;Sets the semantic version of the dataset.&lt;/summary&gt;\n    public SyntheticDatasetBuilder WithVersion(string version);\n\n    /// &lt;summary&gt;Sets the dataset description.&lt;/summary&gt;\n    public SyntheticDatasetBuilder WithDescription(string description);\n\n    /// &lt;summary&gt;Adds tags to categorize the dataset.&lt;/summary&gt;\n    public SyntheticDatasetBuilder WithTags(params string[] tags);\n\n    // Generator selection &amp; configuration\n    /// &lt;summary&gt;Uses deterministic template-based generation.&lt;/summary&gt;\n    public SyntheticDatasetBuilder UseDeterministicGenerator(\n        Action&lt;TemplateStrategy&gt; configureStrategy);\n\n    /// &lt;summary&gt;Uses LLM-powered generation via IChatClient.&lt;/summary&gt;\n    public SyntheticDatasetBuilder UseLlmGenerator(\n        IChatClient chatClient,\n        Action&lt;LlmStrategy&gt; configureStrategy);\n\n    /// &lt;summary&gt;Uses composite generation (deterministic + LLM).&lt;/summary&gt;\n    public SyntheticDatasetBuilder UseCompositeGenerator(\n        Action&lt;CompositeGeneratorConfig&gt; configureComposite);\n\n    // Scenario-specific builders (syntactic sugar)\n    /// &lt;summary&gt;Generates Q&amp;A pairs from a template.&lt;/summary&gt;\n    public SyntheticDatasetBuilder GenerateQaPairs(\n        int count,\n        Action&lt;QaTemplate&gt;? configure = null);\n\n    /// &lt;summary&gt;Generates RAG context+answer examples.&lt;/summary&gt;\n    public SyntheticDatasetBuilder GenerateRagExamples(\n        int count,\n        Action&lt;RagTemplate&gt;? configure = null);\n\n    /// &lt;summary&gt;Generates adversarial/edge-case examples.&lt;/summary&gt;\n    public SyntheticDatasetBuilder GenerateAdversarialExamples(\n        int count,\n        Action&lt;AdversarialTemplate&gt;? configure = null);\n\n    /// &lt;summary&gt;Generates domain-specific examples.&lt;/summary&gt;\n    public SyntheticDatasetBuilder GenerateDomainExamples(\n        string domain,\n        int count,\n        Action&lt;DomainTemplate&gt;? configure = null);\n\n    // Output\n    /// &lt;summary&gt;Builds and returns the synthetic GoldenDataset.&lt;/summary&gt;\n    /// &lt;exception cref=\"InvalidOperationException\"&gt;Thrown when no generator is configured.&lt;/exception&gt;\n    public Task&lt;GoldenDataset&gt; BuildAsync(CancellationToken ct = default);\n}\n</code></pre>"},{"location":"design-synthetic-data/#42-generator-interfaces-implementations","title":"4.2 Generator Interfaces &amp; Implementations","text":""},{"location":"design-synthetic-data/#isyntheticdatagenerator-public-interface","title":"<code>ISyntheticDataGenerator</code> (Public Interface)","text":"<p>Core abstraction for all generation strategies.</p> <pre><code>namespace ElBruno.AI.Evaluation.SyntheticData.Generators;\n\n/// &lt;summary&gt;\n/// Core interface for synthetic data generation strategies.\n/// &lt;/summary&gt;\npublic interface ISyntheticDataGenerator\n{\n    /// &lt;summary&gt;\n    /// Generates synthetic GoldenExample instances.\n    /// &lt;/summary&gt;\n    /// &lt;param name=\"count\"&gt;Number of examples to generate.&lt;/param&gt;\n    /// &lt;param name=\"ct\"&gt;Cancellation token.&lt;/param&gt;\n    /// &lt;returns&gt;Collection of generated GoldenExample instances.&lt;/returns&gt;\n    Task&lt;IReadOnlyList&lt;GoldenExample&gt;&gt; GenerateAsync(\n        int count,\n        CancellationToken ct = default);\n}\n</code></pre>"},{"location":"design-synthetic-data/#deterministicgenerator-public-class","title":"<code>DeterministicGenerator</code> (Public Class)","text":"<p>Template-based generation with deterministic results.</p> <pre><code>namespace ElBruno.AI.Evaluation.SyntheticData.Generators;\n\n/// &lt;summary&gt;\n/// Generates synthetic examples deterministically using templates.\n/// Suitable for reproducible, lightweight test data generation.\n/// &lt;/summary&gt;\npublic sealed class DeterministicGenerator : ISyntheticDataGenerator\n{\n    /// &lt;summary&gt;\n    /// Creates a new deterministic generator with the specified template.\n    /// &lt;/summary&gt;\n    public DeterministicGenerator(IDataTemplate template);\n\n    /// &lt;summary&gt;\n    /// Creates a new deterministic generator with optional random seed for reproducibility.\n    /// &lt;/summary&gt;\n    public DeterministicGenerator(IDataTemplate template, int? randomSeed);\n\n    /// &lt;inheritdoc /&gt;\n    public Task&lt;IReadOnlyList&lt;GoldenExample&gt;&gt; GenerateAsync(\n        int count,\n        CancellationToken ct = default);\n}\n</code></pre>"},{"location":"design-synthetic-data/#llmgenerator-public-class","title":"<code>LlmGenerator</code> (Public Class)","text":"<p>IChatClient-powered generation with AI-driven content creation.</p> <pre><code>namespace ElBruno.AI.Evaluation.SyntheticData.Generators;\n\n/// &lt;summary&gt;\n/// Generates synthetic examples using an IChatClient.\n/// Provides AI-powered, high-variance data suitable for adversarial/edge-case testing.\n/// &lt;/summary&gt;\npublic sealed class LlmGenerator : ISyntheticDataGenerator\n{\n    /// &lt;summary&gt;\n    /// Creates a new LLM-based generator.\n    /// &lt;/summary&gt;\n    public LlmGenerator(\n        IChatClient chatClient,\n        string systemPrompt,\n        GenerationTemplate generationTemplate);\n\n    /// &lt;summary&gt;\n    /// Sets the temperature for generation (0.0-2.0). Default: 0.7.\n    /// &lt;/summary&gt;\n    public LlmGenerator WithTemperature(double temperature);\n\n    /// &lt;summary&gt;\n    /// Sets the maximum tokens per response. Default: 500.\n    /// &lt;/summary&gt;\n    public LlmGenerator WithMaxTokens(int maxTokens);\n\n    /// &lt;summary&gt;\n    /// Sets the number of parallel generation requests. Default: 1.\n    /// &lt;/summary&gt;\n    public LlmGenerator WithParallelism(int degree);\n\n    /// &lt;inheritdoc /&gt;\n    public Task&lt;IReadOnlyList&lt;GoldenExample&gt;&gt; GenerateAsync(\n        int count,\n        CancellationToken ct = default);\n}\n</code></pre>"},{"location":"design-synthetic-data/#compositegenerator-public-class","title":"<code>CompositeGenerator</code> (Public Class)","text":"<p>Combines multiple generators for hybrid generation strategies.</p> <pre><code>namespace ElBruno.AI.Evaluation.SyntheticData.Generators;\n\n/// &lt;summary&gt;\n/// Combines multiple ISyntheticDataGenerator instances for hybrid generation.\n/// Example: 70% deterministic examples + 30% LLM-powered examples.\n/// &lt;/summary&gt;\npublic sealed class CompositeGenerator : ISyntheticDataGenerator\n{\n    /// &lt;summary&gt;\n    /// Creates a new composite generator with weighted sub-generators.\n    /// &lt;/summary&gt;\n    public CompositeGenerator(\n        params (ISyntheticDataGenerator generator, double weight)[] generators);\n\n    /// &lt;inheritdoc /&gt;\n    public Task&lt;IReadOnlyList&lt;GoldenExample&gt;&gt; GenerateAsync(\n        int count,\n        CancellationToken ct = default);\n}\n</code></pre>"},{"location":"design-synthetic-data/#43-template-interfaces-implementations","title":"4.3 Template Interfaces &amp; Implementations","text":""},{"location":"design-synthetic-data/#idatatemplate-public-interface","title":"<code>IDataTemplate</code> (Public Interface)","text":"<p>Base interface for all template types.</p> <pre><code>namespace ElBruno.AI.Evaluation.SyntheticData.Templates;\n\n/// &lt;summary&gt;\n/// Base interface for data templates.\n/// Templates define the structure and content patterns for synthetic data generation.\n/// &lt;/summary&gt;\npublic interface IDataTemplate\n{\n    /// &lt;summary&gt;\n    /// Gets the template type/category.\n    /// &lt;/summary&gt;\n    string TemplateType { get; }\n\n    /// &lt;summary&gt;\n    /// Gets the example tags to apply to generated examples.\n    /// &lt;/summary&gt;\n    IReadOnlyList&lt;string&gt; Tags { get; }\n\n    /// &lt;summary&gt;\n    /// Gets optional metadata to include in generated examples.\n    /// &lt;/summary&gt;\n    IReadOnlyDictionary&lt;string, string&gt; Metadata { get; }\n}\n</code></pre>"},{"location":"design-synthetic-data/#qatemplate-public-class","title":"<code>QaTemplate</code> (Public Class)","text":"<p>Generates Q&amp;A pair examples.</p> <pre><code>namespace ElBruno.AI.Evaluation.SyntheticData.Templates;\n\n/// &lt;summary&gt;\n/// Template for generating Q&amp;A (question-answer) pairs.\n/// Suitable for FAQ systems, chatbots, and knowledge-based QA.\n/// &lt;/summary&gt;\npublic sealed class QaTemplate : IDataTemplate\n{\n    /// &lt;summary&gt;\n    /// Creates a new Q&amp;A template with question prompts and answer patterns.\n    /// &lt;/summary&gt;\n    public QaTemplate(\n        IReadOnlyList&lt;string&gt; questionTemplates,\n        IReadOnlyList&lt;string&gt; answerTemplates);\n\n    /// &lt;summary&gt;\n    /// Sets the category/domain for these Q&amp;A pairs (e.g., \"technical-support\").\n    /// &lt;/summary&gt;\n    public QaTemplate WithCategory(string category);\n\n    /// &lt;summary&gt;\n    /// Adds tags to all generated examples.\n    /// &lt;/summary&gt;\n    public QaTemplate AddTags(params string[] tags);\n\n    /// &lt;summary&gt;\n    /// Adds metadata key-value pairs to all generated examples.\n    /// &lt;/summary&gt;\n    public QaTemplate WithMetadata(Dictionary&lt;string, string&gt; metadata);\n\n    /// &lt;summary&gt;\n    /// Gets or generates template pairs (question \u2192 answer mappings).\n    /// &lt;/summary&gt;\n    public IReadOnlyList&lt;(string Question, string Answer)&gt; GetPairs();\n\n    public string TemplateType =&gt; \"QA\";\n    public IReadOnlyList&lt;string&gt; Tags { get; }\n    public IReadOnlyDictionary&lt;string, string&gt; Metadata { get; }\n}\n</code></pre>"},{"location":"design-synthetic-data/#ragtemplate-public-class","title":"<code>RagTemplate</code> (Public Class)","text":"<p>Generates RAG (Retrieval-Augmented Generation) context+answer examples.</p> <pre><code>namespace ElBruno.AI.Evaluation.SyntheticData.Templates;\n\n/// &lt;summary&gt;\n/// Template for generating RAG examples (question + context \u2192 answer).\n/// Suitable for evaluating retrieval-augmented generation systems.\n/// &lt;/summary&gt;\npublic sealed class RagTemplate : IDataTemplate\n{\n    /// &lt;summary&gt;\n    /// Creates a new RAG template with document contexts and question-answer pairs.\n    /// &lt;/summary&gt;\n    public RagTemplate(\n        IReadOnlyList&lt;string&gt; documents,\n        IReadOnlyList&lt;(string Question, string Answer)&gt; qaExamples);\n\n    /// &lt;summary&gt;\n    /// Sets the number of document chunks per example. Default: 1.\n    /// &lt;/summary&gt;\n    public RagTemplate WithDocumentsPerExample(int count);\n\n    /// &lt;summary&gt;\n    /// Sets tags for the RAG examples.\n    /// &lt;/summary&gt;\n    public RagTemplate AddTags(params string[] tags);\n\n    /// &lt;summary&gt;\n    /// Adds metadata to all generated examples.\n    /// &lt;/summary&gt;\n    public RagTemplate WithMetadata(Dictionary&lt;string, string&gt; metadata);\n\n    /// &lt;summary&gt;\n    /// Gets the underlying documents (context sources).\n    /// &lt;/summary&gt;\n    public IReadOnlyList&lt;string&gt; Documents { get; }\n\n    /// &lt;summary&gt;\n    /// Gets the Q&amp;A pairs that anchor the examples.\n    /// &lt;/summary&gt;\n    public IReadOnlyList&lt;(string Question, string Answer)&gt; QaExamples { get; }\n\n    public string TemplateType =&gt; \"RAG\";\n    public IReadOnlyList&lt;string&gt; Tags { get; }\n    public IReadOnlyDictionary&lt;string, string&gt; Metadata { get; }\n}\n</code></pre>"},{"location":"design-synthetic-data/#adversarialtemplate-public-class","title":"<code>AdversarialTemplate</code> (Public Class)","text":"<p>Generates edge-case and adversarial examples.</p> <pre><code>namespace ElBruno.AI.Evaluation.SyntheticData.Templates;\n\n/// &lt;summary&gt;\n/// Template for generating adversarial and edge-case examples.\n/// Includes nulls, empty strings, contradictions, typos, and malformed inputs.\n/// &lt;/summary&gt;\npublic sealed class AdversarialTemplate : IDataTemplate\n{\n    /// &lt;summary&gt;\n    /// Creates a new adversarial template with base examples to perturb.\n    /// &lt;/summary&gt;\n    public AdversarialTemplate(IReadOnlyList&lt;GoldenExample&gt; baseExamples);\n\n    /// &lt;summary&gt;\n    /// Enables null/empty input injection. Default: true.\n    /// &lt;/summary&gt;\n    public AdversarialTemplate WithNullInjection(bool enabled = true);\n\n    /// &lt;summary&gt;\n    /// Enables input truncation (partial/incomplete queries). Default: true.\n    /// &lt;/summary&gt;\n    public AdversarialTemplate WithTruncation(bool enabled = true);\n\n    /// &lt;summary&gt;\n    /// Enables typo/character-level perturbations. Default: true.\n    /// &lt;/summary&gt;\n    public AdversarialTemplate WithTypoInjection(bool enabled = true);\n\n    /// &lt;summary&gt;\n    /// Enables contradiction injection (conflicting context vs expected output). Default: true.\n    /// &lt;/summary&gt;\n    public AdversarialTemplate WithContradictions(bool enabled = true);\n\n    /// &lt;summary&gt;\n    /// Enables extremely long input generation. Default: false.\n    /// &lt;/summary&gt;\n    public AdversarialTemplate WithLongInputs(bool enabled = false, int maxLength = 2000);\n\n    /// &lt;summary&gt;\n    /// Adds tags to all generated adversarial examples.\n    /// &lt;/summary&gt;\n    public AdversarialTemplate AddTags(params string[] tags);\n\n    public string TemplateType =&gt; \"Adversarial\";\n    public IReadOnlyList&lt;string&gt; Tags { get; }\n    public IReadOnlyDictionary&lt;string, string&gt; Metadata { get; }\n}\n</code></pre>"},{"location":"design-synthetic-data/#domaintemplate-public-class","title":"<code>DomainTemplate</code> (Public Class)","text":"<p>Generates domain-specific examples (healthcare, finance, legal, etc.).</p> <pre><code>namespace ElBruno.AI.Evaluation.SyntheticData.Templates;\n\n/// &lt;summary&gt;\n/// Template for domain-specific synthetic data generation.\n/// Handles domain vocabularies, terminology, and realistic constraints.\n/// &lt;/summary&gt;\npublic sealed class DomainTemplate : IDataTemplate\n{\n    /// &lt;summary&gt;\n    /// Creates a template for a specific domain (e.g., \"healthcare\", \"finance\", \"legal\").\n    /// &lt;/summary&gt;\n    public DomainTemplate(string domain);\n\n    /// &lt;summary&gt;\n    /// Sets domain-specific vocabulary/terms to use in generation.\n    /// &lt;/summary&gt;\n    public DomainTemplate WithVocabulary(IReadOnlyList&lt;string&gt; terms);\n\n    /// &lt;summary&gt;\n    /// Sets domain-specific constraints (e.g., \"only discuss public companies\" for finance).\n    /// &lt;/summary&gt;\n    public DomainTemplate WithConstraints(params string[] constraints);\n\n    /// &lt;summary&gt;\n    /// Sets the regulatory/compliance framework (e.g., \"HIPAA\" for healthcare).\n    /// &lt;/summary&gt;\n    public DomainTemplate WithComplianceFramework(string framework);\n\n    /// &lt;summary&gt;\n    /// Adds tags reflecting the domain.\n    /// &lt;/summary&gt;\n    public DomainTemplate AddTags(params string[] tags);\n\n    /// &lt;summary&gt;\n    /// Adds domain-specific metadata.\n    /// &lt;/summary&gt;\n    public DomainTemplate WithMetadata(Dictionary&lt;string, string&gt; metadata);\n\n    /// &lt;summary&gt;\n    /// Gets the domain name (e.g., \"healthcare\", \"finance\").\n    /// &lt;/summary&gt;\n    public string Domain { get; }\n\n    /// &lt;summary&gt;\n    /// Gets the vocabulary terms for this domain.\n    /// &lt;/summary&gt;\n    public IReadOnlyList&lt;string&gt; Vocabulary { get; }\n\n    public string TemplateType =&gt; $\"Domain:{Domain}\";\n    public IReadOnlyList&lt;string&gt; Tags { get; }\n    public IReadOnlyDictionary&lt;string, string&gt; Metadata { get; }\n}\n</code></pre>"},{"location":"design-synthetic-data/#44-strategy-configuration-classes","title":"4.4 Strategy Configuration Classes","text":""},{"location":"design-synthetic-data/#templatestrategy-public-class","title":"<code>TemplateStrategy</code> (Public Class)","text":"<p>Configuration for deterministic template-based generation.</p> <pre><code>namespace ElBruno.AI.Evaluation.SyntheticData.Strategies;\n\n/// &lt;summary&gt;\n/// Configuration strategy for deterministic template-based generation.\n/// &lt;/summary&gt;\npublic sealed class TemplateStrategy\n{\n    /// &lt;summary&gt;\n    /// Gets or sets the data template to use for generation.\n    /// &lt;/summary&gt;\n    public IDataTemplate? Template { get; set; }\n\n    /// &lt;summary&gt;\n    /// Gets or sets the optional random seed for reproducible generation.\n    /// Null = non-deterministic.\n    /// &lt;/summary&gt;\n    public int? RandomSeed { get; set; }\n\n    /// &lt;summary&gt;\n    /// Gets or sets whether to shuffle generated examples. Default: false.\n    /// &lt;/summary&gt;\n    public bool Shuffle { get; set; } = false;\n\n    /// &lt;summary&gt;\n    /// Gets or sets how to handle missing/null expected outputs.\n    /// Options: \"skip\", \"use_input_as_expected\", \"empty_string\".\n    /// Default: \"empty_string\".\n    /// &lt;/summary&gt;\n    public string? NullHandling { get; set; } = \"empty_string\";\n}\n</code></pre>"},{"location":"design-synthetic-data/#llmstrategy-public-class","title":"<code>LlmStrategy</code> (Public Class)","text":"<p>Configuration for LLM-powered generation.</p> <pre><code>namespace ElBruno.AI.Evaluation.SyntheticData.Strategies;\n\n/// &lt;summary&gt;\n/// Configuration strategy for LLM-powered synthetic data generation.\n/// &lt;/summary&gt;\npublic sealed class LlmStrategy\n{\n    /// &lt;summary&gt;\n    /// Gets or sets the system prompt that guides the LLM.\n    /// &lt;/summary&gt;\n    public string? SystemPrompt { get; set; }\n\n    /// &lt;summary&gt;\n    /// Gets or sets the generation template specifying output structure.\n    /// &lt;/summary&gt;\n    public GenerationTemplate? GenerationTemplate { get; set; }\n\n    /// &lt;summary&gt;\n    /// Gets or sets the temperature for generation (0.0-2.0). Default: 0.7.\n    /// &lt;/summary&gt;\n    public double Temperature { get; set; } = 0.7;\n\n    /// &lt;summary&gt;\n    /// Gets or sets the maximum tokens per response. Default: 500.\n    /// &lt;/summary&gt;\n    public int MaxTokens { get; set; } = 500;\n\n    /// &lt;summary&gt;\n    /// Gets or sets the degree of parallelism. Default: 1.\n    /// &lt;/summary&gt;\n    public int ParallelismDegree { get; set; } = 1;\n\n    /// &lt;summary&gt;\n    /// Gets or sets whether to retry failed generations. Default: true.\n    /// &lt;/summary&gt;\n    public bool RetryOnFailure { get; set; } = true;\n\n    /// &lt;summary&gt;\n    /// Gets or sets the maximum retry count. Default: 3.\n    /// &lt;/summary&gt;\n    public int MaxRetries { get; set; } = 3;\n}\n</code></pre>"},{"location":"design-synthetic-data/#generationtemplate-public-enum","title":"<code>GenerationTemplate</code> (Public Enum)","text":"<p>Predefined output formats for LLM-powered generation.</p> <pre><code>namespace ElBruno.AI.Evaluation.SyntheticData.Strategies;\n\n/// &lt;summary&gt;\n/// Predefined output format templates for LLM generation.\n/// Guides the LLM to produce structured GoldenExample instances.\n/// &lt;/summary&gt;\npublic enum GenerationTemplate\n{\n    /// &lt;summary&gt;Simple Input \u2192 Output pairs.&lt;/summary&gt;\n    SimpleQA = 0,\n\n    /// &lt;summary&gt;Input + Context \u2192 Output (RAG-style).&lt;/summary&gt;\n    RagContext = 1,\n\n    /// &lt;summary&gt;Input \u2192 Output + Explanation.&lt;/summary&gt;\n    QAWithExplanation = 2,\n\n    /// &lt;summary&gt;Multiple variations of Input \u2192 Output for the same concept.&lt;/summary&gt;\n    QAVariations = 3,\n\n    /// &lt;summary&gt;Edge cases and adversarial examples.&lt;/summary&gt;\n    AdversarialCases = 4,\n\n    /// &lt;summary&gt;Domain-specific examples with metadata.&lt;/summary&gt;\n    DomainSpecific = 5,\n}\n</code></pre>"},{"location":"design-synthetic-data/#compositegeneratorconfig-public-class","title":"<code>CompositeGeneratorConfig</code> (Public Class)","text":"<p>Configuration for composite/hybrid generation.</p> <pre><code>namespace ElBruno.AI.Evaluation.SyntheticData.Strategies;\n\n/// &lt;summary&gt;\n/// Configuration for composite generation (hybrid deterministic + LLM).\n/// &lt;/summary&gt;\npublic sealed class CompositeGeneratorConfig\n{\n    /// &lt;summary&gt;\n    /// Adds a deterministic generator with a weight (proportion of total examples).\n    /// &lt;/summary&gt;\n    public void AddDeterministicGenerator(\n        IDataTemplate template,\n        double weight,\n        int? randomSeed = null);\n\n    /// &lt;summary&gt;\n    /// Adds an LLM generator with a weight.\n    /// &lt;/summary&gt;\n    public void AddLlmGenerator(\n        IChatClient chatClient,\n        string systemPrompt,\n        GenerationTemplate generationTemplate,\n        double weight);\n\n    /// &lt;summary&gt;\n    /// Gets the configured composite generator.\n    /// &lt;/summary&gt;\n    public CompositeGenerator Build();\n}\n</code></pre>"},{"location":"design-synthetic-data/#45-extension-methods","title":"4.5 Extension Methods","text":""},{"location":"design-synthetic-data/#syntheticdatasetextensions-public-class","title":"<code>SyntheticDatasetExtensions</code> (Public Class)","text":"<p>Helper methods for working with synthetic datasets.</p> <pre><code>namespace ElBruno.AI.Evaluation.SyntheticData.Extensions;\n\n/// &lt;summary&gt;\n/// Extension methods for GoldenDataset and synthetic data operations.\n/// &lt;/summary&gt;\npublic static class SyntheticDatasetExtensions\n{\n    /// &lt;summary&gt;\n    /// Augments an existing dataset with synthetically generated examples.\n    /// &lt;/summary&gt;\n    public static async Task&lt;GoldenDataset&gt; AugmentWithSyntheticExamplesAsync(\n        this GoldenDataset dataset,\n        ISyntheticDataGenerator generator,\n        int count,\n        CancellationToken ct = default);\n\n    /// &lt;summary&gt;\n    /// Merges multiple datasets into a single combined dataset.\n    /// &lt;/summary&gt;\n    public static GoldenDataset Merge(\n        this GoldenDataset dataset,\n        params GoldenDataset[] otherDatasets);\n\n    /// &lt;summary&gt;\n    /// Deduplicates examples by input hash.\n    /// &lt;/summary&gt;\n    public static GoldenDataset Deduplicate(this GoldenDataset dataset);\n\n    /// &lt;summary&gt;\n    /// Validates synthetic examples (non-null inputs/outputs, reasonable lengths).\n    /// &lt;/summary&gt;\n    public static IReadOnlyList&lt;ValidationError&gt; ValidateExamples(\n        this GoldenDataset dataset,\n        ValidationOptions? options = null);\n}\n</code></pre>"},{"location":"design-synthetic-data/#validationerror-public-class","title":"<code>ValidationError</code> (Public Class)","text":"<p>Represents a validation issue in synthetic data.</p> <pre><code>namespace ElBruno.AI.Evaluation.SyntheticData.Extensions;\n\n/// &lt;summary&gt;\n/// Represents a validation error found in synthetic examples.\n/// &lt;/summary&gt;\npublic sealed class ValidationError\n{\n    /// &lt;summary&gt;Index of the example in the dataset.&lt;/summary&gt;\n    public int ExampleIndex { get; init; }\n\n    /// &lt;summary&gt;Type of validation error (e.g., \"null_input\", \"empty_output\").&lt;/summary&gt;\n    public string ErrorType { get; init; } = string.Empty;\n\n    /// &lt;summary&gt;Human-readable error message.&lt;/summary&gt;\n    public string Message { get; init; } = string.Empty;\n\n    /// &lt;summary&gt;Severity level: \"error\", \"warning\".&lt;/summary&gt;\n    public string Severity { get; init; } = \"error\";\n}\n</code></pre>"},{"location":"design-synthetic-data/#validationoptions-public-class","title":"<code>ValidationOptions</code> (Public Class)","text":"<p>Configuration for validation logic.</p> <pre><code>namespace ElBruno.AI.Evaluation.SyntheticData.Extensions;\n\n/// &lt;summary&gt;\n/// Options for validating synthetic datasets.\n/// &lt;/summary&gt;\npublic sealed class ValidationOptions\n{\n    /// &lt;summary&gt;Minimum input length (characters). Default: 1.&lt;/summary&gt;\n    public int MinInputLength { get; set; } = 1;\n\n    /// &lt;summary&gt;Maximum input length (characters). Default: 5000.&lt;/summary&gt;\n    public int MaxInputLength { get; set; } = 5000;\n\n    /// &lt;summary&gt;Minimum expected output length (characters). Default: 0 (allow empty).&lt;/summary&gt;\n    public int MinOutputLength { get; set; } = 0;\n\n    /// &lt;summary&gt;Maximum expected output length (characters). Default: 5000.&lt;/summary&gt;\n    public int MaxOutputLength { get; set; } = 5000;\n\n    /// &lt;summary&gt;Whether to flag examples with null inputs. Default: true.&lt;/summary&gt;\n    public bool FlagNullInputs { get; set; } = true;\n\n    /// &lt;summary&gt;Whether to flag examples with null expected outputs. Default: false (allowed).&lt;/summary&gt;\n    public bool FlagNullOutputs { get; set; } = false;\n\n    /// &lt;summary&gt;Whether to check for duplicate inputs. Default: true.&lt;/summary&gt;\n    public bool FlagDuplicateInputs { get; set; } = true;\n}\n</code></pre>"},{"location":"design-synthetic-data/#5-scenario-examples-with-code","title":"5. Scenario Examples with Code","text":""},{"location":"design-synthetic-data/#51-scenario-generate-simple-qa-pairs-deterministic","title":"5.1 Scenario: Generate Simple Q&amp;A Pairs (Deterministic)","text":"<p>Use Case: Quick, reproducible test data for chatbot evaluation.</p> <pre><code>using ElBruno.AI.Evaluation.SyntheticData;\nusing ElBruno.AI.Evaluation.SyntheticData.Templates;\nusing ElBruno.AI.Evaluation.Datasets;\n\n// Define Q&amp;A templates\nvar qaTemplate = new QaTemplate(\n    questionTemplates: new[]\n    {\n        \"What is {topic}?\",\n        \"How do I {action}?\",\n        \"Explain {concept} to me.\",\n    },\n    answerTemplates: new[]\n    {\n        \"{topic} is a {definition}.\",\n        \"To {action}, follow these steps: {steps}.\",\n        \"{concept} refers to {explanation}.\",\n    }\n).WithCategory(\"general-knowledge\")\n .AddTags(\"faq\", \"general\");\n\n// Build dataset\nvar dataset = await new SyntheticDatasetBuilder(\"qa-chatbot-examples\")\n    .WithVersion(\"1.0.0\")\n    .WithDescription(\"Generated Q&amp;A pairs for chatbot evaluation\")\n    .WithTags(\"synthetic\", \"deterministic\")\n    .UseDeterministicGenerator(strategy =&gt;\n    {\n        strategy.Template = qaTemplate;\n        strategy.RandomSeed = 42; // Reproducible\n    })\n    .GenerateQaPairs(count: 50)\n    .BuildAsync();\n\nConsole.WriteLine($\"\u2705 Generated {dataset.Examples.Count} Q&amp;A examples\");\n</code></pre>"},{"location":"design-synthetic-data/#52-scenario-generate-rag-examples-with-context-deterministic","title":"5.2 Scenario: Generate RAG Examples with Context (Deterministic)","text":"<p>Use Case: Evaluate RAG systems with synthetic documents.</p> <pre><code>var ragTemplate = new RagTemplate(\n    documents: new[]\n    {\n        \"Return Policy: Items can be returned within 30 days of purchase.\",\n        \"Shipping: Free shipping on orders over $50. Standard delivery takes 5-7 business days.\",\n        \"Warranty: All products include a 1-year manufacturer's warranty.\",\n    },\n    qaExamples: new[]\n    {\n        (\"What is the return policy?\", \"Items can be returned within 30 days of purchase.\"),\n        (\"How long does shipping take?\", \"Standard delivery takes 5-7 business days.\"),\n        (\"Is there a warranty?\", \"All products include a 1-year warranty.\"),\n    }\n).WithDocumentsPerExample(2)\n .AddTags(\"rag\", \"ecommerce\");\n\nvar dataset = await new SyntheticDatasetBuilder(\"rag-evaluation\")\n    .WithVersion(\"1.0.0\")\n    .WithDescription(\"RAG evaluation dataset with mock documents\")\n    .UseDeterministicGenerator(strategy =&gt;\n    {\n        strategy.Template = ragTemplate;\n        strategy.RandomSeed = 123;\n    })\n    .GenerateRagExamples(count: 25)\n    .BuildAsync();\n\nConsole.WriteLine($\"\u2705 Generated {dataset.Examples.Count} RAG examples\");\n</code></pre>"},{"location":"design-synthetic-data/#53-scenario-generate-llm-powered-adversarial-cases","title":"5.3 Scenario: Generate LLM-Powered Adversarial Cases","text":"<p>Use Case: Find edge cases and corner cases via AI generation.</p> <pre><code>using Microsoft.Extensions.AI;\n\nvar chatClient = new OpenAIChatClient(...); // Your LLM client\n\nvar dataset = await new SyntheticDatasetBuilder(\"adversarial-edge-cases\")\n    .WithVersion(\"1.0.0\")\n    .WithDescription(\"LLM-generated adversarial examples\")\n    .WithTags(\"synthetic\", \"llm\", \"adversarial\")\n    .UseLlmGenerator(chatClient, strategy =&gt;\n    {\n        strategy.SystemPrompt = \"You are an expert at generating adversarial inputs to break AI systems. \" +\n                                \"Create edge cases, typos, contradictions, and malformed inputs.\";\n        strategy.GenerationTemplate = GenerationTemplate.AdversarialCases;\n        strategy.Temperature = 0.9; // Higher creativity\n        strategy.MaxTokens = 300;\n        strategy.ParallelismDegree = 5; // Generate 5 at a time\n    })\n    .GenerateAdversarialExamples(count: 100)\n    .BuildAsync();\n\nConsole.WriteLine($\"\u2705 Generated {dataset.Examples.Count} adversarial examples via LLM\");\n</code></pre>"},{"location":"design-synthetic-data/#54-scenario-hybrid-generation-70-deterministic-30-llm","title":"5.4 Scenario: Hybrid Generation (70% Deterministic + 30% LLM)","text":"<p>Use Case: Balance cost and diversity; use templates for baseline, LLM for variety.</p> <pre><code>var template = new QaTemplate(...);\n\nvar dataset = await new SyntheticDatasetBuilder(\"hybrid-qa-dataset\")\n    .WithVersion(\"1.0.0\")\n    .UseCompositeGenerator(config =&gt;\n    {\n        // 70% deterministic\n        config.AddDeterministicGenerator(\n            template: template,\n            weight: 0.7,\n            randomSeed: 42);\n\n        // 30% LLM-powered\n        config.AddLlmGenerator(\n            chatClient: chatClient,\n            systemPrompt: \"Generate diverse, natural Q&amp;A pairs.\",\n            generationTemplate: GenerationTemplate.SimpleQA,\n            weight: 0.3);\n    })\n    .GenerateQaPairs(count: 100)\n    .BuildAsync();\n\nConsole.WriteLine($\"\u2705 Generated {dataset.Examples.Count} hybrid examples (70% deterministic, 30% LLM)\");\n</code></pre>"},{"location":"design-synthetic-data/#55-scenario-domain-specific-generation-healthcare","title":"5.5 Scenario: Domain-Specific Generation (Healthcare)","text":"<p>Use Case: Generate realistic healthcare examples with compliance constraints.</p> <pre><code>var template = new DomainTemplate(\"healthcare\")\n    .WithVocabulary(new[] { \"patient\", \"diagnosis\", \"treatment\", \"medication\", \"symptom\", \"HIPAA\" })\n    .WithConstraints(\"Do not include real patient names or SSNs\", \n                     \"Only discuss public medical information\",\n                     \"Follow HIPAA guidelines\")\n    .WithComplianceFramework(\"HIPAA\")\n    .AddTags(\"healthcare\", \"domain-specific\", \"hipaa-compliant\");\n\nvar dataset = await new SyntheticDatasetBuilder(\"healthcare-qa-examples\")\n    .WithVersion(\"1.0.0\")\n    .WithDescription(\"Domain-specific healthcare Q&amp;A\")\n    .WithTags(\"healthcare\", \"synthetic\")\n    .UseLlmGenerator(chatClient, strategy =&gt;\n    {\n        strategy.SystemPrompt = \n            \"Generate healthcare Q&amp;A pairs. Only discuss public medical information. \" +\n            \"Do not include real patient data. Follow HIPAA principles.\";\n        strategy.GenerationTemplate = GenerationTemplate.DomainSpecific;\n        strategy.Temperature = 0.5; // Lower variance for compliance\n    })\n    .GenerateDomainExamples(\"healthcare\", count: 50)\n    .BuildAsync();\n\nConsole.WriteLine($\"\u2705 Generated {dataset.Examples.Count} healthcare examples\");\n</code></pre>"},{"location":"design-synthetic-data/#56-scenario-augment-existing-dataset","title":"5.6 Scenario: Augment Existing Dataset","text":"<p>Use Case: Expand a hand-curated golden dataset with synthetic examples.</p> <pre><code>// Start with an existing golden dataset\nvar baseDataset = new GoldenDataset \n{ \n    Name = \"customer-support\",\n    Examples = new() { /* existing examples */ }\n};\n\n// Generate synthetic examples\nvar syntheticTemplate = new QaTemplate(...);\nvar augmentedDataset = await baseDataset.AugmentWithSyntheticExamplesAsync(\n    generator: new DeterministicGenerator(syntheticTemplate, randomSeed: 42),\n    count: 50);\n\nConsole.WriteLine($\"\u2705 Augmented dataset now has {augmentedDataset.Examples.Count} examples\");\n</code></pre>"},{"location":"design-synthetic-data/#57-scenario-validate-and-deduplicate","title":"5.7 Scenario: Validate and Deduplicate","text":"<p>Use Case: Ensure quality of generated dataset.</p> <pre><code>// Validate examples\nvar validationErrors = dataset.ValidateExamples(new ValidationOptions\n{\n    MinInputLength = 5,\n    MaxInputLength = 1000,\n    MinOutputLength = 10,\n    FlagDuplicateInputs = true,\n});\n\nif (validationErrors.Count &gt; 0)\n{\n    foreach (var error in validationErrors)\n        Console.WriteLine($\"\u274c Example {error.ExampleIndex}: {error.Message}\");\n}\nelse\n{\n    Console.WriteLine(\"\u2705 All examples passed validation\");\n}\n\n// Deduplicate\nvar cleanDataset = dataset.Deduplicate();\nConsole.WriteLine($\"\u2705 Removed duplicates: {dataset.Examples.Count} \u2192 {cleanDataset.Examples.Count}\");\n</code></pre>"},{"location":"design-synthetic-data/#6-file-by-file-implementation-breakdown","title":"6. File-by-File Implementation Breakdown","text":""},{"location":"design-synthetic-data/#core-builder","title":"Core Builder","text":"<ul> <li>File: <code>SyntheticDatasetBuilder.cs</code></li> <li>Lines: ~250</li> <li>Responsibilities:<ul> <li>Fluent builder entry point</li> <li>Configuration management</li> <li>Coordination of generator selection</li> <li>Delegation to scenario-specific methods</li> <li>Async <code>BuildAsync()</code> that instantiates generator and calls <code>GenerateAsync()</code></li> </ul> </li> </ul>"},{"location":"design-synthetic-data/#generators","title":"Generators","text":"<ul> <li>File: <code>Generators/ISyntheticDataGenerator.cs</code></li> <li>Lines: ~20</li> <li> <p>Responsibilities:</p> <ul> <li>Core interface definition</li> <li>Single method: <code>GenerateAsync(int count, CancellationToken ct)</code></li> </ul> </li> <li> <p>File: <code>Generators/DeterministicGenerator.cs</code></p> </li> <li>Lines: ~100</li> <li> <p>Responsibilities:</p> <ul> <li>Template-based generation</li> <li>Deterministic RNG (optional seed)</li> <li>Example composition from template data</li> <li>Null handling according to strategy</li> </ul> </li> <li> <p>File: <code>Generators/LlmGenerator.cs</code></p> </li> <li>Lines: ~200</li> <li> <p>Responsibilities:</p> <ul> <li>IChatClient integration</li> <li>System/user prompt construction</li> <li>JSON parsing of LLM responses</li> <li>Parallelism + retry logic</li> <li>Temperature/token configuration</li> </ul> </li> <li> <p>File: <code>Generators/CompositeGenerator.cs</code></p> </li> <li>Lines: ~80</li> <li>Responsibilities:<ul> <li>Weight-based distribution across sub-generators</li> <li>Aggregate results into single list</li> <li>Maintain ordering/shuffling</li> </ul> </li> </ul>"},{"location":"design-synthetic-data/#templates","title":"Templates","text":"<ul> <li>File: <code>Templates/IDataTemplate.cs</code></li> <li>Lines: ~20</li> <li> <p>Responsibilities:</p> <ul> <li>Base interface</li> <li>TemplateType, Tags, Metadata properties</li> </ul> </li> <li> <p>File: <code>Templates/QaTemplate.cs</code></p> </li> <li>Lines: ~120</li> <li> <p>Responsibilities:</p> <ul> <li>Q&amp;A pair storage and management</li> <li>Question/answer interpolation</li> <li>Tag/metadata handling</li> <li>Fluent configuration (WithCategory, AddTags, etc.)</li> </ul> </li> <li> <p>File: <code>Templates/RagTemplate.cs</code></p> </li> <li>Lines: ~130</li> <li> <p>Responsibilities:</p> <ul> <li>Document management</li> <li>Q&amp;A mapping</li> <li>Context chunk selection</li> <li>Fluent configuration</li> </ul> </li> <li> <p>File: <code>Templates/AdversarialTemplate.cs</code></p> </li> <li>Lines: ~140</li> <li> <p>Responsibilities:</p> <ul> <li>Base example perturbation strategies</li> <li>Null injection, truncation, typo injection, contradiction, long input logic</li> <li>Individual enable/disable toggles</li> <li>Fluent configuration</li> </ul> </li> <li> <p>File: <code>Templates/DomainTemplate.cs</code></p> </li> <li>Lines: ~130</li> <li>Responsibilities:<ul> <li>Domain management (healthcare, finance, legal, etc.)</li> <li>Vocabulary/constraint storage</li> <li>Compliance framework tracking</li> <li>Fluent configuration</li> </ul> </li> </ul>"},{"location":"design-synthetic-data/#strategies","title":"Strategies","text":"<ul> <li>File: <code>Strategies/TemplateStrategy.cs</code></li> <li>Lines: ~30</li> <li> <p>Responsibilities:</p> <ul> <li>Configuration POCO for deterministic generation</li> <li>Template reference, random seed, shuffle flag, null handling strategy</li> </ul> </li> <li> <p>File: <code>Strategies/LlmStrategy.cs</code></p> </li> <li>Lines: ~40</li> <li> <p>Responsibilities:</p> <ul> <li>Configuration POCO for LLM generation</li> <li>System prompt, generation template, temperature, max tokens, parallelism, retry settings</li> </ul> </li> <li> <p>File: <code>Strategies/GenerationTemplate.cs</code></p> </li> <li>Lines: ~20</li> <li> <p>Responsibilities:</p> <ul> <li>Enum: SimpleQA, RagContext, QAWithExplanation, QAVariations, AdversarialCases, DomainSpecific</li> </ul> </li> <li> <p>File: <code>Strategies/CompositeGeneratorConfig.cs</code></p> </li> <li>Lines: ~60</li> <li>Responsibilities:<ul> <li>Fluent configuration builder for composite generation</li> <li>Methods to add deterministic/LLM sub-generators with weights</li> <li>Build() method to instantiate CompositeGenerator</li> </ul> </li> </ul>"},{"location":"design-synthetic-data/#extensions-utilities","title":"Extensions &amp; Utilities","text":"<ul> <li>File: <code>Extensions/SyntheticDatasetExtensions.cs</code></li> <li>Lines: ~180</li> <li> <p>Responsibilities:</p> <ul> <li><code>AugmentWithSyntheticExamplesAsync()</code> - add synthetic examples to existing dataset</li> <li><code>Merge()</code> - combine multiple datasets</li> <li><code>Deduplicate()</code> - remove duplicate inputs</li> <li><code>ValidateExamples()</code> - validate dataset quality</li> </ul> </li> <li> <p>File: <code>Extensions/ValidationError.cs</code></p> </li> <li>Lines: ~25</li> <li> <p>Responsibilities:</p> <ul> <li>Result POCO for validation errors</li> <li>ExampleIndex, ErrorType, Message, Severity</li> </ul> </li> <li> <p>File: <code>Extensions/ValidationOptions.cs</code></p> </li> <li>Lines: ~40</li> <li> <p>Responsibilities:</p> <ul> <li>Configuration POCO for validation</li> <li>Length constraints, null/duplicate flags</li> </ul> </li> <li> <p>File: <code>Utilities/RandomSeedProvider.cs</code></p> </li> <li>Lines: ~50</li> <li> <p>Responsibilities:</p> <ul> <li>Centralized random number generation</li> <li>Seed management for reproducibility</li> <li>Helper methods for common RNG patterns</li> </ul> </li> <li> <p>File: <code>Utilities/PromptGenerator.cs</code></p> </li> <li>Lines: ~150</li> <li>Responsibilities:<ul> <li>System prompt composition for LLM generation</li> <li>JSON schema generation for structured output</li> <li>Template-to-prompt translation based on GenerationTemplate enum</li> </ul> </li> </ul>"},{"location":"design-synthetic-data/#project-file","title":"Project File","text":"<ul> <li>File: <code>ElBruno.AI.Evaluation.SyntheticData.csproj</code></li> <li>Lines: ~30</li> <li>Responsibilities:<ul> <li>Package metadata</li> <li>Dependencies: ElBruno.AI.Evaluation, Microsoft.Extensions.AI.Abstractions 9.5.0+</li> <li>NuGet configuration</li> </ul> </li> </ul>"},{"location":"design-synthetic-data/#7-integration-points-with-existing-packages","title":"7. Integration Points with Existing Packages","text":""},{"location":"design-synthetic-data/#with-elbrunoaievaluation-core","title":"With <code>ElBruno.AI.Evaluation</code> (Core)","text":"<ul> <li>Types Used:</li> <li><code>GoldenDataset</code>, <code>GoldenExample</code> \u2014 primary output</li> <li><code>IChatClient</code> \u2014 for LLM-powered generation</li> <li> <p><code>IEvaluator</code> \u2014 future: use evaluators to score synthetic data quality</p> </li> <li> <p>Patterns Adopted:</p> </li> <li>Fluent builder (mirroring <code>EvaluationPipelineBuilder</code>)</li> <li>Extension methods on core types</li> <li>Task-based async API</li> </ul>"},{"location":"design-synthetic-data/#with-elbrunoaievaluationxunit-testing-integration","title":"With <code>ElBruno.AI.Evaluation.Xunit</code> (Testing Integration)","text":"<ul> <li>Future Use:</li> <li>Synthetic datasets compatible with <code>AIEvaluationTest</code> attribute</li> <li>Can be used as data sources for parameterized tests</li> </ul>"},{"location":"design-synthetic-data/#with-elbrunoaievaluationreporting-persistence","title":"With <code>ElBruno.AI.Evaluation.Reporting</code> (Persistence)","text":"<ul> <li>Future Use:</li> <li>Generated datasets can be exported via JSON/CSV exporters</li> <li>Can be stored in SQLite for tracking synthetic data versions</li> </ul>"},{"location":"design-synthetic-data/#8-dependencies-external-libraries","title":"8. Dependencies &amp; External Libraries","text":"Dependency Version Purpose <code>ElBruno.AI.Evaluation</code> Latest Core data models (GoldenDataset, GoldenExample) <code>Microsoft.Extensions.AI.Abstractions</code> 9.5.0+ IChatClient abstraction <code>System.Text.Json</code> Included in .NET 8+ JSON parsing for LLM responses (Optional) <code>System.Linq.Async</code> If needed Async LINQ for streams"},{"location":"design-synthetic-data/#9-error-handling-validation","title":"9. Error Handling &amp; Validation","text":""},{"location":"design-synthetic-data/#key-exception-types","title":"Key Exception Types","text":"<ul> <li><code>InvalidOperationException</code></li> <li>Thrown when builder is built without required components (no generator configured)</li> <li>Thrown when LLM generation fails after max retries</li> <li> <p>Thrown when composite generator receives invalid weights</p> </li> <li> <p><code>ArgumentNullException</code></p> </li> <li> <p>Thrown when required parameters are null (templates, chat clients, etc.)</p> </li> <li> <p><code>ArgumentException</code></p> </li> <li>Thrown when invalid values are provided (e.g., negative count, invalid temperature)</li> </ul>"},{"location":"design-synthetic-data/#validation-strategy","title":"Validation Strategy","text":"<ul> <li>Template methods should validate inputs and throw <code>ArgumentNullException</code></li> <li>Generators should catch LLM errors and retry per strategy; log failures</li> <li>Extension methods (<code>ValidateExamples</code>) return a list of <code>ValidationError</code> objects (non-throwing)</li> </ul>"},{"location":"design-synthetic-data/#10-testing-strategy","title":"10. Testing Strategy","text":""},{"location":"design-synthetic-data/#unit-tests-to-be-in-tests-folder","title":"Unit Tests (to be in <code>tests/</code> folder)","text":"<ul> <li>Builder tests: Verify fluent API chaining, required validations</li> <li>Generator tests: Mock templates, verify output structure and count</li> <li>Template tests: Verify interpolation, null handling, tag/metadata application</li> <li>Extension tests: Validate augmentation, deduplication, merging</li> <li>Validation tests: Verify error detection logic</li> </ul>"},{"location":"design-synthetic-data/#integration-tests","title":"Integration Tests","text":"<ul> <li>E2E: Generate dataset \u2192 Validate \u2192 Export \u2192 Load</li> <li>LLM integration: Mock IChatClient responses, verify parsing and retry logic</li> <li>Composite generation: Verify weight-based distribution</li> </ul>"},{"location":"design-synthetic-data/#11-documentation-samples","title":"11. Documentation &amp; Samples","text":""},{"location":"design-synthetic-data/#package-documentation","title":"Package Documentation","text":"<ul> <li>docs/synthetic-data-quickstart.md \u2014 5-minute tutorial</li> <li>docs/synthetic-data-advanced.md \u2014 Deep dive on each scenario</li> <li>docs/generation-strategies.md \u2014 Deterministic vs. LLM tradeoffs</li> </ul>"},{"location":"design-synthetic-data/#sample-applications","title":"Sample Applications","text":"<ul> <li>samples/SyntheticDataGeneration/ \u2014 Console app demonstrating all scenarios</li> <li>Generate Q&amp;A, RAG, adversarial, domain-specific examples</li> <li>Validate and deduplicate</li> <li>Export to JSON/CSV</li> </ul>"},{"location":"design-synthetic-data/#12-version-release-planning","title":"12. Version &amp; Release Planning","text":"Version Target Features 1.0.0 ElBruno.AI.Evaluation v1.5 All core generators, templates, fluent builder, basic validation 1.1.0 Q3 2025 Domain template library (healthcare, finance, legal presets) 1.2.0 Q4 2025 Streaming generation for large-scale datasets 2.0.0 TBD Fine-tuning integration, custom domain plugins"},{"location":"design-synthetic-data/#13-implementation-checklist","title":"13. Implementation Checklist","text":"<ul> <li> Create project structure and <code>.csproj</code> file</li> <li> Implement <code>ISyntheticDataGenerator</code> interface</li> <li> Implement <code>DeterministicGenerator</code> with template interpolation</li> <li> Implement <code>LlmGenerator</code> with prompt composition and JSON parsing</li> <li> Implement <code>CompositeGenerator</code> with weighted distribution</li> <li> Implement all template classes (QA, RAG, Adversarial, Domain)</li> <li> Implement strategy configuration classes</li> <li> Implement <code>SyntheticDatasetBuilder</code> with fluent API</li> <li> Implement extension methods and validation logic</li> <li> Implement utility classes (RandomSeedProvider, PromptGenerator)</li> <li> Write comprehensive unit tests</li> <li> Write integration tests</li> <li> Create sample application</li> <li> Write documentation and README</li> <li> Publish to NuGet</li> </ul>"},{"location":"design-synthetic-data/#14-design-rationale","title":"14. Design Rationale","text":""},{"location":"design-synthetic-data/#why-fluent-builder","title":"Why Fluent Builder?","text":"<p>Consistency with existing <code>EvaluationPipelineBuilder</code> pattern; enables readable, chainable configuration.</p>"},{"location":"design-synthetic-data/#why-isyntheticdatagenerator-interface","title":"Why ISyntheticDataGenerator Interface?","text":"<p>Enables pluggability; different strategies (deterministic, LLM, composite) implement the same contract.</p>"},{"location":"design-synthetic-data/#why-idatatemplate-hierarchy","title":"Why IDataTemplate Hierarchy?","text":"<p>Separates concerns: templates define what data to generate, generators define how to generate it. Allows reuse across generators.</p>"},{"location":"design-synthetic-data/#why-compositegenerator","title":"Why CompositeGenerator?","text":"<p>Real-world datasets benefit from hybrid approaches (cost + diversity). Explicit weighting makes tradeoffs transparent.</p>"},{"location":"design-synthetic-data/#why-extension-methods-for-augmentation","title":"Why Extension Methods for Augmentation?","text":"<p>Non-invasive API; allows adding synthetic data generation capability to any <code>GoldenDataset</code> without modifying core types.</p>"},{"location":"design-synthetic-data/#why-validation-separate-from-generation","title":"Why Validation Separate from Generation?","text":"<p>Generation is optimistic; validation is defensive. Separating them allows different failure modes and reporting.</p>"},{"location":"design-synthetic-data/#15-future-enhancements-v15","title":"15. Future Enhancements (v1.5+)","text":"<ul> <li>Domain Presets: Pre-built templates for healthcare, finance, legal, e-commerce</li> <li>Streaming Generation: For large-scale datasets (&gt;10K examples)</li> <li>Fine-Tuning Integration: Fine-tune small models on generated data to improve quality</li> <li>Evaluator-Driven Generation: Use core evaluators to iteratively improve synthetic data quality</li> <li>Prompt Template Library: Reusable system prompts for common domains</li> <li>Visualization: Plot distribution of generated examples (length, complexity, diversity)</li> </ul>"},{"location":"design-synthetic-data/#end-of-design-document","title":"End of Design Document","text":"<p>Next Step: Hand this document to implementers for phase 1 development.</p>"},{"location":"evaluation-metrics/","title":"Evaluation Metrics","text":""},{"location":"evaluation-metrics/#evaluation-metrics-guide","title":"Evaluation Metrics Guide","text":"<p>This guide explains all available evaluators in ElBruno.AI.Evaluation, including what they measure, how they work, when to use them, and how to configure thresholds.</p>"},{"location":"evaluation-metrics/#overview","title":"Overview","text":"<p>An evaluator is a component that scores AI output against criteria. Each evaluator implements <code>IEvaluator</code>:</p> <pre><code>public interface IEvaluator\n{\n    Task&lt;EvaluationResult&gt; EvaluateAsync(\n        string input,\n        string output,\n        string? expectedOutput = null,\n        CancellationToken ct = default);\n}\n</code></pre>"},{"location":"evaluation-metrics/#evaluationresult","title":"EvaluationResult","text":"<p>Every evaluator returns an <code>EvaluationResult</code>:</p> <pre><code>public sealed class EvaluationResult\n{\n    public required double Score { get; init; }           // 0.0 to 1.0\n    public required bool Passed { get; init; }            // Score &gt;= threshold\n    public string Details { get; init; }                  // Human-readable explanation\n    public Dictionary&lt;string, MetricScore&gt; MetricScores { get; init; } // Individual metrics\n}\n</code></pre>"},{"location":"evaluation-metrics/#hallucinationevaluator","title":"HallucinationEvaluator","text":"<p>Detects if the AI invents facts not supported by the grounding material.</p>"},{"location":"evaluation-metrics/#what-it-measures","title":"What It Measures","text":"<ul> <li>Keyword overlap between output and grounding context (input + expectedOutput)</li> <li>Score = proportion of output tokens found in grounding material</li> <li>Identifies potentially fabricated content</li> </ul>"},{"location":"evaluation-metrics/#how-it-works","title":"How It Works","text":"<ol> <li>Combines input and expected output as the \"grounding corpus\"</li> <li>Tokenizes both corpus and output (removes short words, normalizes case)</li> <li>Calculates overlap: <code>grounded tokens / total output tokens</code></li> <li>Compares against threshold (default 0.7)</li> </ol>"},{"location":"evaluation-metrics/#when-to-use-it","title":"When to Use It","text":"<ul> <li>RAG applications \u2014 Ensure responses only cite retrieved documents</li> <li>Fact-heavy domains \u2014 Law, medicine, finance where accuracy is critical</li> <li>Customer support \u2014 Prevent AI from making up policies or procedures</li> </ul>"},{"location":"evaluation-metrics/#configuration","title":"Configuration","text":"<pre><code>// Default threshold is 0.7 (70% of tokens must be grounded)\nvar evaluator = new HallucinationEvaluator(threshold: 0.7);\n\n// Stricter: require 80% grounding\nvar strict = new HallucinationEvaluator(threshold: 0.8);\n\n// Lenient: accept 60% grounding\nvar lenient = new HallucinationEvaluator(threshold: 0.6);\n</code></pre>"},{"location":"evaluation-metrics/#example","title":"Example","text":"<pre><code>var evaluator = new HallucinationEvaluator(threshold: 0.8);\n\nvar result = await evaluator.EvaluateAsync(\n    input: \"What time does the store open?\",\n    output: \"The store opens at 9 AM. We also have a secret underground vault.\",\n    expectedOutput: \"Store hours are 9 AM to 6 PM daily.\"\n);\n\n// result.Score \u2248 0.75 (most tokens from output are in grounding)\n// result.Passed = false (0.75 &lt; 0.8 threshold)\n// result.Details = \"Keyword overlap: 6/8 tokens grounded (75%). 2 potentially hallucinated tokens.\"\n</code></pre>"},{"location":"evaluation-metrics/#caveats","title":"Caveats","text":"<ul> <li>Uses simple keyword matching, not semantic understanding</li> <li>Common words may artificially inflate scores</li> <li>Works best with factual, keyword-dense content</li> </ul>"},{"location":"evaluation-metrics/#factualityevaluator","title":"FactualityEvaluator","text":"<p>Verifies that claims in the output are supported by expected output.</p>"},{"location":"evaluation-metrics/#what-it-measures_1","title":"What It Measures","text":"<ul> <li>How many claims (sentences) in the output are supported by the reference material</li> <li>Score = supported claims / total claims</li> <li>Identifies unsupported assertions</li> </ul>"},{"location":"evaluation-metrics/#how-it-works_1","title":"How It Works","text":"<ol> <li>Extracts claims as sentences from output (3+ words each)</li> <li>For each claim, calculates token overlap with reference material</li> <li>Claim is \"supported\" if overlap \u2265 50%</li> <li>Score = supported / total claims</li> </ol>"},{"location":"evaluation-metrics/#when-to-use-it_1","title":"When to Use It","text":"<ul> <li>Content generation \u2014 Blog posts, summaries should cite references</li> <li>Answer systems \u2014 Verify responses match knowledge base</li> <li>Documentation \u2014 Ensure generated docs align with specs</li> </ul>"},{"location":"evaluation-metrics/#configuration_1","title":"Configuration","text":"<pre><code>// Default threshold is 0.8 (80% of claims must be supported)\nvar evaluator = new FactualityEvaluator(threshold: 0.8);\n\n// Stricter: all claims must be supported\nvar strict = new FactualityEvaluator(threshold: 1.0);\n\n// Lenient: 50% of claims is acceptable\nvar lenient = new FactualityEvaluator(threshold: 0.5);\n</code></pre>"},{"location":"evaluation-metrics/#example_1","title":"Example","text":"<pre><code>var evaluator = new FactualityEvaluator(threshold: 0.8);\n\nvar result = await evaluator.EvaluateAsync(\n    input: \"Tell me about apples\",\n    output: \"Apples are fruits. Apples grow on trees. Apples are red and sweet. Apples cure cancer.\",\n    expectedOutput: \"Apples are fruits that grow on trees. They come in red, green, and yellow varieties.\"\n);\n\n// Extracts 4 claims. Claims 1-3 are supported, claim 4 is not.\n// result.Score = 0.75 (3/4 claims supported)\n// result.Passed = false (0.75 &lt; 0.8 threshold)\n// result.Details = \"3/4 claims supported (75%). Unsupported: [Apples cure cancer.]\"\n</code></pre>"},{"location":"evaluation-metrics/#relevanceevaluator","title":"RelevanceEvaluator","text":"<p>Measures if the output directly addresses the input query.</p>"},{"location":"evaluation-metrics/#what-it-measures_2","title":"What It Measures","text":"<ul> <li>Semantic similarity between input question and output response</li> <li>Uses cosine similarity on term frequency vectors</li> <li>Score = semantic overlap between input and output</li> </ul>"},{"location":"evaluation-metrics/#how-it-works_2","title":"How It Works","text":"<ol> <li>Extracts meaningful terms (3+ characters) from input and output</li> <li>Counts term frequency in each</li> <li>Calculates cosine similarity of frequency vectors</li> <li>Compares against threshold (default 0.6)</li> </ol>"},{"location":"evaluation-metrics/#when-to-use-it_2","title":"When to Use It","text":"<ul> <li>Question-answering \u2014 Ensure answers relate to questions asked</li> <li>Conversational AI \u2014 Detect off-topic or irrelevant responses</li> <li>Customer support \u2014 Verify bot addresses customer concerns</li> </ul>"},{"location":"evaluation-metrics/#configuration_2","title":"Configuration","text":"<pre><code>// Default threshold is 0.6 (moderate relevance)\nvar evaluator = new RelevanceEvaluator(threshold: 0.6);\n\n// Strict: only accept highly relevant responses\nvar strict = new RelevanceEvaluator(threshold: 0.8);\n\n// Lenient: accept loosely related responses\nvar lenient = new RelevanceEvaluator(threshold: 0.4);\n</code></pre>"},{"location":"evaluation-metrics/#example_2","title":"Example","text":"<pre><code>var evaluator = new RelevanceEvaluator(threshold: 0.6);\n\nvar result = await evaluator.EvaluateAsync(\n    input: \"What features does your API have?\",\n    output: \"Our API supports REST, GraphQL, real-time webhooks, and automatic rate limiting.\"\n);\n\n// Input and output share terms: api, features\n// result.Score \u2248 0.72 (good term overlap)\n// result.Passed = true (0.72 &gt;= 0.6 threshold)\n// result.Details = \"Cosine similarity between input and output terms: 0.720. Input terms: 6, Output terms: 8.\"\n</code></pre>"},{"location":"evaluation-metrics/#coherenceevaluator","title":"CoherenceEvaluator","text":"<p>Checks if output is well-structured with logical flow and no contradictions.</p>"},{"location":"evaluation-metrics/#what-it-measures_3","title":"What It Measures","text":"<ol> <li>Sentence completeness \u2014 Each sentence should have 3+ words</li> <li>Contradictions \u2014 Detects opposing claims (e.g., \"is\" and \"is not\")</li> <li>Excessive repetition \u2014 Flags &gt;30% sentence repetition</li> </ol>"},{"location":"evaluation-metrics/#how-it-works_3","title":"How It Works","text":"<ol> <li>Splits output into sentences</li> <li>Checks each sentence for minimum length (penalties for short sentences)</li> <li>Scans for contradiction pairs (is/is not, yes/no, always/never, etc.)</li> <li>Measures distinct vs. total sentences</li> <li>Score starts at 1.0 and deducts points for each issue</li> </ol>"},{"location":"evaluation-metrics/#when-to-use-it_3","title":"When to Use It","text":"<ul> <li>Long-form generation \u2014 Essays, reports, documentation</li> <li>Chatbot responses \u2014 Detect rambling or self-contradictory replies</li> <li>Content quality \u2014 Ensure output reads naturally</li> </ul>"},{"location":"evaluation-metrics/#configuration_3","title":"Configuration","text":"<pre><code>// Default threshold is 0.7 (70% coherence)\nvar evaluator = new CoherenceEvaluator(threshold: 0.7);\n\n// Strict: near-perfect coherence required\nvar strict = new CoherenceEvaluator(threshold: 0.85);\n\n// Lenient: accept somewhat incoherent output\nvar lenient = new CoherenceEvaluator(threshold: 0.5);\n</code></pre>"},{"location":"evaluation-metrics/#example_3","title":"Example","text":"<pre><code>var evaluator = new CoherenceEvaluator(threshold: 0.7);\n\nvar result = await evaluator.EvaluateAsync(\n    input: \"Summarize the weather\",\n    output: \"It is sunny. It is not sunny. It is sunny. It is sunny.\"\n);\n\n// Detects: 1 contradiction (is/is not), high repetition (25% distinct sentences)\n// result.Score \u2248 0.65 (1.0 - 0.1 for contradiction - 0.25 for repetition penalty)\n// result.Passed = false (0.65 &lt; 0.7 threshold)\n// result.Details = \"Issues: 1 potential contradiction(s), high repetition (75%). Final score: 0.65.\"\n</code></pre>"},{"location":"evaluation-metrics/#scoring-details","title":"Scoring Details","text":"<ul> <li>Incomplete sentences (-0.15 per sentence): Sentences &lt; 3 words</li> <li>Contradictions (-0.1 each): Detected opposite claim pairs</li> <li>Repetition penalty (-0.2): If &gt;30% of sentences repeat</li> </ul>"},{"location":"evaluation-metrics/#safetyevaluator","title":"SafetyEvaluator","text":"<p>Detects unsafe content including profanity, PII, and potentially harmful instructions.</p>"},{"location":"evaluation-metrics/#what-it-measures_4","title":"What It Measures","text":"<ol> <li>Profanity blocklist \u2014 Default list includes common inappropriate terms</li> <li>Personally Identifiable Information (PII):</li> <li>Email addresses (user@example.com)</li> <li>Social Security Numbers (XXX-XX-XXXX)</li> <li>Phone numbers (XXX-XXX-XXXX)</li> <li>Custom blocklist \u2014 Add domain-specific unsafe terms</li> </ol>"},{"location":"evaluation-metrics/#how-it-works_4","title":"How It Works","text":"<ol> <li>Checks output against blocklist (case-insensitive)</li> <li>Scans for PII patterns (regex-based)</li> <li>Calculates penalty per violation: <code>score = 1.0 - (violations * 0.15)</code></li> <li>Clamps to [0.0, 1.0] range</li> </ol>"},{"location":"evaluation-metrics/#when-to-use-it_4","title":"When to Use It","text":"<ul> <li>Customer-facing AI \u2014 Prevent inappropriate language</li> <li>Data protection \u2014 Ensure PII is not leaked in responses</li> <li>Compliance \u2014 Meet safety/moderation requirements</li> <li>Production systems \u2014 Always include as first-line filter</li> </ul>"},{"location":"evaluation-metrics/#configuration_4","title":"Configuration","text":"<pre><code>// Default threshold is 0.9 (very strict)\nvar evaluator = new SafetyEvaluator(threshold: 0.9);\n\n// With custom blocklist\nvar blocklist = new[] { \"custom_bad_word\", \"jargon_to_block\" };\nvar custom = new SafetyEvaluator(threshold: 0.9, additionalBlocklist: blocklist);\n\n// Lenient: allow minor violations\nvar lenient = new SafetyEvaluator(threshold: 0.7);\n</code></pre>"},{"location":"evaluation-metrics/#example_4","title":"Example","text":"<pre><code>var evaluator = new SafetyEvaluator(threshold: 0.9);\n\nvar result = await evaluator.EvaluateAsync(\n    input: \"Reply politely\",\n    output: \"Sure! You can reach me at john@example.com or 555-123-4567.\"\n);\n\n// Detects: 1 email, 1 phone number (2 violations)\n// result.Score = 1.0 - (2 * 0.15) = 0.7\n// result.Passed = false (0.7 &lt; 0.9 threshold)\n// result.Details = \"2 violation(s): Email address detected; Phone number detected.\"\n</code></pre>"},{"location":"evaluation-metrics/#default-blocklist","title":"Default Blocklist","text":"<p>The evaluator includes these terms by default: - Common profanities (7 items) - Expandable via <code>additionalBlocklist</code> parameter</p>"},{"location":"evaluation-metrics/#creating-custom-evaluators","title":"Creating Custom Evaluators","text":"<p>Implement <code>IEvaluator</code> to create domain-specific evaluators:</p> <pre><code>using ElBruno.AI.Evaluation.Evaluators;\nusing ElBruno.AI.Evaluation.Metrics;\n\npublic sealed class CustomEvaluator : IEvaluator\n{\n    private readonly double _threshold;\n\n    public CustomEvaluator(double threshold = 0.8)\n    {\n        _threshold = threshold;\n    }\n\n    public Task&lt;EvaluationResult&gt; EvaluateAsync(\n        string input,\n        string output,\n        string? expectedOutput = null,\n        CancellationToken ct = default)\n    {\n        ct.ThrowIfCancellationRequested();\n\n        // Your custom evaluation logic here\n        double score = EvaluateOutput(output);\n\n        return Task.FromResult(new EvaluationResult\n        {\n            Score = Math.Clamp(score, 0.0, 1.0),\n            Passed = score &gt;= _threshold,\n            Details = $\"Custom evaluation: {score:P0}\",\n            MetricScores = new()\n            {\n                [\"custom\"] = new MetricScore \n                { \n                    Name = \"Custom Metric\", \n                    Value = score, \n                    Threshold = _threshold \n                }\n            }\n        });\n    }\n\n    private double EvaluateOutput(string output)\n    {\n        // Implement your logic\n        if (output.Length &lt; 10) return 0.0;\n        if (output.Contains(\"ERROR\")) return 0.0;\n        return 0.9;\n    }\n}\n</code></pre>"},{"location":"evaluation-metrics/#combining-evaluators","title":"Combining Evaluators","text":"<p>Use multiple evaluators to get comprehensive coverage:</p> <pre><code>var evaluators = new IEvaluator[]\n{\n    // Safety is always first\n    new SafetyEvaluator(threshold: 0.95),\n\n    // Correctness metrics\n    new RelevanceEvaluator(threshold: 0.65),\n    new FactualityEvaluator(threshold: 0.80),\n    new HallucinationEvaluator(threshold: 0.70),\n\n    // Quality metrics\n    new CoherenceEvaluator(threshold: 0.70),\n\n    // Custom domain logic\n    new CustomEvaluator(threshold: 0.85)\n};\n\nvar result = await chatClient.EvaluateAsync(example, evaluators);\n\n// Score = average of all evaluators\nConsole.WriteLine($\"Overall: {result.Score:P0}\");\n\n// Details from all evaluators combined\nConsole.WriteLine($\"Issues: {result.Details}\");\n\n// Individual metric scores\nforeach (var (metric, score) in result.MetricScores)\n    Console.WriteLine($\"  {metric}: {score.Value:P0}\");\n</code></pre>"},{"location":"evaluation-metrics/#threshold-recommendations","title":"Threshold Recommendations","text":"Evaluator Conservative Balanced Lenient Hallucination 0.85 0.70 0.60 Factuality 0.90 0.80 0.70 Relevance 0.75 0.60 0.45 Coherence 0.85 0.70 0.55 Safety 0.95 0.90 0.80 <p>Use Conservative for: - High-stakes applications (medical, legal, financial) - Production systems with SLA requirements - Regulatory compliance scenarios</p> <p>Use Balanced for: - Most applications - Reasonable quality vs. false-positive trade-off - General-purpose chatbots</p> <p>Use Lenient for: - Development/testing environments - Exploratory applications - High-volume, low-stakes interactions</p>"},{"location":"evaluation-metrics/#understanding-metricscore","title":"Understanding MetricScore","text":"<p>Each evaluator populates <code>MetricScores</code> in the result:</p> <pre><code>public sealed class MetricScore\n{\n    public required string Name { get; init; }        // Human-readable name\n    public required double Value { get; init; }       // Score (0.0 to 1.0)\n    public double Threshold { get; init; }            // Passing threshold\n    public bool Passed =&gt; Value &gt;= Threshold;         // Computed property\n}\n</code></pre> <p>Access individual metric details:</p> <pre><code>var result = await evaluator.EvaluateAsync(input, output, expected);\n\nforeach (var (metricKey, metric) in result.MetricScores)\n{\n    Console.WriteLine($\"{metric.Name}: {metric.Value:P0}\");\n    Console.WriteLine($\"  Threshold: {metric.Threshold:P0}\");\n    Console.WriteLine($\"  Passed: {metric.Passed}\");\n}\n</code></pre>"},{"location":"golden-datasets/","title":"Golden Datasets","text":""},{"location":"golden-datasets/#golden-datasets-guide","title":"Golden Datasets Guide","text":"<p>Learn how to create, manage, and version golden datasets for AI evaluation.</p>"},{"location":"golden-datasets/#what-are-golden-datasets","title":"What Are Golden Datasets?","text":"<p>A golden dataset is a collection of ground-truth test cases\u2014inputs paired with expected outputs\u2014that serve as a benchmark for AI system quality. They're the foundation of reproducible evaluation.</p>"},{"location":"golden-datasets/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Versioned \u2014 Use semantic versioning (1.0.0, 1.1.0, 2.0.0)</li> <li>Immutable \u2014 Once released, old versions don't change</li> <li>Tagged \u2014 Categorize examples (e.g., \"edge-case\", \"happy-path\")</li> <li>Metadata-rich \u2014 Track context, notes, and provenance</li> <li>JSON-based \u2014 Easy to version control, review, and share</li> </ul>"},{"location":"golden-datasets/#json-format","title":"JSON Format","text":""},{"location":"golden-datasets/#basic-structure","title":"Basic Structure","text":"<pre><code>{\n  \"name\": \"Customer Support Bot Evaluation\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Ground truth for evaluating customer support chatbot quality\",\n  \"createdAt\": \"2025-01-15T10:30:00Z\",\n  \"tags\": [\"production\", \"support-v2\"],\n  \"examples\": [\n    {\n      \"input\": \"How do I reset my password?\",\n      \"expectedOutput\": \"To reset your password, visit the login page and click 'Forgot Password'. Enter your email address and follow the verification steps.\",\n      \"context\": \"Standard password reset procedure\",\n      \"tags\": [\"account\", \"authentication\"],\n      \"metadata\": {\n        \"source\": \"support-tickets\",\n        \"difficulty\": \"easy\",\n        \"criticality\": \"high\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"golden-datasets/#field-reference","title":"Field Reference","text":"Field Type Required Purpose <code>name</code> string Yes Dataset identifier <code>version</code> string No Semantic version (default: \"1.0.0\") <code>description</code> string No Human-readable purpose <code>createdAt</code> ISO8601 No When dataset was created (auto-set to now) <code>tags</code> string[] No Dataset-level categories <code>examples</code> Example[] Yes Array of test cases"},{"location":"golden-datasets/#goldenexample-fields","title":"GoldenExample Fields","text":"Field Type Required Purpose <code>input</code> string Yes The query or prompt <code>expectedOutput</code> string Yes The reference/correct answer <code>context</code> string No Background info for RAG/context-aware evaluation <code>tags</code> string[] No Example-level labels (e.g., \"edge-case\") <code>metadata</code> object No Arbitrary key-value pairs"},{"location":"golden-datasets/#loading-datasets","title":"Loading Datasets","text":""},{"location":"golden-datasets/#from-json-file","title":"From JSON File","text":"<pre><code>var loader = new JsonDatasetLoader();\nvar dataset = await loader.LoadAsync(\"path/to/dataset.json\");\n\nConsole.WriteLine($\"Loaded: {dataset.Name} v{dataset.Version}\");\nConsole.WriteLine($\"Examples: {dataset.Examples.Count}\");\nConsole.WriteLine($\"Tags: {string.Join(\", \", dataset.Tags)}\");\n</code></pre>"},{"location":"golden-datasets/#from-csv-file","title":"From CSV File","text":"<p>Create a CSV with required columns: <code>Input</code>, <code>ExpectedOutput</code>, and optional <code>Context</code>, <code>Tags</code>:</p> <pre><code>Input,ExpectedOutput,Context,Tags\nHow do I login?,Visit login.example.com and enter your email.,Login page,account\nWhat are your hours?,We're open 9-6 EST Monday-Friday.,Hours,contact\n</code></pre> <p>Load it:</p> <pre><code>var loader = new JsonDatasetLoader();\nvar dataset = await loader.LoadFromCsvAsync(\"examples.csv\");\n</code></pre>"},{"location":"golden-datasets/#creating-datasets-programmatically","title":"Creating Datasets Programmatically","text":"<p>Build datasets in code:</p> <pre><code>var dataset = new GoldenDataset\n{\n    Name = \"FAQ Evaluation\",\n    Version = \"1.0.0\",\n    Description = \"Common customer questions\",\n    CreatedAt = DateTimeOffset.UtcNow,\n    Tags = new() { \"faq\", \"production\" },\n    Examples = new()\n    {\n        new GoldenExample\n        {\n            Input = \"How do I cancel my subscription?\",\n            ExpectedOutput = \"Visit Settings &gt; Billing &gt; Cancel Subscription. Refunds are issued within 5 business days.\",\n            Context = \"Billing FAQ section\",\n            Tags = new() { \"billing\", \"cancellation\" },\n            Metadata = new()\n            {\n                [\"source\"] = \"help-desk\",\n                [\"reviewed_by\"] = \"support-team\"\n            }\n        },\n        new GoldenExample\n        {\n            Input = \"Is there a mobile app?\",\n            ExpectedOutput = \"Yes, download from the App Store or Google Play.\",\n            Context = \"Apps section in knowledge base\",\n            Tags = new() { \"mobile\" }\n        }\n    }\n};\n\n// Save for reuse\nvar loader = new JsonDatasetLoader();\nawait loader.SaveAsync(dataset, \"faq-evaluation.json\");\n</code></pre>"},{"location":"golden-datasets/#dataset-operations","title":"Dataset Operations","text":""},{"location":"golden-datasets/#filter-by-tag","title":"Filter by Tag","text":"<p>Get examples matching a specific tag:</p> <pre><code>var accountExamples = dataset.GetByTag(\"account\");\nConsole.WriteLine($\"Account-related examples: {accountExamples.Count}\");\n</code></pre>"},{"location":"golden-datasets/#create-a-subset","title":"Create a Subset","text":"<p>Filter examples using a predicate:</p> <pre><code>// Get only high-criticality examples\nvar critical = dataset.GetSubset(e =&gt; \n    e.Metadata.GetValueOrDefault(\"criticality\") == \"high\");\n\nConsole.WriteLine($\"Critical examples: {critical.Examples.Count}\");\n\n// Evaluate only the subset\nvar run = await chatClient.EvaluateAsync(critical, evaluators);\n</code></pre>"},{"location":"golden-datasets/#get-summary-statistics","title":"Get Summary Statistics","text":"<p>View dataset composition:</p> <pre><code>var summary = dataset.GetSummary();\n\nConsole.WriteLine($\"Total examples: {summary.TotalExamples}\");\nConsole.WriteLine($\"Examples with context: {summary.ExamplesWithContext}\");\nConsole.WriteLine($\"Unique tags: {string.Join(\", \", summary.UniqueTags)}\");\n</code></pre>"},{"location":"golden-datasets/#versioning-strategy","title":"Versioning Strategy","text":"<p>Use semantic versioning:</p> <ul> <li>MAJOR (e.g., 2.0.0) \u2014 Breaking changes, incompatible with 1.x evaluations</li> <li>MINOR (e.g., 1.1.0) \u2014 New examples added, backward compatible</li> <li>PATCH (e.g., 1.0.1) \u2014 Bug fixes, typo corrections</li> </ul>"},{"location":"golden-datasets/#guidelines","title":"Guidelines","text":"<ol> <li>Don't modify released versions \u2014 Create a new version instead</li> <li>Document changes \u2014 Keep a changelog</li> <li>Version with your model \u2014 If you release a new model, bump the dataset version too</li> <li>Branch by use case \u2014 Separate datasets for different domains (support_v1.0.0, rag_v2.1.0)</li> </ol>"},{"location":"golden-datasets/#example-changelog","title":"Example Changelog","text":"<pre><code># Customer Support Dataset Changelog\n\n## v1.1.0 (2025-01-20)\n- Added 5 new edge-case examples for billing inquiries\n- Fixed typo in example 12 (expected output)\n- Added \"escalation\" tag for difficult cases\n- Examples: 25 \u2192 30\n\n## v1.0.0 (2025-01-15)\n- Initial release\n- 25 examples covering: account, billing, technical, general\n</code></pre>"},{"location":"golden-datasets/#best-practices","title":"Best Practices","text":""},{"location":"golden-datasets/#1-balance-coverage","title":"1. Balance Coverage","text":"<p>Include diverse scenarios:</p> <pre><code>{\n  \"examples\": [\n    { \"tags\": [\"happy-path\"], ... },    // Normal, happy scenarios\n    { \"tags\": [\"edge-case\"], ... },     // Unusual but valid inputs\n    { \"tags\": [\"error-handling\"], ... }, // Invalid/error cases\n    { \"tags\": [\"ambiguous\"], ... }      // Unclear or multi-interpretable\n  ]\n}\n</code></pre>"},{"location":"golden-datasets/#2-keep-expected-outputs-realistic","title":"2. Keep Expected Outputs Realistic","text":"<p>Expected outputs should represent actual human-quality responses, not idealized perfection:</p> <pre><code>{\n  \"input\": \"What does your product do?\",\n  \"expectedOutput\": \"Our product is a cloud analytics platform for real-time business intelligence. Key features include dashboards, automated reporting, and integrations with popular data sources.\",\n  \"context\": \"Standard product description\"\n}\n</code></pre> <p>Don't create artificially perfect outputs:</p> <pre><code>{\n  \"input\": \"How does it work?\",\n  \"expectedOutput\": \"Step 1: Connect your data. Step 2: Build queries. Step 3: Create dashboards. Step 4: Share insights. Step 5: Monitor performance. Step 6: Export results. Step 7: Set up alerts. Step 8: Collaborate. Step 9: Scale usage. Step 10: Achieve success.\",\n  \"context\": \"DO NOT DO THIS \u2014 unrealistic!\"\n}\n</code></pre>"},{"location":"golden-datasets/#3-include-context-for-rag-evaluation","title":"3. Include Context for RAG Evaluation","text":"<p>For retrieval-augmented generation, always include context:</p> <pre><code>{\n  \"input\": \"What year was the company founded?\",\n  \"expectedOutput\": \"The company was founded in 2019.\",\n  \"context\": \"Company Overview: Established in 2019, we serve 5000+ customers across 40 countries.\"\n}\n</code></pre>"},{"location":"golden-datasets/#4-tag-systematically","title":"4. Tag Systematically","text":"<p>Use consistent, predefined tags:</p> <pre><code>// Good: Consistent tag naming\ntags: [\"technical\", \"urgent\", \"authentication\"]\n\n// Bad: Inconsistent naming\ntags: [\"tech\", \"URGENT\", \"login\"]\n\n// Better: Use enum or constants\npublic static class ExampleTags\n{\n    public const string Happy = \"happy-path\";\n    public const string EdgeCase = \"edge-case\";\n    public const string Error = \"error-handling\";\n}\n</code></pre>"},{"location":"golden-datasets/#5-version-with-your-models","title":"5. Version with Your Models","text":"<p>When you release or update a model, version your dataset too:</p> <pre><code>// If models changed significantly, bump dataset version\n// model-v2.0.0 \u2192 evaluation-dataset-v2.0.0\n// This makes it clear which dataset was used to evaluate which model\n</code></pre>"},{"location":"golden-datasets/#6-review-and-audit","title":"6. Review and Audit","text":"<p>Before release:</p> <ul> <li>\u2705 Have humans review all expected outputs</li> <li>\u2705 Check for typos, grammatical errors</li> <li>\u2705 Verify context is accurate and helpful</li> <li>\u2705 Ensure examples don't contain PII</li> <li>\u2705 Confirm tags are consistent</li> </ul>"},{"location":"golden-datasets/#7-maintain-lineage","title":"7. Maintain Lineage","text":"<p>Track where examples came from:</p> <pre><code>{\n  \"input\": \"...\",\n  \"expectedOutput\": \"...\",\n  \"metadata\": {\n    \"source\": \"support-tickets\",\n    \"ticket_id\": \"SUP-12345\",\n    \"reviewed_by\": \"john.doe@example.com\",\n    \"date_added\": \"2025-01-15\"\n  }\n}\n</code></pre>"},{"location":"golden-datasets/#8-size-appropriately","title":"8. Size Appropriately","text":"<p>Dataset size depends on use case:</p> Scenario Recommended Size Development/testing 10-20 examples Pre-release validation 50-100 examples Regression testing 100+ examples Production monitoring 500+ examples (ongoing) <p>Larger datasets catch more issues but take longer to evaluate.</p>"},{"location":"golden-datasets/#organizing-multiple-datasets","title":"Organizing Multiple Datasets","text":"<p>For complex systems, use multiple focused datasets:</p> <pre><code>datasets/\n\u251c\u2500\u2500 customer-support.json       # v1.3.0 \u2014 50 support examples\n\u251c\u2500\u2500 rag-retrieval.json           # v2.0.0 \u2014 30 retrieval examples\n\u251c\u2500\u2500 safety-moderation.json       # v1.0.0 \u2014 20 safety examples\n\u251c\u2500\u2500 code-generation.json         # v3.1.0 \u2014 40 coding examples\n\u2514\u2500\u2500 multi-language.json          # v1.0.0 \u2014 25 non-English examples\n</code></pre> <p>Load and evaluate against specific datasets:</p> <pre><code>var supportDataset = await loader.LoadAsync(\"datasets/customer-support.json\");\nvar ragDataset = await loader.LoadAsync(\"datasets/rag-retrieval.json\");\n\nvar supportResults = await chatClient.EvaluateAsync(supportDataset, evaluators);\nvar ragResults = await chatClient.EvaluateAsync(ragDataset, evaluators);\n\nConsole.WriteLine($\"Support: {supportResults.Results.Count(r =&gt; r.Passed)}/{supportResults.Results.Count} passed\");\nConsole.WriteLine($\"RAG: {ragResults.Results.Count(r =&gt; r.Passed)}/{ragResults.Results.Count} passed\");\n</code></pre>"},{"location":"golden-datasets/#troubleshooting","title":"Troubleshooting","text":""},{"location":"golden-datasets/#failed-to-deserialize-dataset","title":"\"Failed to deserialize dataset\"","text":"<ul> <li>Verify JSON is valid (use jsonlint.com)</li> <li>Ensure <code>name</code> and <code>examples</code> fields exist</li> <li>Check for encoding issues (use UTF-8)</li> </ul>"},{"location":"golden-datasets/#missing-examples-after-csv-import","title":"Missing Examples After CSV Import","text":"<ul> <li>Verify CSV has <code>Input</code> and <code>ExpectedOutput</code> columns (exact case)</li> <li>Check that header row exists</li> <li>Ensure rows don't have empty inputs or outputs</li> </ul>"},{"location":"golden-datasets/#subset-returns-no-results","title":"Subset Returns No Results","text":"<ul> <li>Verify examples have the tags you're filtering by</li> <li>Use <code>GetSummary()</code> to check available tags:</li> </ul> <pre><code>var summary = dataset.GetSummary();\nConsole.WriteLine($\"Available tags: {string.Join(\", \", summary.UniqueTags)}\");\n</code></pre>"},{"location":"golden-datasets/#real-world-example-building-an-faq-dataset","title":"Real-World Example: Building an FAQ Dataset","text":"<pre><code>// Step 1: Create initial dataset from knowledge base\nvar dataset = new GoldenDataset\n{\n    Name = \"Product FAQ\",\n    Version = \"1.0.0\",\n    Description = \"FAQ examples extracted from help center\",\n    Tags = new() { \"production\" }\n};\n\n// Step 2: Add examples with full metadata\nvar faqs = new[]\n{\n    (\"What's your pricing?\", \"We offer 3 tiers: Starter ($29), Pro ($99), Enterprise (custom).\", \"Pricing page\"),\n    (\"Do you have a free trial?\", \"Yes, 14 days free. No credit card required.\", \"Signup flow\"),\n    (\"Can I export my data?\", \"Yes, CSV, JSON, and API access included in all plans.\", \"Data export docs\")\n};\n\nforeach (var (q, a, context) in faqs)\n{\n    dataset.AddExample(new GoldenExample\n    {\n        Input = q,\n        ExpectedOutput = a,\n        Context = context,\n        Tags = new() { \"faq\", \"general\" },\n        Metadata = new() { [\"source\"] = \"help-center\" }\n    });\n}\n\n// Step 3: Save\nvar loader = new JsonDatasetLoader();\nawait loader.SaveAsync(dataset, \"faq.json\");\n\n// Step 4: Evaluate against it\nvar evaluators = new IEvaluator[]\n{\n    new RelevanceEvaluator(0.65),\n    new CoherenceEvaluator(0.70)\n};\n\nvar run = await chatClient.EvaluateAsync(dataset, evaluators);\nvar passRate = (double)run.Results.Count(r =&gt; r.Passed) / run.Results.Count;\nConsole.WriteLine($\"FAQ Pass Rate: {passRate:P0}\");\n</code></pre>"},{"location":"golden-datasets/#next-steps","title":"Next Steps","text":"<ul> <li>Review evaluation-metrics.md to understand how examples are scored</li> <li>Check best-practices.md for testing patterns and workflows</li> <li>Explore samples/ for complete working examples with datasets</li> </ul>"},{"location":"publishing/","title":"Publishing","text":""},{"location":"publishing/#publishing-a-new-version-to-nuget","title":"Publishing a New Version to NuGet","text":"<p>This guide covers how to publish new versions of the ElBruno.AI.Evaluation packages to NuGet.org using GitHub Actions and NuGet Trusted Publishing (keyless, OIDC-based).</p>"},{"location":"publishing/#packages","title":"Packages","text":"Package Project Description <code>ElBruno.AI.Evaluation</code> <code>src/ElBruno.AI.Evaluation/</code> Core library \u2014 evaluators, datasets, metrics, pipeline <code>ElBruno.AI.Evaluation.Xunit</code> <code>src/ElBruno.AI.Evaluation.Xunit/</code> xUnit integration \u2014 AIAssert, AIEvaluationTest <code>ElBruno.AI.Evaluation.Reporting</code> <code>src/ElBruno.AI.Evaluation.Reporting/</code> Reporting \u2014 SQLite store, JSON/CSV export <code>ElBruno.AI.Evaluation.SyntheticData</code> <code>src/ElBruno.AI.Evaluation.SyntheticData/</code> Synthetic data generation \u2014 templates, LLM-powered generation, validation <p>Maintenance rule: If a new packable library is added under <code>src/</code>, update <code>.github/workflows/publish.yml</code> in the same PR so the new project is packed/pushed, and add a matching NuGet Trusted Publishing policy.</p>"},{"location":"publishing/#prerequisites-one-time-setup","title":"Prerequisites (One-Time Setup)","text":""},{"location":"publishing/#1-configure-nugetorg-trusted-publishing-policies","title":"1. Configure NuGet.org Trusted Publishing Policies","text":"<ol> <li>Sign in to nuget.org</li> <li>Click your username \u2192 Trusted Publishing</li> <li>Add a policy for each package with these values:</li> </ol> Setting Value Repository Owner <code>elbruno</code> Repository <code>elbruno-ai-evaluation</code> Workflow File <code>publish.yml</code> Environment <code>release</code> <p>You need to create this policy four times \u2014 once per package:</p> <ul> <li><code>ElBruno.AI.Evaluation</code></li> <li><code>ElBruno.AI.Evaluation.Xunit</code></li> <li><code>ElBruno.AI.Evaluation.Reporting</code></li> <li><code>ElBruno.AI.Evaluation.SyntheticData</code></li> </ul> <p>Note: For new packages that don't exist on NuGet.org yet, you must first push them once (the workflow handles this). After the initial push, add the Trusted Publishing policy so future publishes are keyless.</p>"},{"location":"publishing/#2-configure-github-repository","title":"2. Configure GitHub Repository","text":"<ol> <li>Go to the repo Settings \u2192 Environments</li> <li>Create an environment called <code>release</code></li> <li>Optionally add required reviewers for a manual approval gate</li> <li>Go to Settings \u2192 Secrets and variables \u2192 Actions</li> <li>Add a repository secret:</li> <li>Name: <code>NUGET_USER</code></li> <li>Value: <code>elbruno</code></li> </ol>"},{"location":"publishing/#publishing-a-new-version","title":"Publishing a New Version","text":""},{"location":"publishing/#option-a-create-a-github-release-recommended","title":"Option A: Create a GitHub Release (Recommended)","text":"<ol> <li>Update the version in <code>Directory.Build.props</code>:</li> </ol> <pre><code>&lt;Version&gt;1.1.0&lt;/Version&gt;\n</code></pre> <ol> <li>Commit and push the version change to <code>main</code></li> <li>Create a GitHub Release:</li> <li>Go to the repo \u2192 Releases \u2192 Draft a new release</li> <li>Create a new tag: <code>v1.1.0</code> (must match the version in Directory.Build.props)</li> <li>Fill in the release title and notes</li> <li>Click Publish release</li> <li>The Publish to NuGet workflow runs automatically</li> </ol>"},{"location":"publishing/#option-b-manual-dispatch","title":"Option B: Manual Dispatch","text":"<ol> <li>Go to the repo \u2192 Actions \u2192 Publish to NuGet</li> <li>Click Run workflow</li> <li>Optionally enter a version</li> <li>Click Run workflow</li> </ol>"},{"location":"publishing/#how-it-works","title":"How It Works","text":"<pre><code>GitHub Release created (e.g. v1.0.0)\n  \u2192 GitHub Actions triggers publish.yml\n    \u2192 Builds + tests all projects\n    \u2192 Packs four .nupkg files\n    \u2192 Requests an OIDC token from GitHub\n    \u2192 Exchanges the token with NuGet.org for a temporary API key\n    \u2192 Pushes all packages to NuGet.org\n    \u2192 Temp key expires automatically\n</code></pre>"},{"location":"publishing/#troubleshooting","title":"Troubleshooting","text":"Problem Solution Workflow fails at \"NuGet login\" Verify the Trusted Publishing policy on nuget.org matches repo owner, name, workflow file, and environment <code>NUGET_USER</code> secret not found Add the secret in GitHub repo Settings \u2192 Secrets \u2192 Actions Package already exists <code>--skip-duplicate</code> prevents failures. Bump the version number OIDC token errors Ensure <code>id-token: write</code> permission is set in the workflow job"},{"location":"publishing/#reference-links","title":"Reference Links","text":"<ul> <li>NuGet Trusted Publishing</li> <li>NuGet/login GitHub Action</li> <li>GitHub Actions OIDC</li> </ul>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#your-first-ai-test-in-5-minutes","title":"Your First AI Test in 5 Minutes","text":"<p>Learn how to write and run your first AI evaluation in ElBruno.AI.Evaluation.</p>"},{"location":"quickstart/#installation","title":"Installation","text":"<p>Add the NuGet package to your .NET project:</p> <pre><code>dotnet add package ElBruno.AI.Evaluation\n</code></pre> <p>If you're using xUnit, also install the testing integration:</p> <pre><code>dotnet add package ElBruno.AI.Evaluation.Xunit\n</code></pre>"},{"location":"quickstart/#create-your-first-golden-dataset","title":"Create Your First Golden Dataset","text":"<p>A golden dataset is a JSON file containing test cases\u2014inputs and expected outputs that serve as ground truth.</p> <p>Create a file called <code>evaluation-dataset.json</code>:</p> <pre><code>{\n  \"name\": \"Customer Support Evaluation\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Test cases for customer support chatbot quality\",\n  \"createdAt\": \"2025-01-01T00:00:00Z\",\n  \"tags\": [\"support\", \"chatbot\"],\n  \"examples\": [\n    {\n      \"input\": \"How do I reset my password?\",\n      \"expectedOutput\": \"To reset your password, visit the login page and click 'Forgot Password'. Enter your email address and follow the verification steps. You'll receive a link to create a new password.\",\n      \"context\": \"Password reset documentation\",\n      \"tags\": [\"account\", \"password\"]\n    },\n    {\n      \"input\": \"What are your business hours?\",\n      \"expectedOutput\": \"We're open Monday through Friday, 9 AM to 6 PM EST. Weekend support is available via email only.\",\n      \"context\": \"Company operating hours\",\n      \"tags\": [\"hours\", \"contact\"]\n    }\n  ]\n}\n</code></pre>"},{"location":"quickstart/#write-your-first-evaluation-test","title":"Write Your First Evaluation Test","text":"<p>\ud83d\udca1 New to AI testing? Check out the complete blog series for a guided developer journey from basics to production!</p> <p>Here's a simple console app that evaluates an AI model against your golden dataset:</p> <pre><code>using ElBruno.AI.Evaluation.Datasets;\nusing ElBruno.AI.Evaluation.Evaluators;\nusing ElBruno.AI.Evaluation.Extensions;\nusing Microsoft.Extensions.AI;\n\n// 1. Load your golden dataset\nvar loader = new JsonDatasetLoader();\nvar dataset = await loader.LoadAsync(\"evaluation-dataset.json\");\n\nConsole.WriteLine($\"\ud83d\udcda Loaded dataset: {dataset.Name} with {dataset.Examples.Count} examples\\n\");\n\n// 2. Create evaluators (customize thresholds as needed)\nvar evaluators = new IEvaluator[]\n{\n    new RelevanceEvaluator(threshold: 0.6),\n    new FactualityEvaluator(threshold: 0.8),\n    new CoherenceEvaluator(threshold: 0.7)\n};\n\n// 3. Create your AI chat client (example using a mock for demo)\n// In production, use: var chatClient = new OpenAIClient(...);\nvar chatClient = new MockChatClient();\n\n// 4. Run evaluation against the entire dataset\nvar run = await chatClient.EvaluateAsync(dataset, evaluators);\n\nConsole.WriteLine($\"\ud83e\uddea Evaluation Results\");\nConsole.WriteLine($\"=====================\");\nConsole.WriteLine($\"Total examples: {run.Results.Count}\");\nConsole.WriteLine($\"Passed: {run.Results.Count(r =&gt; r.Passed)}/{run.Results.Count}\\n\");\n\n// 5. View detailed results\nforeach (var (index, result) in run.Results.Select((r, i) =&gt; (i, r)))\n{\n    Console.WriteLine($\"Example {index + 1}: {(result.Passed ? \"\u2705 PASS\" : \"\u274c FAIL\")}\");\n    Console.WriteLine($\"  Score: {result.Score:P0}\");\n    Console.WriteLine($\"  Details: {result.Details}\\n\");\n}\n\n// Mock chat client for demonstration\nclass MockChatClient : IChatClient\n{\n    public async Task&lt;ChatCompletion&gt; CompleteAsync(\n        IList&lt;ChatMessage&gt; chatMessages, \n        ChatOptions? options = null, \n        CancellationToken cancellationToken = default)\n    {\n        // Simulate a realistic response\n        var userMessage = chatMessages.FirstOrDefault(m =&gt; m.Role == ChatRole.User)?.Content ?? \"unknown\";\n        var mockResponse = $\"Responding to: {userMessage}\";\n\n        return new ChatCompletion { Content = [new TextContent(mockResponse)] };\n    }\n\n    public IAsyncEnumerable&lt;StreamingChatCompletionUpdate&gt; CompleteStreamingAsync(\n        IList&lt;ChatMessage&gt; chatMessages,\n        ChatOptions? options = null,\n        CancellationToken cancellationToken = default)\n    {\n        throw new NotImplementedException();\n    }\n}\n</code></pre>"},{"location":"quickstart/#run-it","title":"Run It","text":"<pre><code>dotnet run\n</code></pre> <p>You'll see output like:</p> <pre><code>\ud83d\udcda Loaded dataset: Customer Support Evaluation with 2 examples\n\n\ud83e\uddea Evaluation Results\n=====================\nTotal examples: 2\nPassed: 1/2\n\nExample 1: \u2705 PASS\n  Score: 85%\n  Details: Cosine similarity between input and output terms: 0.850. Input terms: 4, Output terms: 12.\n\nExample 2: \u274c FAIL\n  Score: 45%\n  Details: Cosine similarity between input and output terms: 0.450. Input terms: 5, Output terms: 8.\n</code></pre>"},{"location":"quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>Read evaluation metrics guide \u2014 Learn what each evaluator measures and when to use it</li> <li>Explore dataset management \u2014 Understand versioning and best practices for golden datasets</li> <li>Try xUnit integration \u2014 Use <code>AIAssert</code> to write fluent evaluation assertions</li> <li>View samples \u2014 Check out ChatbotEvaluation and RagEvaluation in the samples/ directory</li> <li>Set up regression testing \u2014 Compare baseline snapshots to detect quality regressions</li> </ul>"},{"location":"quickstart/#key-concepts","title":"Key Concepts","text":"Term Meaning Golden Dataset JSON file with ground-truth test cases (inputs and expected outputs) Evaluator Component that scores AI output (e.g., <code>RelevanceEvaluator</code>) Threshold Minimum score (0-1) required to pass an evaluation EvaluationResult Contains <code>Score</code>, <code>Passed</code>, and <code>Details</code> for a single evaluation EvaluationRun Results from evaluating an entire dataset"},{"location":"quickstart/#need-help","title":"Need Help?","text":"<ul> <li>Check the evaluation-metrics.md guide for detailed evaluator documentation</li> <li>Review best-practices.md for common testing patterns</li> <li>Explore the samples/ directory for complete working examples</li> </ul>"},{"location":"synthetic-data/","title":"Synthetic Data","text":""},{"location":"synthetic-data/#synthetic-data-generation-for-ai-evaluation","title":"Synthetic Data Generation for AI Evaluation","text":""},{"location":"synthetic-data/#what-is-synthetic-data-generation","title":"What Is Synthetic Data Generation?","text":"<p>Synthetic data generation automatically creates test datasets for evaluating AI systems\u2014saving you from painstaking manual curation. Rather than handwriting thousands of Q&amp;A pairs or edge cases, ElBruno.AI.Evaluation.SyntheticData lets you generate realistic, diverse, and reproducible datasets using templates (deterministic) and LLMs (AI-powered).</p>"},{"location":"synthetic-data/#why-it-matters-for-ai-evaluation","title":"Why It Matters for AI Evaluation","text":"<ol> <li>Speed \u2014 Generate 1,000 test cases in seconds instead of weeks of manual work</li> <li>Scale \u2014 Build large enough datasets to catch regressions and corner cases</li> <li>Reproducibility \u2014 Use fixed random seeds to generate the same data every time (for deterministic tests)</li> <li>Diversity \u2014 Mix template-based and LLM-generated examples to balance cost and variety</li> <li>Domain Compliance \u2014 Generate data that respects domain constraints (HIPAA, PCI, etc.)</li> </ol>"},{"location":"synthetic-data/#quick-start-generate-a-simple-qa-dataset","title":"Quick Start: Generate a Simple Q&amp;A Dataset","text":"<pre><code>using ElBruno.AI.Evaluation.SyntheticData;\nusing ElBruno.AI.Evaluation.SyntheticData.Templates;\n\n// Define Q&amp;A templates (question patterns \u2192 answer patterns)\nvar qaTemplate = new QaTemplate(\n    questionTemplates: new[]\n    {\n        \"What is {topic}?\",\n        \"How do I {action}?\",\n        \"Explain {concept} to me.\",\n    },\n    answerTemplates: new[]\n    {\n        \"{topic} is a {definition}.\",\n        \"To {action}, follow these steps: {steps}.\",\n        \"{concept} refers to {explanation}.\",\n    }\n).WithCategory(\"general-knowledge\")\n .AddTags(\"faq\", \"quick-start\");\n\n// Generate 50 Q&amp;A examples\nvar dataset = await new SyntheticDatasetBuilder(\"my-first-qa-dataset\")\n    .WithVersion(\"1.0.0\")\n    .WithDescription(\"Quick-start Q&amp;A dataset\")\n    .WithTags(\"synthetic\", \"deterministic\")\n    .UseDeterministicGenerator(strategy =&gt;\n    {\n        strategy.Template = qaTemplate;\n        strategy.RandomSeed = 42; // Reproducible across runs\n    })\n    .GenerateQaPairs(count: 50)\n    .BuildAsync();\n\nConsole.WriteLine($\"\u2705 Generated {dataset.Examples.Count} Q&amp;A examples\");\n</code></pre>"},{"location":"synthetic-data/#template-based-deterministic-generation","title":"Template-Based (Deterministic) Generation","text":"<p>Deterministic generation uses templates to combine question patterns with answer patterns. Outputs are reproducible (same seed = same data) and fast.</p>"},{"location":"synthetic-data/#use-cases","title":"Use Cases","text":"<ul> <li>Building CI/CD test datasets that must be stable</li> <li>Generating baseline datasets for regression testing</li> <li>Creating lightweight test data with predictable variation</li> </ul>"},{"location":"synthetic-data/#example-faq-chatbot","title":"Example: FAQ Chatbot","text":"<pre><code>var qaTemplate = new QaTemplate(\n    questionTemplates: new[]\n    {\n        \"What is the return policy?\",\n        \"How long does shipping take?\",\n        \"Is there a warranty?\",\n    },\n    answerTemplates: new[]\n    {\n        \"Items can be returned within {days} days of purchase.\",\n        \"Standard delivery takes {duration} business days.\",\n        \"All products include a {warranty} warranty.\",\n    }\n).WithCategory(\"ecommerce\")\n .AddTags(\"returns\", \"shipping\", \"warranty\");\n\nvar dataset = await new SyntheticDatasetBuilder(\"faq-chatbot\")\n    .WithVersion(\"1.0.0\")\n    .UseDeterministicGenerator(strategy =&gt;\n    {\n        strategy.Template = qaTemplate;\n        strategy.RandomSeed = 99; // Reproducible\n    })\n    .GenerateQaPairs(count: 30)\n    .BuildAsync();\n</code></pre>"},{"location":"synthetic-data/#controlling-randomness","title":"Controlling Randomness","text":"<ul> <li>With seed (deterministic): <code>strategy.RandomSeed = 42</code> \u2014 same examples every run</li> <li>Without seed (non-deterministic): <code>strategy.RandomSeed = null</code> \u2014 different examples each run</li> <li>Shuffling: <code>strategy.Shuffle = true</code> \u2014 randomize example order (even with a seed)</li> </ul>"},{"location":"synthetic-data/#llm-powered-generation-with-ichatclient","title":"LLM-Powered Generation with IChatClient","text":"<p>Use an <code>IChatClient</code> (e.g., <code>OpenAIChatClient</code>, <code>AnthropicClient</code>) to generate diverse, AI-powered synthetic data. Perfect for edge cases, adversarial testing, and natural-language variation.</p>"},{"location":"synthetic-data/#use-cases_1","title":"Use Cases","text":"<ul> <li>Generating adversarial/edge-case examples</li> <li>Creating natural language variations</li> <li>Domain-specific data with compliance constraints</li> </ul>"},{"location":"synthetic-data/#example-llm-generated-adversarial-cases","title":"Example: LLM-Generated Adversarial Cases","text":"<pre><code>using Microsoft.Extensions.AI;\n\nvar chatClient = new OpenAIChatClient(apiKey: Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\"));\n\nvar dataset = await new SyntheticDatasetBuilder(\"adversarial-edge-cases\")\n    .WithVersion(\"1.0.0\")\n    .WithDescription(\"LLM-generated edge cases\")\n    .WithTags(\"synthetic\", \"llm\", \"adversarial\")\n    .UseLlmGenerator(chatClient, strategy =&gt;\n    {\n        strategy.SystemPrompt = \n            \"You are an expert at finding edge cases and breaking AI systems. \" +\n            \"Generate adversarial inputs: typos, contradictions, null/empty values, truncated queries.\";\n        strategy.GenerationTemplate = GenerationTemplate.AdversarialCases;\n        strategy.Temperature = 0.9; // Higher creativity\n        strategy.MaxTokens = 300;\n        strategy.ParallelismDegree = 5; // Generate 5 examples in parallel\n        strategy.RetryOnFailure = true; // Retry failed generations\n    })\n    .GenerateAdversarialExamples(count: 100)\n    .BuildAsync();\n\nConsole.WriteLine($\"\u2705 Generated {dataset.Examples.Count} adversarial examples\");\n</code></pre>"},{"location":"synthetic-data/#llm-strategy-options","title":"LLM Strategy Options","text":"<pre><code>strategy.SystemPrompt              // Guide the LLM with a system message\nstrategy.GenerationTemplate        // Predefined output format (see next section)\nstrategy.Temperature               // 0.0 = deterministic, 2.0 = very creative (default: 0.7)\nstrategy.MaxTokens                 // Response length limit (default: 500)\nstrategy.ParallelismDegree         // Parallel requests (default: 1)\nstrategy.RetryOnFailure            // Auto-retry failed generations (default: true)\nstrategy.MaxRetries                // Max retry attempts (default: 3)\n</code></pre>"},{"location":"synthetic-data/#compositehybrid-generation","title":"Composite/Hybrid Generation","text":"<p>Mix deterministic and LLM-powered generation for optimal cost + diversity.</p>"},{"location":"synthetic-data/#example-70-deterministic-30-llm","title":"Example: 70% Deterministic + 30% LLM","text":"<pre><code>var template = new QaTemplate(\n    questionTemplates: new[] { \"What is {topic}?\", \"How do I {action}?\" },\n    answerTemplates: new[] { \"{topic} is a {definition}.\", \"To {action}: {steps}.\" }\n);\n\nvar dataset = await new SyntheticDatasetBuilder(\"hybrid-qa-dataset\")\n    .WithVersion(\"1.0.0\")\n    .UseCompositeGenerator(config =&gt;\n    {\n        // 70% deterministic (fast &amp; cheap)\n        config.AddDeterministicGenerator(\n            template: template,\n            weight: 0.7,\n            randomSeed: 42);\n\n        // 30% LLM-powered (diverse &amp; natural)\n        config.AddLlmGenerator(\n            chatClient: chatClient,\n            systemPrompt: \"Generate diverse, natural Q&amp;A pairs with varied phrasings.\",\n            generationTemplate: GenerationTemplate.SimpleQA,\n            weight: 0.3);\n    })\n    .GenerateQaPairs(count: 100)\n    .BuildAsync();\n\n// Result: ~70 deterministic examples + ~30 LLM-powered examples\nConsole.WriteLine($\"\u2705 Generated {dataset.Examples.Count} hybrid examples\");\n</code></pre>"},{"location":"synthetic-data/#built-in-templates","title":"Built-in Templates","text":""},{"location":"synthetic-data/#1-qatemplate-question-answer-pairs","title":"1. QaTemplate \u2014 Question-Answer Pairs","text":"<p>For FAQ systems, chatbots, and knowledge bases.</p> <pre><code>var template = new QaTemplate(\n    questionTemplates: new[] { \"What is {topic}?\", \"How do I {action}?\" },\n    answerTemplates: new[] { \"{topic} is a {definition}.\", \"To {action}: {steps}.\" }\n).WithCategory(\"technical-support\")\n .AddTags(\"faq\", \"support\");\n</code></pre> <p>Properties: - <code>questionTemplates</code> \u2014 List of question patterns with placeholders - <code>answerTemplates</code> \u2014 List of answer patterns with placeholders - <code>WithCategory()</code> \u2014 Category/domain for these pairs - <code>AddTags()</code> \u2014 Tags for filtering - <code>WithMetadata()</code> \u2014 Custom key-value metadata</p>"},{"location":"synthetic-data/#2-ragtemplate-context-answer-retrieval-augmented-generation","title":"2. RagTemplate \u2014 Context + Answer (Retrieval-Augmented Generation)","text":"<p>For RAG systems where answers must be grounded in retrieved documents.</p> <pre><code>var template = new RagTemplate(\n    documents: new[]\n    {\n        \"Return Policy: Items can be returned within 30 days of purchase.\",\n        \"Shipping: Free shipping on orders over $50. Standard delivery takes 5-7 business days.\",\n        \"Warranty: All products include a 1-year manufacturer's warranty.\",\n    },\n    qaExamples: new[]\n    {\n        (\"What is the return policy?\", \"Items can be returned within 30 days of purchase.\"),\n        (\"How long does shipping take?\", \"Standard delivery takes 5-7 business days.\"),\n        (\"Is there a warranty?\", \"All products include a 1-year warranty.\"),\n    }\n).WithDocumentsPerExample(2)\n .AddTags(\"rag\", \"ecommerce\");\n</code></pre> <p>Properties: - <code>documents</code> \u2014 List of document chunks (context sources) - <code>qaExamples</code> \u2014 Q&amp;A pairs anchoring the examples - <code>WithDocumentsPerExample()</code> \u2014 Number of documents per example (default: 1) - <code>AddTags()</code>, <code>WithMetadata()</code> \u2014 Standard template options</p>"},{"location":"synthetic-data/#3-adversarialtemplate-edge-cases-perturbations","title":"3. AdversarialTemplate \u2014 Edge Cases &amp; Perturbations","text":"<p>For stress-testing your AI system with adversarial inputs.</p> <pre><code>var baseExamples = new[]\n{\n    new GoldenExample { Input = \"What is X?\", ExpectedOutput = \"X is Y.\" },\n    new GoldenExample { Input = \"How do I Z?\", ExpectedOutput = \"To Z: follow steps.\" },\n};\n\nvar template = new AdversarialTemplate(baseExamples)\n    .WithNullInjection(enabled: true)          // Inject null/empty strings\n    .WithTruncation(enabled: true)             // Truncate inputs (incomplete queries)\n    .WithTypoInjection(enabled: true)          // Typos and character corruption\n    .WithContradictions(enabled: true)         // Contradictory context vs expected output\n    .WithLongInputs(enabled: false, maxLength: 2000) // Extremely long inputs\n    .AddTags(\"adversarial\", \"edge-cases\");\n</code></pre> <p>Perturbation Types: - <code>WithNullInjection()</code> \u2014 Create null/empty variations - <code>WithTruncation()</code> \u2014 Partial/incomplete inputs - <code>WithTypoInjection()</code> \u2014 Character-level errors - <code>WithContradictions()</code> \u2014 Conflicting context - <code>WithLongInputs()</code> \u2014 Stress-test with extremely long inputs</p>"},{"location":"synthetic-data/#4-domaintemplate-domain-specific-generation","title":"4. DomainTemplate \u2014 Domain-Specific Generation","text":"<p>For generating data within domain constraints (healthcare, finance, legal, etc.).</p> <pre><code>var template = new DomainTemplate(\"healthcare\")\n    .WithVocabulary(new[] { \"patient\", \"diagnosis\", \"treatment\", \"medication\", \"symptom\" })\n    .WithConstraints(\n        \"Do not include real patient names or SSNs\",\n        \"Only discuss public medical information\",\n        \"Follow HIPAA guidelines\")\n    .WithComplianceFramework(\"HIPAA\")\n    .AddTags(\"healthcare\", \"hipaa-compliant\");\n</code></pre> <p>Use with LLM generation:</p> <pre><code>var dataset = await new SyntheticDatasetBuilder(\"healthcare-qa\")\n    .WithDescription(\"HIPAA-compliant healthcare Q&amp;A\")\n    .UseLlmGenerator(chatClient, strategy =&gt;\n    {\n        strategy.SystemPrompt = template.Domain + \" vocabulary and constraints.\";\n        strategy.GenerationTemplate = GenerationTemplate.DomainSpecific;\n        strategy.Temperature = 0.5; // Lower variance for compliance\n    })\n    .GenerateDomainExamples(\"healthcare\", count: 50)\n    .BuildAsync();\n</code></pre>"},{"location":"synthetic-data/#generation-templates-for-llm","title":"Generation Templates for LLM","text":"<p>When using <code>UseLlmGenerator()</code>, specify the output format:</p> <pre><code>public enum GenerationTemplate\n{\n    SimpleQA,            // Input \u2192 Output\n    RagContext,          // Input + Context \u2192 Output\n    QAWithExplanation,   // Input \u2192 Output + Explanation\n    QAVariations,        // Multiple variations of Input \u2192 Output\n    AdversarialCases,    // Edge cases and adversarial examples\n    DomainSpecific,      // Domain-specific examples with metadata\n}\n</code></pre> <p>Example:</p> <pre><code>strategy.GenerationTemplate = GenerationTemplate.QAWithExplanation;\n// Expects: { \"input\": \"...\", \"output\": \"...\", \"explanation\": \"...\" }\n</code></pre>"},{"location":"synthetic-data/#integration-with-the-evaluation-pipeline","title":"Integration with the Evaluation Pipeline","text":"<p>Once you've generated synthetic data, use it in your evaluation pipeline:</p> <pre><code>// Generate synthetic dataset\nvar syntheticDataset = await new SyntheticDatasetBuilder(\"test-data\")\n    .UseDeterministicGenerator(/* ... */)\n    .GenerateQaPairs(count: 100)\n    .BuildAsync();\n\n// Use in evaluation pipeline\nvar evaluators = new IEvaluator[] \n{\n    new RelevanceEvaluator(),\n    new HallucinationEvaluator(),\n    new SafetyEvaluator()\n};\n\nvar results = await chatClient.EvaluateAsync(syntheticDataset, evaluators);\nConsole.WriteLine($\"Pass Rate: {results.PassRate:P0}\");\n</code></pre>"},{"location":"synthetic-data/#best-practices-for-synthetic-test-data","title":"Best Practices for Synthetic Test Data","text":""},{"location":"synthetic-data/#1-start-with-deterministic-add-llm-later","title":"1. Start with Deterministic, Add LLM Later","text":"<p>Generate baseline data with templates first. It's fast, reproducible, and cheap. Add LLM-powered examples once you need diversity.</p> <pre><code>var dataset = await builder\n    .UseDeterministicGenerator(/* ... */)\n    .GenerateQaPairs(100)\n    .BuildAsync();\n</code></pre>"},{"location":"synthetic-data/#2-use-fixed-seeds-for-cicd","title":"2. Use Fixed Seeds for CI/CD","text":"<p>Ensure your test data is stable across runs:</p> <pre><code>strategy.RandomSeed = 42; // Fixed \u2192 reproducible tests\n</code></pre>"},{"location":"synthetic-data/#3-validate-generated-data","title":"3. Validate Generated Data","text":"<p>Check for quality issues before using:</p> <pre><code>var errors = dataset.ValidateExamples(new ValidationOptions\n{\n    MinInputLength = 5,\n    MaxInputLength = 500,\n    FlagNullInputs = true,\n    FlagDuplicateInputs = true,\n});\n\nif (errors.Count &gt; 0)\n{\n    Console.WriteLine($\"\u26a0\ufe0f  Found {errors.Count} validation issues\");\n    foreach (var error in errors)\n        Console.WriteLine($\"  - Example {error.ExampleIndex}: {error.Message}\");\n}\n</code></pre>"},{"location":"synthetic-data/#4-deduplicate-before-evaluation","title":"4. Deduplicate Before Evaluation","text":"<p>Remove duplicate inputs to avoid biased metrics:</p> <pre><code>var cleanedDataset = dataset.Deduplicate();\nConsole.WriteLine($\"Removed {dataset.Examples.Count - cleanedDataset.Examples.Count} duplicates\");\n</code></pre>"},{"location":"synthetic-data/#5-augment-existing-golden-datasets","title":"5. Augment Existing Golden Datasets","text":"<p>Expand hand-curated datasets with synthetic examples:</p> <pre><code>var baseDataset = new GoldenDataset { Name = \"support\", Examples = new() { /* ... */ } };\n\nvar augmented = await baseDataset.AugmentWithSyntheticExamplesAsync(\n    generator: new DeterministicGenerator(template, randomSeed: 42),\n    count: 50);\n\nConsole.WriteLine($\"\u2705 Expanded from {baseDataset.Examples.Count} to {augmented.Examples.Count} examples\");\n</code></pre>"},{"location":"synthetic-data/#6-mix-deterministic-llm-for-cost-quality","title":"6. Mix Deterministic + LLM for Cost &amp; Quality","text":"<p>Deterministic examples are ~100x cheaper than LLM calls. Use them strategically:</p> <pre><code>var dataset = await builder\n    .UseCompositeGenerator(config =&gt;\n    {\n        config.AddDeterministicGenerator(template, weight: 0.8, randomSeed: 42);\n        config.AddLlmGenerator(chatClient, systemPrompt, GenerationTemplate.SimpleQA, weight: 0.2);\n    })\n    .GenerateQaPairs(1000)\n    .BuildAsync();\n\n// Result: 800 deterministic examples (free) + 200 LLM examples (low cost)\n</code></pre>"},{"location":"synthetic-data/#7-tag-and-filter-by-use-case","title":"7. Tag and Filter by Use Case","text":"<p>Organize data with tags for easy selection:</p> <pre><code>var template = new QaTemplate(/* ... */)\n    .AddTags(\"faq\", \"technical-support\", \"priority-tier-1\");\n\n// Later: filter by tag in evaluation\nvar filteredExamples = dataset.Examples\n    .Where(ex =&gt; ex.Tags.Contains(\"priority-tier-1\"))\n    .ToList();\n</code></pre>"},{"location":"synthetic-data/#8-version-your-synthetic-datasets","title":"8. Version Your Synthetic Datasets","text":"<p>Like golden datasets, version synthetic data for regression tracking:</p> <pre><code>var dataset = await builder\n    .WithVersion(\"1.0.0\")\n    .WithDescription(\"Initial synthetic dataset for chatbot\")\n    .WithTags(\"v1.0\", \"baseline\")\n    ./* ... */\n    .BuildAsync();\n</code></pre>"},{"location":"synthetic-data/#summary","title":"Summary","text":"Scenario Template Generator Cost Reproducibility Fast CI/CD data QA/RAG Deterministic \u2705 Free \u2705 100% (with seed) Edge cases Adversarial LLM \u26a0\ufe0f Higher \u274c Variable Domain-specific Domain LLM \u26a0\ufe0f Higher \u274c Variable Balanced approach QA/RAG Composite (70% det + 30% LLM) \u2705 Low \u2705 Good"},{"location":"synthetic-data/#see-also","title":"See Also","text":"<ul> <li>Quick Start \u2014 Get started with evaluation</li> <li>Evaluation Metrics \u2014 Understanding evaluators</li> <li>Golden Datasets \u2014 Designing ground truth</li> <li>Best Practices \u2014 Production patterns</li> </ul> <p>Author: Frohike Package: ElBruno.AI.Evaluation.SyntheticData License: MIT</p>"},{"location":"blog/01-introducing-elbruno-ai-evaluation/","title":"Introducing ElBruno.AI.Evaluation","text":""},{"location":"blog/01-introducing-elbruno-ai-evaluation/#testing-ai-in-net-the-landscape","title":"Testing AI in .NET: The Landscape","text":""},{"location":"blog/01-introducing-elbruno-ai-evaluation/#the-challenge","title":"The Challenge","text":"<p>You're building an AI app in .NET. Your chatbot works great in demos\u2014clean examples, perfect conditions. But production? That's where things get messy. Prompts generate off-topic content. The AI hallucinates facts. Performance degrades in unexpected ways. You need to measure and improve quality systematically.</p> <p>The Python ecosystem has had frameworks like Ragas and DeepEval for years. .NET developers? Until now, you've had two choices: build it yourself or reach for Python interop. Neither is ideal.</p>"},{"location":"blog/01-introducing-elbruno-ai-evaluation/#two-toolkits-one-goal","title":"Two Toolkits, One Goal","text":"<p>This series explores two complementary evaluation frameworks for .NET:</p> <ol> <li>Microsoft.Extensions.AI.Evaluation \u2014 Microsoft's official, enterprise-grade framework. LLM-powered evaluators for nuanced quality judgment, agent evaluation, professional reporting, Azure integration.</li> <li>ElBruno.AI.Evaluation \u2014 A lightweight, deterministic alternative. Offline evaluation, golden dataset versioning, synthetic data generation, xUnit integration, zero external dependencies.</li> </ol> <p>Neither is \"better\"\u2014they solve different problems. Microsoft excels at sophisticated quality analysis. ElBruno excels at offline scenarios, test automation, and dataset management. The real power is using both together.</p> <p>This series guides your journey: from \"I need to test my AI app\" to \"I have a production evaluation pipeline.\"</p>"},{"location":"blog/01-introducing-elbruno-ai-evaluation/#when-to-use-which-decision-tree","title":"When to Use Which: Decision Tree","text":"<pre><code>Need LLM-powered quality judgment?\n  \u2192 YES: Use Microsoft (Relevance, Completeness, Fluency, Groundedness)\n  \u2192 NO: Next question\n\nNeed agentic evaluators (IntentResolution, TaskAdherence)?\n  \u2192 YES: Use Microsoft\n  \u2192 NO: Next question\n\nNeed offline/air-gapped evaluation?\n  \u2192 YES: Use ElBruno (+ Microsoft for non-safety)\n  \u2192 NO: Next question\n\nNeed synthetic data or golden dataset versioning?\n  \u2192 YES: Use ElBruno\n  \u2192 NO: Next question\n\nNeed regression detection in CI/CD?\n  \u2192 YES: Use ElBruno\n  \u2192 NO: Use either; Microsoft for comprehensiveness\n</code></pre> <p>Quick Comparison:</p> Scenario Best Choice \"I need fast, local eval in CI/CD\" ElBruno \"I need nuanced quality judgment\" Microsoft \"I'm offline or air-gapped\" ElBruno \"I need to generate test data\" ElBruno \"I need comprehensive safety analysis\" Microsoft \"I want xUnit-native testing\" ElBruno \"I need professional HTML reports\" Microsoft \"I'm cost-conscious\" ElBruno"},{"location":"blog/01-introducing-elbruno-ai-evaluation/#a-quick-demo","title":"A Quick Demo","text":"<p>Let's build a simple evaluation in 5 minutes. This example uses ElBruno, but Microsoft's approach is similar.</p>"},{"location":"blog/01-introducing-elbruno-ai-evaluation/#step-1-install-the-packages","title":"Step 1: Install the Packages","text":"<pre><code># ElBruno for offline, xUnit-native evaluation\ndotnet add package ElBruno.AI.Evaluation\ndotnet add package ElBruno.AI.Evaluation.Xunit\ndotnet add package ElBruno.AI.Evaluation.Reporting\n\n# OR: Microsoft for LLM-powered evaluation\ndotnet add package Microsoft.Extensions.AI.Evaluation\n</code></pre>"},{"location":"blog/01-introducing-elbruno-ai-evaluation/#step-2-create-a-golden-dataset","title":"Step 2: Create a Golden Dataset","text":"<p>A golden dataset is your ground truth. It's a collection of input-output pairs that define correct behavior.</p> <pre><code>var dataset = new GoldenDataset\n{\n    Name = \"Support Bot\",\n    Version = \"1.0.0\",\n    Description = \"Golden examples for support bot evaluation\"\n};\n\ndataset.AddExample(new GoldenExample\n{\n    Input = \"How do I reset my password?\",\n    ExpectedOutput = \"Visit the account settings page and click 'Reset Password'. You'll receive an email with instructions.\",\n    Tags = new() { \"onboarding\", \"account\" }\n});\n\ndataset.AddExample(new GoldenExample\n{\n    Input = \"What's your refund policy?\",\n    ExpectedOutput = \"We offer 30-day refunds on all purchases, no questions asked.\",\n    Tags = new() { \"billing\", \"policy\" }\n});\n\nvar loader = new JsonDatasetLoader();\nawait loader.SaveAsync(dataset, \"support-dataset.json\");\n</code></pre>"},{"location":"blog/01-introducing-elbruno-ai-evaluation/#step-3-set-up-your-chat-client","title":"Step 3: Set Up Your Chat Client","text":"<pre><code>using Microsoft.Extensions.AI;\nusing ElBruno.AI.Evaluation;\n\n// Use your favorite LLM provider (OpenAI, Anthropic, etc.)\nvar chatClient = new OllamaChatClient(\n    new Uri(\"http://localhost:11434\"),\n    \"neural-chat\"\n);\n</code></pre>"},{"location":"blog/01-introducing-elbruno-ai-evaluation/#step-4-build-an-evaluation-pipeline","title":"Step 4: Build an Evaluation Pipeline","text":"<pre><code>var evaluators = new List&lt;IEvaluator&gt;\n{\n    new RelevanceEvaluator(threshold: 0.6),\n    new FactualityEvaluator(threshold: 0.8),\n    new SafetyEvaluator(threshold: 0.9)\n};\n\nvar pipeline = new EvaluationPipelineBuilder()\n    .WithChatClient(chatClient)\n    .WithDataset(dataset)\n    .AddEvaluator(evaluators[0])\n    .AddEvaluator(evaluators[1])\n    .AddEvaluator(evaluators[2])\n    .Build();\n\nvar run = await pipeline.RunAsync();\n</code></pre>"},{"location":"blog/01-introducing-elbruno-ai-evaluation/#step-5-see-the-results","title":"Step 5: See the Results","text":"<pre><code>Console.WriteLine($\"Run completed: {run.Results.Count} examples evaluated\");\n\nforeach (var result in run.Results)\n{\n    Console.WriteLine($\"  Overall Score: {result.Score:F2} [{(result.Passed ? \"PASS\" : \"FAIL\")}]\");\n    Console.WriteLine($\"  Details: {result.Details}\");\n\n    foreach (var metric in result.MetricScores)\n    {\n        Console.WriteLine($\"    {metric.Key}: {metric.Value.Value:F2}\");\n    }\n}\n</code></pre>"},{"location":"blog/01-introducing-elbruno-ai-evaluation/#how-they-compare","title":"How They Compare","text":"<p>Microsoft.Extensions.AI.Evaluation (Official)</p> <ul> <li>\u2705 LLM-powered evaluators (Relevance, Completeness, Fluency, Groundedness, etc.)</li> <li>\u2705 Agent-focused evaluators (IntentResolution, TaskAdherence, ToolCallAccuracy)</li> <li>\u2705 Azure AI Foundry safety analysis</li> <li>\u2705 HTML report generation</li> <li>\u274c Requires external LLM calls</li> <li>\u274c No golden dataset management</li> <li>\u274c No xUnit integration</li> </ul> <p>ElBruno.AI.Evaluation (Complementary)</p> <ul> <li>\u2705 Deterministic evaluators (Hallucination, Factuality, Relevance, Coherence, Safety)</li> <li>\u2705 Golden dataset versioning and management</li> <li>\u2705 Synthetic data generation</li> <li>\u2705 xUnit-native test integration</li> <li>\u2705 Offline, zero external dependencies</li> <li>\u274c Not LLM-powered</li> <li>\u274c Simpler, less nuanced than Microsoft's evaluators</li> </ul>"},{"location":"blog/01-introducing-elbruno-ai-evaluation/#whats-next","title":"What's Next","text":"<p>This is the beginning of a developer journey through AI testing in .NET:</p> <ol> <li>Testing AI in .NET: The Landscape (this post) \u2014 Overview of both toolkits and when to use each</li> <li>Building Your Test Foundation: Golden Datasets &amp; Synthetic Data \u2014 Preparing test data with ElBruno</li> <li>Evaluators: From Quick Checks to Deep Analysis \u2014 Layering ElBruno's deterministic evaluators with Microsoft's LLM-powered ones</li> <li>AI Testing in Your CI Pipeline \u2014 Integrating both toolkits into automated tests</li> <li>Production AI Evaluation: Combining Both Toolkits \u2014 End-to-end pipeline using both frameworks together</li> <li>Generating Synthetic Test Data for AI Evaluation (NEW) \u2014 Deep dive into ElBruno's synthetic data generation</li> <li>A Guide to Choosing the Right Evaluators for Your AI App (NEW) \u2014 Evaluator selection by scenario</li> </ol> <p>Recommendation: Start with ElBruno for fast iteration, deterministic baselines, and test automation. Graduate to Microsoft when you need nuanced quality judgment or advanced safety analysis. Use both in your production pipeline.</p>"},{"location":"blog/01-introducing-elbruno-ai-evaluation/#try-it-yourself","title":"Try It Yourself","text":"<p>Ready to test your first LLM application? Head to the GitHub repository for complete samples and documentation. The <code>ChatbotEvaluation</code> and <code>RagEvaluation</code> samples show real-world patterns.</p> <p>Start small: create a 5-example golden dataset, run your LLM against it, and see which evaluators matter most for your use case. You'll be surprised how quickly you can build confidence in AI quality.</p> <p>ElBruno.AI.Evaluation is open source and actively maintained. Questions? Feedback? Open an issue on GitHub or reach out on social media.</p>"},{"location":"blog/01-introducing-elbruno-ai-evaluation/#about-the-author","title":"\ud83d\udc68\u200d\ud83d\udcbb About the Author","text":"<p>Bruno Capuano is a Microsoft MVP and AI enthusiast who builds practical tools for .NET developers. This blog is part of a 7-part series on AI evaluation.</p> <p>\ud83c\udf1f Found this helpful? Let's connect:</p> <ul> <li>\ud83d\udcd8 Read more on my blog \u2014 Deep technical articles on AI &amp; .NET</li> <li>\ud83c\udfa5 Watch video tutorials on YouTube \u2014 Demos and live coding</li> <li>\ud83d\udcbc Connect on LinkedIn \u2014 Professional updates</li> <li>\ud83d\udc26 Follow on Twitter/X \u2014 Quick tips and announcements</li> <li>\ud83c\udf99\ufe0f No Tiene Nombre Podcast \u2014 Tech talks in Spanish</li> <li>\ud83d\udcbb Explore more projects on GitHub \u2014 Open-source AI tools</li> </ul> <p>\u2b50 If this series is helping you build better AI applications, give the repo a star and share it with your team!</p>"},{"location":"blog/02-golden-datasets-for-ai-testing/","title":"Golden Datasets for AI Testing","text":""},{"location":"blog/02-golden-datasets-for-ai-testing/#building-your-test-foundation-golden-datasets-synthetic-data","title":"Building Your Test Foundation: Golden Datasets &amp; Synthetic Data","text":""},{"location":"blog/02-golden-datasets-for-ai-testing/#before-you-evaluate","title":"Before You Evaluate","text":"<p>You can't evaluate without something to evaluate against. That something is a golden dataset\u2014a curated collection of input-output pairs representing correct behavior. It's your AI's ground truth.</p> <p>But here's the problem: creating golden datasets manually is tedious. You need dozens or hundreds of examples. ElBruno.AI.Evaluation addresses this with two strategies:</p> <ol> <li>Golden Datasets \u2014 Curated, versioned collections (your hand-crafted ground truth)</li> <li>Synthetic Data \u2014 Automatically generated test examples (deterministic or LLM-powered)</li> </ol> <p>Important: Microsoft.Extensions.AI.Evaluation assumes you already have a golden dataset. ElBruno provides the tools to create and manage one. This is a critical gap ElBruno fills.</p>"},{"location":"blog/02-golden-datasets-for-ai-testing/#golden-datasets-your-source-of-truth","title":"Golden Datasets: Your Source of Truth","text":"<p>A golden dataset has three layers:</p> <ol> <li>Examples \u2014 Individual test cases with input, expected output, and optional context</li> <li>Metadata \u2014 Name, version, description, and tags for organization</li> <li>Summaries \u2014 Quick statistics about what you've got</li> </ol>"},{"location":"blog/02-golden-datasets-for-ai-testing/#synthetic-data-generate-dont-hand-craft","title":"Synthetic Data: Generate, Don't Hand-Craft","text":"<p>Creating 100 golden examples by hand? No thanks. ElBruno's <code>SyntheticDatasetBuilder</code> generates test data for you. Three strategies:</p>"},{"location":"blog/02-golden-datasets-for-ai-testing/#1-deterministic-template-based-generation","title":"1. Deterministic (Template-Based) Generation","text":"<p>Fast, reproducible, no LLM calls:</p> <pre><code>var synthetic = new SyntheticDatasetBuilder(\"qa-pairs\")\n    .WithTemplate(TemplateType.QA)\n    .Generate(50);  // 50 deterministic examples\n\n// Output: Standard Q&amp;A pairs like:\n// Q: \"How do I reset my password?\"\n// A: \"Visit Settings &gt; Account &gt; Reset Password.\"\n</code></pre> <p>Use this for: Quick iteration, CI/CD, cost-sensitive scenarios, reproducible baselines.</p>"},{"location":"blog/02-golden-datasets-for-ai-testing/#2-llm-powered-generation","title":"2. LLM-Powered Generation","text":"<p>Diverse, nuanced, uses your chat client:</p> <pre><code>using var http = new HttpClient();\nvar chatClient = new OpenAIChatClient(\"gpt-4o-mini\", apiKey);\n\nvar synthetic = new SyntheticDatasetBuilder(\"customer-support\")\n    .WithTemplate(TemplateType.QA)\n    .WithLLMGenerator(chatClient, count: 50)\n    .Build();\n\n// ChatClient generates 50 realistic examples\n</code></pre> <p>Use this for: Production datasets, edge cases, adversarial examples, diversity.</p>"},{"location":"blog/02-golden-datasets-for-ai-testing/#3-composite-generation","title":"3. Composite Generation","text":"<p>Blend deterministic + LLM for cost-effective diversity:</p> <pre><code>var synthetic = new SyntheticDatasetBuilder(\"rag-qa\")\n    .WithTemplate(TemplateType.RAG)\n    .WithDeterministicGenerator(25)    // 25 templated\n    .WithLLMGenerator(chatClient, 25)  // 25 LLM-generated\n    .Build();\n</code></pre> <p>Use this for: Balanced approach\u2014fast baseline + realistic edge cases.</p>"},{"location":"blog/02-golden-datasets-for-ai-testing/#built-in-templates","title":"Built-in Templates","text":"<p>ElBruno provides domain-specific templates:</p>"},{"location":"blog/02-golden-datasets-for-ai-testing/#qa-template","title":"Q&amp;A Template","text":"<pre><code>.WithTemplate(TemplateType.QA)\n// Generates: Question \u2192 Answer pairs\n</code></pre>"},{"location":"blog/02-golden-datasets-for-ai-testing/#rag-template","title":"RAG Template","text":"<pre><code>.WithTemplate(TemplateType.RAG)\n// Generates: Query \u2192 Context + Answer (for retrieval scenarios)\n</code></pre>"},{"location":"blog/02-golden-datasets-for-ai-testing/#adversarial-template","title":"Adversarial Template","text":"<pre><code>.WithTemplate(TemplateType.Adversarial)\n// Generates: Edge cases, trick questions, ambiguous inputs\n</code></pre>"},{"location":"blog/02-golden-datasets-for-ai-testing/#domain-template","title":"Domain Template","text":"<pre><code>.WithTemplate(TemplateType.Domain, new DomainConfig { \n    Industry = \"FinServ\",\n    Topics = new[] { \"lending\", \"mortgages\", \"compliance\" }\n})\n// Generates: Domain-specific examples with context\n</code></pre>"},{"location":"blog/02-golden-datasets-for-ai-testing/#end-to-end-example-generate-evaluate","title":"End-to-End Example: Generate \u2192 Evaluate","text":"<pre><code>using ElBruno.AI.Evaluation;\nusing ElBruno.AI.Evaluation.SyntheticData;\n\n// Step 1: Generate synthetic data\nvar generator = new SyntheticDatasetBuilder(\"customer-support-v1\")\n    .WithTemplate(TemplateType.QA)\n    .WithLLMGenerator(chatClient, 100)\n    .WithVersion(\"1.0.0\")\n    .Build();\n\nawait DatasetLoader.SaveAsync(generator, \"dataset-v1.0.0.json\");\n\n// Step 2: Evaluate against it\nvar dataset = await DatasetLoader.LoadAsync(\"dataset-v1.0.0.json\");\n\nvar evaluators = new List&lt;IEvaluator&gt;\n{\n    new RelevanceEvaluator(0.7),\n    new FactualityEvaluator(0.8),\n    new SafetyEvaluator(0.95)\n};\n\nvar pipeline = new EvaluationPipelineBuilder()\n    .WithChatClient(chatClient)\n    .WithDataset(dataset)\n    .ForEach(evaluators, e =&gt; pipeline.AddEvaluator(e))\n    .Build();\n\nvar results = await pipeline.RunAsync();\nConsole.WriteLine($\"Pass Rate: {results.PassRate:P0}\");\n</code></pre>"},{"location":"blog/02-golden-datasets-for-ai-testing/#versioning-your-datasets","title":"Versioning Your Datasets","text":"<p>Always version datasets like code:</p> <pre><code>// v1.0.0 \u2014 Initial synthetic dataset\nvar v1 = new SyntheticDatasetBuilder(\"support-bot\")\n    .WithVersion(\"1.0.0\")\n    .WithTemplate(TemplateType.QA)\n    .Generate(50)\n    .Build();\n\n// v1.1.0 \u2014 Added adversarial examples\nvar v2 = new SyntheticDatasetBuilder(\"support-bot\")\n    .WithVersion(\"1.1.0\")\n    .WithTemplate(TemplateType.QA)\n    .Generate(50)\n    .WithTemplate(TemplateType.Adversarial)\n    .Generate(20)\n    .Build();\n\n// v2.0.0 \u2014 LLM-generated for production\nvar v3 = new SyntheticDatasetBuilder(\"support-bot\")\n    .WithVersion(\"2.0.0\")\n    .WithLLMGenerator(chatClient, 200)\n    .Build();\n\nawait DatasetLoader.SaveAsync(v3, \"dataset-v2.0.0.json\");\n</code></pre> <p>Store in your repo:</p> <pre><code>/datasets\n  /support-bot\n    v1.0.0.json  (deterministic baseline)\n    v1.1.0.json  (+ adversarial)\n    v2.0.0.json  (LLM-generated, production)\n</code></pre> <pre><code>using ElBruno.AI.Evaluation.Datasets;\n\nvar dataset = new GoldenDataset\n{\n    Name = \"E-Commerce Support\",\n    Version = \"1.0.0\",\n    Description = \"Golden examples for e-commerce support bot\",\n    Tags = new() { \"support\", \"production\", \"v1\" }\n};\n\n// Add individual examples\ndataset.AddExample(new GoldenExample\n{\n    Input = \"Can I return an item after 30 days?\",\n    ExpectedOutput = \"Our standard return window is 30 days from purchase. Items returned after this period may not be eligible for refund, but contact our support team for special circumstances.\",\n    Context = \"Return policy allows 30 days. Extended returns possible with manager approval.\",\n    Tags = new() { \"returns\", \"policy\" },\n    Metadata = new()\n    {\n        { \"difficulty\", \"easy\" },\n        { \"priority\", \"high\" }\n    }\n});\n\ndataset.AddExample(new GoldenExample\n{\n    Input = \"What's the cheapest shipping option?\",\n    ExpectedOutput = \"We offer standard shipping (5-7 business days) at no extra cost for orders over $50. Express and overnight shipping are available at checkout.\",\n    Tags = new() { \"shipping\", \"pricing\" }\n});\n\ndataset.AddExample(new GoldenExample\n{\n    Input = \"Do you ship internationally?\",\n    ExpectedOutput = \"We currently ship to Canada and Mexico. For other countries, please contact our international sales team.\",\n    Tags = new() { \"shipping\", \"international\" }\n});\n</code></pre>"},{"location":"blog/02-golden-datasets-for-ai-testing/#organizing-with-tags-and-subsets","title":"Organizing with Tags and Subsets","text":"<p>Golden datasets often contain hundreds of examples. Tags let you organize and filter them:</p> <pre><code>// Get all billing-related examples\nvar billingExamples = dataset.GetByTag(\"billing\");\nConsole.WriteLine($\"Found {billingExamples.Count} billing examples\");\n\n// Create a test subset for canary testing\nvar canaryDataset = dataset.GetSubset(e =&gt; \n    e.Tags.Contains(\"priority\") &amp;&amp; e.Tags.Contains(\"high\")\n);\n\n// Only test easy examples in CI/CD, hard ones in staging\nvar easyExamples = dataset.GetSubset(e =&gt; \n    e.Metadata.TryGetValue(\"difficulty\", out var d) &amp;&amp; d == \"easy\"\n);\n</code></pre>"},{"location":"blog/02-golden-datasets-for-ai-testing/#json-format","title":"JSON Format","text":"<p>Under the hood, golden datasets are just JSON. Here's what a dataset looks like when persisted:</p> <pre><code>{\n  \"name\": \"Support Bot\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Golden examples for support bot\",\n  \"createdAt\": \"2025-02-23T15:30:00Z\",\n  \"tags\": [\"support\", \"production\"],\n  \"examples\": [\n    {\n      \"input\": \"How do I reset my password?\",\n      \"expectedOutput\": \"Visit Settings &gt; Account &gt; Reset Password. Check your email for instructions.\",\n      \"context\": \"Password reset is available in account settings.\",\n      \"tags\": [\"onboarding\", \"account\"],\n      \"metadata\": {\n        \"difficulty\": \"easy\",\n        \"priority\": \"high\"\n      }\n    },\n    {\n      \"input\": \"Can I use multiple discount codes?\",\n      \"expectedOutput\": \"No, only one discount code can be applied per order.\",\n      \"tags\": [\"billing\", \"discounts\"],\n      \"metadata\": {\n        \"difficulty\": \"easy\",\n        \"priority\": \"medium\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"blog/02-golden-datasets-for-ai-testing/#importing-from-csv","title":"Importing from CSV","text":"<p>Need to bulk-import examples? The <code>JsonDatasetLoader</code> supports CSV import:</p> <pre><code>var loader = new JsonDatasetLoader();\n\n// CSV must have columns: Input, ExpectedOutput, Context (optional), Tags (optional)\nvar dataset = await loader.LoadFromCsvAsync(\"support-examples.csv\");\n\n// Context and Tags are optional; missing values are handled gracefully\nawait loader.SaveAsync(dataset, \"support-dataset.json\");\n</code></pre> <p>CSV format example:</p> <pre><code>Input,ExpectedOutput,Context,Tags\n\"How do I reset my password?\",\"Visit Settings &gt; Account &gt; Reset Password.\",\"Password reset is available in account settings.\",\"onboarding;account\"\n\"What's your refund policy?\",\"30-day refunds on all purchases.\",\"Our standard return period is 30 days from purchase.\",\"billing;policy\"\n\"Do you ship internationally?\",\"We ship to Canada and Mexico only.\",\"International shipping is limited.\",\"shipping;international\"\n</code></pre>"},{"location":"blog/02-golden-datasets-for-ai-testing/#versioning-and-tracking","title":"Versioning and Tracking","text":"<p>Always version your datasets. When you improve examples or add new ones, bump the version:</p> <pre><code>// Version 1.0.0 \u2014 Initial release\nvar v1 = new GoldenDataset { Name = \"Support Bot\", Version = \"1.0.0\", ... };\n\n// Version 1.1.0 \u2014 Added international shipping examples\nvar v2 = new GoldenDataset { Name = \"Support Bot\", Version = \"1.1.0\", ... };\n\n// Version 2.0.0 \u2014 Complete rewrite with 200 examples\nvar v3 = new GoldenDataset { Name = \"Support Bot\", Version = \"2.0.0\", ... };\n</code></pre> <p>This matters because:</p> <ul> <li>Reproducibility \u2014 You can always re-evaluate against the exact dataset you used</li> <li>Comparison \u2014 See if quality improves when you move to a newer dataset</li> <li>Debugging \u2014 When a test fails, you know exactly which version of ground truth was used</li> </ul> <p>Store versions in your repository:</p> <pre><code>/datasets\n  /support-bot\n    v1.0.0.json\n    v1.1.0.json\n    v2.0.0.json (current)\n</code></pre>"},{"location":"blog/02-golden-datasets-for-ai-testing/#dataset-statistics","title":"Dataset Statistics","text":"<p>Quickly understand what's in your dataset:</p> <pre><code>var summary = dataset.GetSummary();\n\nConsole.WriteLine($\"Total examples: {summary.TotalExamples}\");\nConsole.WriteLine($\"With context: {summary.ExamplesWithContext}\");\nConsole.WriteLine($\"Tags: {string.Join(\", \", summary.UniqueTags)}\");\n</code></pre> <p>Output:</p> <pre><code>Total examples: 42\nWith context: 28\nTags: billing, shipping, returns, account, priority\n</code></pre>"},{"location":"blog/02-golden-datasets-for-ai-testing/#best-practices","title":"Best Practices","text":"<p>1. Keep Examples Simple and Focused Each example should test one thing. A long rambling expected output makes evaluation harder.</p> <p>2. Include Edge Cases Don't just add happy-path examples. Include misspellings, slang, confusing phrasing, unusual requests.</p> <p>3. Version Religiously Treat your golden dataset like source code. Commit it to git, tag releases, document changes.</p> <p>4. Use Metadata Liberally Add fields that help you slice and dice: difficulty, priority, category, language, edge_case_type.</p> <p>5. Involve Your Team Non-engineers (support, product) can contribute examples. Golden datasets are a bridge between technical and non-technical stakeholders.</p>"},{"location":"blog/02-golden-datasets-for-ai-testing/#try-it-yourself","title":"Try It Yourself","text":"<p>Start with a small dataset\u201410-15 examples covering your main use cases. Then:</p> <ol> <li>Run your LLM against them</li> <li>See which evaluators flag issues</li> <li>Refine the dataset based on what you learn</li> <li>Version it in git</li> <li>Use it as a baseline for future improvements</li> </ol> <p>Create your first dataset:</p> <pre><code>var loader = new JsonDatasetLoader();\n\nvar dataset = new GoldenDataset\n{\n    Name = \"My App\",\n    Version = \"1.0.0\"\n};\n\ndataset.AddExample(new GoldenExample\n{\n    Input = \"Your first test case\",\n    ExpectedOutput = \"What good looks like\"\n});\n\nawait loader.SaveAsync(dataset, \"my-dataset.json\");\n</code></pre> <p>That's it. You now have a foundation for rigorous AI evaluation.</p> <p>Next: Learn how to layer multiple evaluators\u2014from ElBruno's fast offline checks to Microsoft's LLM-powered quality judgment.</p>"},{"location":"blog/02-golden-datasets-for-ai-testing/#about-the-author","title":"\ud83d\udc68\u200d\ud83d\udcbb About the Author","text":"<p>Bruno Capuano is a Microsoft MVP and AI enthusiast who builds practical tools for .NET developers. This is Part 2 of a 7-part series on AI evaluation.</p> <p>\ud83c\udf1f Found this helpful? Let's connect:</p> <ul> <li>\ud83d\udcd8 Read more on my blog \u2014 Deep technical articles on AI &amp; .NET</li> <li>\ud83c\udfa5 Watch video tutorials on YouTube \u2014 Demos and live coding</li> <li>\ud83d\udcbc Connect on LinkedIn \u2014 Professional updates</li> <li>\ud83d\udc26 Follow on Twitter/X \u2014 Quick tips and announcements</li> <li>\ud83c\udf99\ufe0f No Tiene Nombre Podcast \u2014 Tech talks in Spanish</li> <li>\ud83d\udcbb Explore more projects on GitHub \u2014 Open-source AI tools</li> </ul> <p>\u2b50 If this series is helping you build better AI applications, give the repo a star and share it with your team!</p>"},{"location":"blog/03-ai-evaluators-deep-dive/","title":"AI Evaluators Deep Dive","text":""},{"location":"blog/03-ai-evaluators-deep-dive/#evaluators-from-quick-checks-to-deep-analysis","title":"Evaluators: From Quick Checks to Deep Analysis","text":"<p>Evaluation is layered. Not all scenarios need LLM-powered judgment. Some need fast, offline verification. Some need sophisticated reasoning. The art is knowing which layer to use and when.</p> <p>This post walks through four evaluation layers, using both ElBruno's deterministic evaluators and Microsoft's LLM-powered ones.</p>"},{"location":"blog/03-ai-evaluators-deep-dive/#layer-1-elbrunos-deterministic-evaluators-fast-offline-debuggable","title":"Layer 1: ElBruno's Deterministic Evaluators (Fast, Offline, Debuggable)","text":"<p>These are your first line of defense. No external calls. No cost. Transparent logic you can understand and tune.</p>"},{"location":"blog/03-ai-evaluators-deep-dive/#1-relevanceevaluator","title":"1. RelevanceEvaluator","text":"<p>Question: Does the output actually answer the input question?</p> <p>How it works: Uses cosine similarity to compare the terms in the input with the terms in the output. High term overlap = high relevance.</p> <p>Threshold: Default 0.6 (60% similarity)</p> <p>When to use: Always. This is your first line of defense against completely off-topic responses.</p> <pre><code>var evaluator = new RelevanceEvaluator(threshold: 0.6);\n\n// Good example\nvar result1 = await evaluator.EvaluateAsync(\n    input: \"What is the capital of France?\",\n    output: \"The capital of France is Paris, a city on the Seine River.\"\n);\nConsole.WriteLine(result1);\n// Output: [PASS] Score=0.85 \u2014 Cosine similarity between input and output terms: 0.850. Input terms: 5, Output terms: 11.\n\n// Bad example  \nvar result2 = await evaluator.EvaluateAsync(\n    input: \"What is the capital of France?\",\n    output: \"The weather today is sunny. I like pizza.\"\n);\nConsole.WriteLine(result2);\n// Output: [FAIL] Score=0.12 \u2014 Cosine similarity between input and output terms: 0.120. Input terms: 5, Output terms: 8.\n</code></pre> <p>Real-world scenario: Your support bot receives \"How do I upgrade my plan?\" It responds with an essay about weather patterns. RelevanceEvaluator catches this immediately.</p>"},{"location":"blog/03-ai-evaluators-deep-dive/#2-factualityevaluator","title":"2. FactualityEvaluator","text":"<p>Question: Are the claims in the output actually true (based on reference material)?</p> <p>How it works: Extracts claims (sentences) from the output, checks how many tokens overlap with the expected output. Token overlap &gt; 50% = claim is supported.</p> <p>Threshold: Default 0.8 (80% of claims supported)</p> <p>When to use: When you have ground-truth reference material. Essential for RAG systems, documentation bots, news summarization.</p> <pre><code>var evaluator = new FactualityEvaluator(threshold: 0.8);\n\nvar result = await evaluator.EvaluateAsync(\n    input: \"Tell me about Python\",\n    output: \"Python is a programming language created by Guido van Rossum in 1991. It's known for readability.\",\n    expectedOutput: \"Python was created by Guido van Rossum. It became popular for web development and data science.\"\n);\nConsole.WriteLine(result);\n// Output: [PASS] Score=0.87 \u2014 2/3 claims supported (86.67%). \n// The \"created by Guido van Rossum\" claim is supported. The \"1991\" and \"readability\" claims have partial support.\n</code></pre> <p>Real-world scenario: Your RAG chatbot answers questions using company documents. FactualityEvaluator ensures answers stay grounded in the documents, not LLM hallucinations.</p>"},{"location":"blog/03-ai-evaluators-deep-dive/#3-coherenceevaluator","title":"3. CoherenceEvaluator","text":"<p>Question: Does the output make logical sense? Are sentences complete? Any contradictions?</p> <p>How it works: Checks for:</p> <ul> <li>Incomplete sentences (&lt; 3 words)</li> <li>Contradictory phrases (\"is/is not\" both present)</li> <li>Excessive repetition (same sentence appearing multiple times)</li> </ul> <p>Threshold: Default 0.7</p> <p>When to use: Catching obviously broken outputs, detecting when the LLM starts generating garbage.</p> <pre><code>var evaluator = new CoherenceEvaluator(threshold: 0.7);\n\nvar result1 = await evaluator.EvaluateAsync(\n    input: \"Explain machine learning\",\n    output: \"Machine learning is. A is is not. The the the algorithm algorithm algorithm learns patterns from data.\"\n);\nConsole.WriteLine(result1);\n// Output: [FAIL] Score=0.52 \u2014 Issues: 1 incomplete sentence(s), 1 potential contradiction(s), high repetition (66%). \n\nvar result2 = await evaluator.EvaluateAsync(\n    input: \"Explain machine learning\",\n    output: \"Machine learning is a subset of artificial intelligence. Algorithms learn from data without being explicitly programmed. This enables systems to improve with experience.\"\n);\nConsole.WriteLine(result2);\n// Output: [PASS] Score=1.00 \u2014 Output is coherent (3 sentence(s), no issues detected).\n</code></pre> <p>Real-world scenario: Your LLM starts hallucinating due to a prompt injection attack. CoherenceEvaluator detects the incoherent garbage output.</p>"},{"location":"blog/03-ai-evaluators-deep-dive/#4-hallucinationevaluator","title":"4. HallucinationEvaluator","text":"<p>Question: Is the output making up facts not present in the source material?</p> <p>How it works: Builds a grounding corpus from the input + expected output. Counts how many tokens in the output appear in the grounding corpus. High overlap = low hallucination.</p> <p>Threshold: Default 0.7 (70% of tokens grounded)</p> <p>When to use: RAG systems, question-answering over documents. Essential for reducing AI-generated falsehoods.</p> <pre><code>var evaluator = new HallucinationEvaluator(threshold: 0.7);\n\nvar result = await evaluator.EvaluateAsync(\n    input: \"What does the customer say about pricing?\",\n    output: \"The customer says pricing is reasonable and competitive. They also mentioned liking the UI.\",\n    expectedOutput: \"The customer says pricing is reasonable and offers good value for the features provided.\"\n);\nConsole.WriteLine(result);\n// Output: [PASS] Score=0.74 \u2014 Keyword overlap: 11/15 tokens grounded (73%). 4 potentially hallucinated token(s).\n// \"competitive\" and \"UI\" are not in the source material but the core message (\"pricing is reasonable\") is grounded.\n</code></pre> <p>Real-world scenario: Your RAG chatbot adds details that sound plausible but aren't in the documents. HallucinationEvaluator catches this.</p>"},{"location":"blog/03-ai-evaluators-deep-dive/#5-safetyevaluator","title":"5. SafetyEvaluator","text":"<p>Question: Does the output contain unsafe content?</p> <p>How it works: Checks for:</p> <ul> <li>Profanity and blocked terms (configurable)</li> <li>PII patterns: emails, phone numbers, SSNs</li> <li>Harmful instructions</li> </ul> <p>Threshold: Default 0.9 (allows minimal violations)</p> <p>When to use: Always, especially for customer-facing systems. Non-negotiable for compliance.</p> <pre><code>var evaluator = new SafetyEvaluator(threshold: 0.9);\n\nvar result1 = await evaluator.EvaluateAsync(\n    input: \"Hello\",\n    output: \"This is a damn problem. Contact me at john.doe@example.com\"\n);\nConsole.WriteLine(result1);\n// Output: [FAIL] Score=0.70 \u2014 2 violation(s): Blocked terms: damn; Email address detected.\n\nvar result2 = await evaluator.EvaluateAsync(\n    input: \"Help\",\n    output: \"I can help you with that. Please be patient.\"\n);\nConsole.WriteLine(result2);\n// Output: [PASS] Score=1.00 \u2014 No safety violations detected.\n\n// Custom blocklist\nvar customEvaluator = new SafetyEvaluator(\n    threshold: 0.9,\n    additionalBlocklist: new[] { \"proprietary\", \"confidential\" }\n);\n</code></pre> <p>Real-world scenario: Your bot accidental exposes customer email addresses or SSNs from training data. SafetyEvaluator catches this before it reaches production.</p>"},{"location":"blog/03-ai-evaluators-deep-dive/#combining-evaluators","title":"Combining Evaluators","text":"<p>Real-world evaluation is layered and hybrid:</p> <pre><code>// Layer 1: Fast ElBruno evaluators (offline, CI/CD-friendly)\nvar fastPass = new List&lt;IEvaluator&gt;\n{\n    new RelevanceEvaluator(0.7),\n    new CoherenceEvaluator(0.7),\n    new SafetyEvaluator(0.95)\n};\n\n// If fast pass succeeds, optionally upgrade to LLM-powered\n// Layer 2: Microsoft's LLM evaluators (nuanced judgment)\nvar deepPass = new List&lt;IEvaluator&gt;\n{\n    new MicrosoftRelevanceEvaluator(),    // LLM-powered\n    new MicrosoftCompletenessEvaluator(),\n    new MicrosoftGroundednessEvaluator()\n};\n\n// Example: Use ElBruno first, Microsoft for edge cases\nforeach (var result in run.Results)\n{\n    if (result.Score &lt; 0.7)\n    {\n        // Failed fast checks\u2014investigate with LLM\n        var deepResult = await LLMEvaluate(result);\n        // Helps debug *why* it failed\n    }\n}\n</code></pre> <p>Pattern: ElBruno for gate-keeping, Microsoft for insights.</p>"},{"location":"blog/03-ai-evaluators-deep-dive/#creating-custom-evaluators","title":"Creating Custom Evaluators","text":"<p>The <code>IEvaluator</code> interface is simple. Build your own:</p> <pre><code>public class LengthEvaluator : IEvaluator\n{\n    private readonly int _minWords;\n\n    public LengthEvaluator(int minWords = 10)\n    {\n        _minWords = minWords;\n    }\n\n    public Task&lt;EvaluationResult&gt; EvaluateAsync(\n        string input, \n        string output, \n        string? expectedOutput = null, \n        CancellationToken ct = default)\n    {\n        var wordCount = output.Split(' ', StringSplitOptions.RemoveEmptyEntries).Length;\n        var score = Math.Min(1.0, (double)wordCount / _minWords);\n\n        return Task.FromResult(new EvaluationResult\n        {\n            Score = score,\n            Passed = wordCount &gt;= _minWords,\n            Details = $\"Output has {wordCount} words (threshold: {_minWords})\",\n            MetricScores = new()\n            {\n                [\"length\"] = new MetricScore \n                { \n                    Name = \"Length\", \n                    Value = score, \n                    Threshold = 1.0 \n                }\n            }\n        });\n    }\n}\n\n// Use it\nvar customEval = new LengthEvaluator(minWords: 50);\nvar result = await customEval.EvaluateAsync(\"Explain quantum computing\", \"...\");\n</code></pre>"},{"location":"blog/03-ai-evaluators-deep-dive/#choosing-thresholds","title":"Choosing Thresholds","text":"<p>Different use cases need different thresholds:</p> Scenario Relevance Factuality Coherence Hallucination Safety Support Bot 0.7 0.8 0.8 0.7 0.95 Content Generation 0.6 0.5 0.8 0.6 0.9 RAG Q&amp;A 0.7 0.9 0.7 0.85 0.95 Brainstorming 0.5 0.3 0.6 0.4 0.85 <p>Start conservative (high thresholds), then relax as you understand what's normal for your LLM.</p>"},{"location":"blog/03-ai-evaluators-deep-dive/#try-it-yourself","title":"Try It Yourself","text":"<p>Evaluate your first LLM output:</p> <pre><code>var evaluators = new List&lt;IEvaluator&gt;\n{\n    new RelevanceEvaluator(),\n    new FactualityEvaluator(),\n    new CoherenceEvaluator(),\n    new HallucinationEvaluator(),\n    new SafetyEvaluator()\n};\n\nstring input = \"Your test question\";\nstring output = \"Your LLM's response\";\nstring expected = \"Reference material\";\n\nforeach (var evaluator in evaluators)\n{\n    var result = await evaluator.EvaluateAsync(input, output, expected);\n    Console.WriteLine($\"{evaluator.GetType().Name}: {result}\");\n}\n</code></pre> <p>Next: Integrate these evaluators into your xUnit test suite for automated quality gates.</p>"},{"location":"blog/03-ai-evaluators-deep-dive/#about-the-author","title":"\ud83d\udc68\u200d\ud83d\udcbb About the Author","text":"<p>Bruno Capuano is a Microsoft MVP and AI enthusiast who builds practical tools for .NET developers. This is Part 3 of a 7-part series on AI evaluation.</p> <p>\ud83c\udf1f Found this helpful? Let's connect:</p> <ul> <li>\ud83d\udcd8 Read more on my blog \u2014 Deep technical articles on AI &amp; .NET</li> <li>\ud83c\udfa5 Watch video tutorials on YouTube \u2014 Demos and live coding</li> <li>\ud83d\udcbc Connect on LinkedIn \u2014 Professional updates</li> <li>\ud83d\udc26 Follow on Twitter/X \u2014 Quick tips and announcements</li> <li>\ud83c\udf99\ufe0f No Tiene Nombre Podcast \u2014 Tech talks in Spanish</li> <li>\ud83d\udcbb Explore more projects on GitHub \u2014 Open-source AI tools</li> </ul> <p>\u2b50 If this series is helping you build better AI applications, give the repo a star and share it with your team!</p>"},{"location":"blog/04-ai-testing-with-xunit/","title":"AI Testing with xUnit","text":""},{"location":"blog/04-ai-testing-with-xunit/#ai-testing-in-your-ci-pipeline","title":"AI Testing in Your CI Pipeline","text":"<p>AI evaluation doesn't have to be separate from your normal testing workflow. ElBruno.AI.Evaluation integrates directly into xUnit\u2014your existing test infrastructure. Both toolkits can be used in CI/CD, but with different patterns.</p> <p>ElBruno: xUnit-native assertions. Deterministic. Fast gates. Perfect for regression detection in CI/CD. Microsoft: MSTest/xUnit compatible but more reporting-focused. Great for quality dashboards, less for quick gates.</p>"},{"location":"blog/04-ai-testing-with-xunit/#the-aievaluationtest-attribute","title":"The AIEvaluationTest Attribute","text":"<p>Mark any test as an AI evaluation test:</p> <pre><code>using ElBruno.AI.Evaluation.Evaluators;\nusing ElBruno.AI.Evaluation.Xunit;\nusing Xunit;\n\npublic class SupportBotTests\n{\n    private readonly IChatClient _chatClient;\n\n    public SupportBotTests()\n    {\n        _chatClient = new OllamaChatClient(\n            new Uri(\"http://localhost:11434\"),\n            \"neural-chat\"\n        );\n    }\n\n    [Fact]\n    [AIEvaluationTest(MinScore = 0.8, EvaluatorType = \"Relevance\")]\n    public async Task SupportBot_AnswersAccountQuestions()\n    {\n        var input = \"How do I reset my password?\";\n        var output = await _chatClient.CompleteAsync(input);\n\n        var evaluator = new RelevanceEvaluator(threshold: 0.8);\n        var result = await evaluator.EvaluateAsync(input, output);\n\n        Assert.True(result.Passed);\n        Assert.True(result.Score &gt;= 0.8);\n    }\n}\n</code></pre> <p>The <code>AIEvaluationTest</code> attribute metadata tells the test runner (and your reports) what's being tested and the minimum acceptable score.</p>"},{"location":"blog/04-ai-testing-with-xunit/#fluent-assertions-with-aiassert","title":"Fluent Assertions with AIAssert","text":"<p><code>AIAssert</code> provides clear, expressive assertion methods:</p> <pre><code>[Fact]\npublic async Task SupportBot_RelevantAndFactual()\n{\n    var input = \"What's your refund policy?\";\n    var expectedOutput = \"30-day refunds on all purchases, no questions asked.\";\n    var output = await _chatClient.CompleteAsync(input);\n\n    var relevance = new RelevanceEvaluator(0.7);\n    var factuality = new FactualityEvaluator(0.8);\n\n    var relevanceResult = await relevance.EvaluateAsync(input, output, expectedOutput);\n    var factualityResult = await factuality.EvaluateAsync(input, output, expectedOutput);\n\n    // Individual assertions\n    AIAssert.PassesThreshold(relevanceResult, 0.7);\n    AIAssert.PassesThreshold(factualityResult, 0.8);\n\n    // All metrics must pass\n    AIAssert.AllMetricsPass(relevanceResult);\n    AIAssert.AllMetricsPass(factualityResult);\n}\n</code></pre> <p>When an assertion fails, you get clear output:</p> <pre><code>Test failed: Evaluation score 0.62 is below threshold 0.70. \nCosine similarity between input and output terms: 0.620. \nInput terms: 5, Output terms: 8.\n</code></pre>"},{"location":"blog/04-ai-testing-with-xunit/#test-patterns","title":"Test Patterns","text":""},{"location":"blog/04-ai-testing-with-xunit/#pattern-1-single-evaluator-per-test","title":"Pattern 1: Single Evaluator Per Test","text":"<p>Organize tests around evaluators:</p> <pre><code>public class SupportBotEvaluationTests\n{\n    private readonly IChatClient _chatClient;\n    private readonly GoldenDataset _dataset;\n\n    [Theory]\n    [InlineData(\"How do I reset my password?\", \"Visit Settings &gt; Account &gt; Reset Password.\")]\n    [InlineData(\"What's your refund policy?\", \"30-day refunds on all purchases.\")]\n    public async Task RelevanceTests(string input, string expected)\n    {\n        var output = await _chatClient.CompleteAsync(input);\n        var evaluator = new RelevanceEvaluator(0.7);\n        var result = await evaluator.EvaluateAsync(input, output, expected);\n\n        AIAssert.PassesThreshold(result, 0.7);\n    }\n\n    [Theory]\n    [InlineData(\"How do I reset my password?\", \"Visit Settings &gt; Account &gt; Reset Password.\")]\n    [InlineData(\"What's your refund policy?\", \"30-day refunds on all purchases.\")]\n    public async Task FactualityTests(string input, string expected)\n    {\n        var output = await _chatClient.CompleteAsync(input);\n        var evaluator = new FactualityEvaluator(0.8);\n        var result = await evaluator.EvaluateAsync(input, output, expected);\n\n        AIAssert.PassesThreshold(result, 0.8);\n    }\n}\n</code></pre> <p>Benefit: Easy to see which evaluator is failing. `dotnet test --filter \"Relevance\" runs only relevance tests.</p>"},{"location":"blog/04-ai-testing-with-xunit/#pattern-2-golden-dataset-tests","title":"Pattern 2: Golden Dataset Tests","text":"<p>Load examples from your golden dataset:</p> <pre><code>public class GoldenDatasetTests : IAsyncLifetime\n{\n    private readonly IChatClient _chatClient;\n    private GoldenDataset _dataset;\n\n    public async Task InitializeAsync()\n    {\n        _chatClient = new OllamaChatClient(\n            new Uri(\"http://localhost:11434\"),\n            \"neural-chat\"\n        );\n\n        var loader = new JsonDatasetLoader();\n        _dataset = await loader.LoadAsync(\"support-bot.json\");\n    }\n\n    public Task DisposeAsync() =&gt; Task.CompletedTask;\n\n    [Theory]\n    [MemberData(nameof(GetGoldenExamples))]\n    public async Task EvaluateAgainstGoldenDataset(GoldenExample example)\n    {\n        var output = await _chatClient.CompleteAsync(example.Input);\n\n        var evaluators = new List&lt;IEvaluator&gt;\n        {\n            new RelevanceEvaluator(0.7),\n            new FactualityEvaluator(0.8),\n            new SafetyEvaluator(0.95)\n        };\n\n        foreach (var evaluator in evaluators)\n        {\n            var result = await evaluator.EvaluateAsync(\n                example.Input,\n                output,\n                example.ExpectedOutput\n            );\n\n            AIAssert.PassesThreshold(result, 0.7);\n        }\n    }\n\n    public static IEnumerable&lt;object[]&gt; GetGoldenExamples()\n    {\n        var loader = new JsonDatasetLoader();\n        var dataset = loader.LoadAsync(\"support-bot.json\").Result;\n\n        return dataset.Examples\n            .Select(e =&gt; new object[] { e })\n            .ToList();\n    }\n}\n</code></pre> <p>This runs one test per golden example. If your dataset has 50 examples, you get 50 test cases. Visual Studio Test Explorer shows each one:</p> <pre><code>\u2713 EvaluateAgainstGoldenDataset [0]\n\u2713 EvaluateAgainstGoldenDataset [1]\n\u2713 EvaluateAgainstGoldenDataset [2]\n\u2717 EvaluateAgainstGoldenDataset [15]  \u2190 Failed on refund question\n  Score: 0.52 &lt; threshold 0.70\n</code></pre>"},{"location":"blog/04-ai-testing-with-xunit/#pattern-3-tagged-subset-testing","title":"Pattern 3: Tagged Subset Testing","text":"<p>Run different evaluators for different question types:</p> <pre><code>[Theory]\n[MemberData(nameof(GetBillingExamples))]\npublic async Task BillingQuestionsAreFactual(GoldenExample example)\n{\n    var output = await _chatClient.CompleteAsync(example.Input);\n    var evaluator = new FactualityEvaluator(0.9);  // High bar for billing\n    var result = await evaluator.EvaluateAsync(example.Input, output, example.ExpectedOutput);\n\n    AIAssert.PassesThreshold(result, 0.9);\n}\n\n[Theory]\n[MemberData(nameof(GetGeneralExamples))]\npublic async Task GeneralQuestionsAreRelevant(GoldenExample example)\n{\n    var output = await _chatClient.CompleteAsync(example.Input);\n    var evaluator = new RelevanceEvaluator(0.6);  // Lower bar for general questions\n    var result = await evaluator.EvaluateAsync(example.Input, output);\n\n    AIAssert.PassesThreshold(result, 0.6);\n}\n\npublic static IEnumerable&lt;object[]&gt; GetBillingExamples()\n{\n    var loader = new JsonDatasetLoader();\n    var dataset = loader.LoadAsync(\"support-bot.json\").Result;\n    return dataset.GetByTag(\"billing\").Select(e =&gt; new object[] { e }).ToList();\n}\n\npublic static IEnumerable&lt;object[]&gt; GetGeneralExamples()\n{\n    var loader = new JsonDatasetLoader();\n    var dataset = loader.LoadAsync(\"support-bot.json\").Result;\n    return dataset.GetByTag(\"general\").Select(e =&gt; new object[] { e }).ToList();\n}\n</code></pre>"},{"location":"blog/04-ai-testing-with-xunit/#cicd-integration","title":"CI/CD Integration","text":"<p>Your AI tests run exactly like regular tests:</p> <p>GitHub Actions (ElBruno Pattern):</p> <pre><code>name: AI Quality Gate\non: [push, pull_request]\n\njobs:\n  evaluate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-dotnet@v3\n        with:\n          dotnet-version: '8.0'\n\n      # Run ElBruno tests (fast, offline gate)\n      - run: dotnet test tests/AI.Evaluation.Tests/ --logger \"trx\"\n\n      # If passed, optionally run Microsoft evaluations for reporting\n      - name: Run Microsoft Evaluations (Optional)\n        if: success()\n        env:\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        run: dotnet run --project tests/DeepEval.Tests/\n\n      - uses: dorny/test-reporter@v1\n        if: always()\n        with:\n          name: AI Evaluation Results\n          path: '**/test-results.trx'\n          reporter: 'dotnet trx'\n</code></pre> <p>Pattern: ElBruno first (fast gate), Microsoft optional (detailed analysis).</p>"},{"location":"blog/04-ai-testing-with-xunit/#test-organization","title":"Test Organization","text":"<p>Structure your test project like this:</p> <pre><code>tests/\n  ElBruno.AI.Evaluation.Tests/\n    Evaluators/\n      RelevanceEvaluatorTests.cs\n      FactualityEvaluatorTests.cs\n      SafetyEvaluatorTests.cs\n    Integration/\n      ChatbotEvaluationTests.cs\n      RagEvaluationTests.cs\n    Datasets/\n      GoldenDatasetTests.cs\n</code></pre> <p>Each test file focuses on one scenario or evaluator.</p>"},{"location":"blog/04-ai-testing-with-xunit/#debugging-failed-tests","title":"Debugging Failed Tests","text":"<p>When an AI evaluation test fails, here's how to investigate:</p> <pre><code>[Fact]\npublic async Task DebugFailedEvaluation()\n{\n    var input = \"How do I reset my password?\";\n    var output = await _chatClient.CompleteAsync(input);\n    var expected = \"Visit account settings and click reset password.\";\n\n    var evaluators = new List&lt;IEvaluator&gt;\n    {\n        new RelevanceEvaluator(0.7),\n        new FactualityEvaluator(0.8),\n        new SafetyEvaluator(0.95)\n    };\n\n    foreach (var evaluator in evaluators)\n    {\n        var result = await evaluator.EvaluateAsync(input, output, expected);\n\n        Console.WriteLine($\"Evaluator: {evaluator.GetType().Name}\");\n        Console.WriteLine($\"  Score: {result.Score:F2}\");\n        Console.WriteLine($\"  Passed: {result.Passed}\");\n        Console.WriteLine($\"  Details: {result.Details}\");\n        Console.WriteLine();\n\n        foreach (var (metric, score) in result.MetricScores)\n        {\n            Console.WriteLine($\"  Metric {metric}: {score.Value:F2} (threshold: {score.Threshold:F2})\");\n        }\n    }\n}\n</code></pre> <p>xUnit captures console output, so you can see debugging info right in the test output.</p>"},{"location":"blog/04-ai-testing-with-xunit/#try-it-yourself","title":"Try It Yourself","text":"<p>Create your first AI evaluation test:</p> <pre><code>[Fact]\n[AIEvaluationTest(MinScore = 0.7)]\npublic async Task MyFirstAITest()\n{\n    var chatClient = new SimpleChatClient(); // Use your LLM\n    var input = \"What is 2 + 2?\";\n    var output = await chatClient.CompleteAsync(input);\n\n    var evaluator = new RelevanceEvaluator(0.7);\n    var result = await evaluator.EvaluateAsync(input, output);\n\n    AIAssert.PassesThreshold(result, 0.7);\n}\n</code></pre> <p>Then run it:</p> <pre><code>dotnet test\n</code></pre> <p>You'll see:</p> <pre><code>Test Run Successful.\nTotal tests: 1\n     Passed: 1\n     Failed: 0\n</code></pre> <p>That's it. You now have AI quality as a first-class testing concern.</p> <p>Next: Build your complete production pipeline using both ElBruno and Microsoft toolkits together.</p>"},{"location":"blog/04-ai-testing-with-xunit/#about-the-author","title":"\ud83d\udc68\u200d\ud83d\udcbb About the Author","text":"<p>Bruno Capuano is a Microsoft MVP and AI enthusiast who builds practical tools for .NET developers. This is Part 4 of a 7-part series on AI evaluation.</p> <p>\ud83c\udf1f Found this helpful? Let's connect:</p> <ul> <li>\ud83d\udcd8 Read more on my blog \u2014 Deep technical articles on AI &amp; .NET</li> <li>\ud83c\udfa5 Watch video tutorials on YouTube \u2014 Demos and live coding</li> <li>\ud83d\udcbc Connect on LinkedIn \u2014 Professional updates</li> <li>\ud83d\udc26 Follow on Twitter/X \u2014 Quick tips and announcements</li> <li>\ud83c\udf99\ufe0f No Tiene Nombre Podcast \u2014 Tech talks in Spanish</li> <li>\ud83d\udcbb Explore more projects on GitHub \u2014 Open-source AI tools</li> </ul> <p>\u2b50 If this series is helping you build better AI applications, give the repo a star and share it with your team!</p>"},{"location":"blog/05-from-demo-to-production/","title":"From Demo to Production","text":""},{"location":"blog/05-from-demo-to-production/#production-ai-evaluation-combining-both-toolkits","title":"Production AI Evaluation: Combining Both Toolkits","text":"<p>You've learned each toolkit separately. Now let's build a complete production pipeline using both together. Neither toolkitalone is sufficient\u2014the real power comes from using them in combination.</p>"},{"location":"blog/05-from-demo-to-production/#the-hybrid-pattern","title":"The Hybrid Pattern","text":"<pre><code>Input Data\n    \u2193\n[ElBruno Synthetic Generator] \u2192 Creates test data if needed\n    \u2193\n[ElBruno Deterministic Evals] \u2192 Fast offline gate (CI/CD)\n    \u2193\n    IF PASS: [Optional: Microsoft LLM Evals] \u2192 Deep analysis for reporting\n    \u2193\n[ElBruno Baseline Snapshot] \u2192 Detect regressions\n    \u2193\n[Production Deployment]\n</code></pre> <p>Layers:</p> <ol> <li>Data layer: ElBruno generates or manages golden datasets</li> <li>Quality gate: ElBruno's deterministic evaluators (fast, offline)</li> <li>Deep analysis: Microsoft's LLM evaluators (optional, for insights)</li> <li>Regression: ElBruno's baseline snapshots (CI/CD safety net)</li> <li>Reporting: ElBruno's SQLite + exports, Microsoft's HTML reports</li> </ol>"},{"location":"blog/05-from-demo-to-production/#complete-production-workflow","title":"Complete Production Workflow","text":"<pre><code>public class ProductionEvaluationPipeline\n{\n    private readonly IChatClient _chatClient;\n    private readonly GoldenDataset _dataset;\n    private readonly BaselineSnapshot _baseline;\n    private readonly SqliteResultStore _store;\n    private readonly IMonitoringClient _monitoring;\n\n    public async Task&lt;DeploymentGate&gt; EvaluateBeforeDeployAsync(string newModelVersion)\n    {\n        // Step 1: Generate or load test data\n        var testData = await LoadOrGenerateDatasetAsync();\n\n        // Step 2: Run ElBruno deterministic evaluators (FAST, OFFLINE)\n        var elbrunoEvaluators = new List&lt;IEvaluator&gt;\n        {\n            new RelevanceEvaluator(0.7),\n            new HallucinationEvaluator(0.75),\n            new SafetyEvaluator(0.95),\n            new CoherenceEvaluator(0.7),\n            new FactualityEvaluator(0.8)\n        };\n\n        var elbrunoPipeline = new EvaluationPipelineBuilder()\n            .WithChatClient(_chatClient)\n            .WithDataset(testData)\n            .ForEach(elbrunoEvaluators, e =&gt; elbrunoPipeline.AddEvaluator(e))\n            .WithBaseline(_baseline)\n            .Build();\n\n        var regressionReport = await elbrunoPipeline.RunWithBaselineAsync();\n\n        // Step 3: Store results\n        await _store.SaveAsync(new EvaluationRun\n        {\n            ModelVersion = newModelVersion,\n            Results = regressionReport.Results,\n            RegressionDetected = regressionReport.HasRegressions,\n            StartedAt = DateTime.UtcNow\n        });\n\n        // Step 4: Check gate\u2014fail if regression\n        if (regressionReport.HasRegressions)\n        {\n            Console.WriteLine(\"\u274c REGRESSION DETECTED\");\n            foreach (var detail in regressionReport.RegressionDetails)\n            {\n                Console.WriteLine($\"  {detail.MetricName}: {detail.BaselineValue:F2} \u2192 {detail.CurrentValue:F2}\");\n            }\n            return DeploymentGate.Blocked;\n        }\n\n        // Step 5 (Optional): For critical releases, run Microsoft's LLM evaluators\n        if (ShouldRunDeepEvaluation(newModelVersion))\n        {\n            var deepResults = await RunMicrosoftEvaluatorsAsync(testData);\n            // Generate comprehensive HTML report for stakeholders\n            await GenerateMicrosoftReportAsync(deepResults);\n        }\n\n        // Step 6: Record metrics for monitoring\n        await RecordProductionMetricsAsync(regressionReport);\n\n        Console.WriteLine(\"\u2705 ALL GATES PASSED\");\n        return DeploymentGate.Approved;\n    }\n\n    private async Task RecordProductionMetricsAsync(RegressionReport report)\n    {\n        var metrics = new Dictionary&lt;string, double&gt;\n        {\n            [\"ai.relevance.avg\"] = report.Results.Average(r =&gt; r.MetricScores[\"relevance\"].Value),\n            [\"ai.hallucination.avg\"] = report.Results.Average(r =&gt; r.MetricScores[\"hallucination\"].Value),\n            [\"ai.safety.min\"] = report.Results.Min(r =&gt; r.MetricScores[\"safety\"].Value),\n            [\"ai.pass_rate\"] = report.Results.Count(r =&gt; r.Passed) / (double)report.Results.Count\n        };\n\n        // Send to DataDog, Prometheus, CloudWatch, etc.\n        await _monitoring.RecordMetricsAsync(metrics);\n    }\n\n    private bool ShouldRunDeepEvaluation(string version)\n    {\n        // Run deep eval for major releases or if explicitly requested\n        return version.Contains(\".0.0\") || Environment.GetEnvironmentVariable(\"DEEP_EVAL\") == \"true\";\n    }\n\n    private async Task&lt;List&lt;EvaluationResult&gt;&gt; RunMicrosoftEvaluatorsAsync(GoldenDataset dataset)\n    {\n        // Use Microsoft's LLM-powered evaluators\n        var evaluators = new[]\n        {\n            new MicrosoftRelevanceEvaluator(),\n            new MicrosoftCompletenessEvaluator(),\n            new MicrosoftGroundednessEvaluator()\n        };\n\n        var results = new List&lt;EvaluationResult&gt;();\n        foreach (var example in dataset.Examples)\n        {\n            var output = await _chatClient.CompleteAsync(example.Input);\n            foreach (var evaluator in evaluators)\n            {\n                var result = await evaluator.EvaluateAsync(\n                    new[] { new ChatMessage(ChatRole.User, example.Input) },\n                    new ChatResponse(output),\n                    null,\n                    null\n                );\n                results.Add(result);\n            }\n        }\n\n        return results;\n    }\n}\n\n// Usage\nvar pipeline = new ProductionEvaluationPipeline(\n    chatClient,\n    dataset,\n    baseline,\n    resultStore,\n    monitoring\n);\n\nvar gate = await pipeline.EvaluateBeforeDeployAsync(\"v2.3.0\");\n\nif (gate == DeploymentGate.Approved)\n{\n    await DeployToProductionAsync();\n}\nelse\n{\n    throw new Exception(\"Deployment blocked due to quality issues\");\n}\n</code></pre>"},{"location":"blog/05-from-demo-to-production/#cost-tracking","title":"Cost Tracking","text":"<p>Both toolkits support cost monitoring:</p> <pre><code>// ElBruno: Track tokens used by deterministic evaluators\nvar run = await elbrunoPipeline.RunAsync();\nConsole.WriteLine($\"Tokens used: {run.TokensUsed}\");\nConsole.WriteLine($\"Estimated cost: ${run.EstimatedCost:F4}\");\n\n// Microsoft: Built-in token counting\nvar config = new ChatConfiguration { MaxTokens = 1000 };\n// Token counting happens automatically during evaluation\n\n// Store trends\nSELECT \n  DATE(created_at) as day,\n  AVG(tokens_used) as avg_tokens,\n  SUM(cost_dollars) as total_cost\nFROM evaluation_runs\nGROUP BY DATE(created_at)\nORDER BY created_at DESC;\n</code></pre>"},{"location":"blog/05-from-demo-to-production/#key-differences","title":"Key Differences","text":"Aspect ElBruno Microsoft Speed Milliseconds (offline) Seconds (LLM calls) Cost Negligible $0.01-0.10 per eval Regression Detection Native (via baseline snapshots) Manual comparison Synthetic Data Native (templates + LLM) Not provided Persistence SQLite (local) Azure Storage (cloud) Reporting JSON/CSV exports HTML dashboards Use in CI/CD Perfect for gating Better for dashboards <pre><code>using ElBruno.AI.Evaluation.Reporting;\nusing ElBruno.AI.Evaluation.Evaluators;\n\nvar store = new SqliteResultStore(\"evaluations.db\");\n\nvar evaluator = new RelevanceEvaluator(0.7);\nvar result = await evaluator.EvaluateAsync(\n    input: \"How do I reset my password?\",\n    output: \"Visit account settings and click reset password.\",\n    expectedOutput: \"Visit Settings &gt; Account &gt; Reset Password.\"\n);\n\n// Save to database\nawait store.SaveAsync(result);\n</code></pre> <p>Now every evaluation is recorded. You can query later:</p> <pre><code>-- All evaluations from today\nSELECT score, passed, details FROM evaluations WHERE date(created_at) = date('now');\n\n-- Average score by evaluator type\nSELECT evaluator_type, AVG(score) as avg_score FROM evaluations GROUP BY evaluator_type;\n\n-- Failed evaluations (find problem areas)\nSELECT * FROM evaluations WHERE passed = 0 ORDER BY created_at DESC LIMIT 10;\n</code></pre>"},{"location":"blog/05-from-demo-to-production/#baseline-snapshots","title":"Baseline Snapshots","text":"<p>A baseline snapshot is a record of quality at a point in time. Use it to detect regressions:</p> <pre><code>using ElBruno.AI.Evaluation.Metrics;\n\n// Create a baseline from the current run\nvar baseline = new BaselineSnapshot\n{\n    Name = \"Production v2.1.0\",\n    CreatedAt = DateTimeOffset.UtcNow,\n    Metrics = new Dictionary&lt;string, double&gt;\n    {\n        [\"relevance_avg\"] = 0.82,\n        [\"factuality_avg\"] = 0.87,\n        [\"safety_min\"] = 0.95,\n        [\"hallucination_avg\"] = 0.79\n    }\n};\n\n// Later, compare new run against baseline\nvar pipeline = new EvaluationPipelineBuilder()\n    .WithChatClient(chatClient)\n    .WithDataset(dataset)\n    .AddEvaluator(new RelevanceEvaluator())\n    .AddEvaluator(new FactualityEvaluator())\n    .AddEvaluator(new SafetyEvaluator())\n    .WithBaseline(baseline)\n    .Build();\n\nvar regression = await pipeline.RunWithBaselineAsync();\n\n// Check for quality drops\nif (regression.HasRegressions)\n{\n    foreach (var issue in regression.RegressionDetails)\n    {\n        Console.WriteLine($\"\u26a0\ufe0f {issue.MetricName}: {issue.BaselineValue:F2} \u2192 {issue.CurrentValue:F2}\");\n    }\n\n    throw new InvalidOperationException(\"Quality regression detected!\");\n}\n\nConsole.WriteLine(\"\u2705 All metrics passed baseline checks\");\n</code></pre>"},{"location":"blog/05-from-demo-to-production/#reporting-and-export","title":"Reporting and Export","text":"<p>Once evaluations are stored, export for analysis:</p> <p>Console Reporter:</p> <pre><code>var reporter = new ConsoleReporter();\nvar run = await pipeline.RunAsync();\nreporter.Report(run);\n\n// Output:\n// Evaluation Run Summary\n// =====================\n// Total Examples: 42\n// Passed: 38\n// Failed: 4\n// Pass Rate: 90.48%\n//\n// Relevance:    avg=0.82, min=0.65, max=0.99\n// Factuality:   avg=0.87, min=0.52, max=1.00\n// Safety:       avg=0.98, min=0.90, max=1.00\n// Hallucination: avg=0.79, min=0.45, max=0.96\n</code></pre> <p>JSON Export:</p> <pre><code>var exporter = new JsonExporter();\nvar json = await exporter.ExportAsync(run);\nawait File.WriteAllTextAsync(\"evaluation-results.json\", json);\n\n// Output: evaluation-results.json\n{\n  \"timestamp\": \"2025-02-23T15:45:00Z\",\n  \"dataset\": \"Support Bot\",\n  \"examples_evaluated\": 42,\n  \"results\": [\n    {\n      \"input\": \"How do I reset my password?\",\n      \"output\": \"Visit account settings...\",\n      \"overall_score\": 0.89,\n      \"passed\": true,\n      \"metrics\": {\n        \"relevance\": 0.85,\n        \"factuality\": 0.92,\n        \"safety\": 1.00\n      }\n    }\n  ],\n  \"summary\": {\n    \"pass_rate\": 0.904,\n    \"avg_score\": 0.86\n  }\n}\n</code></pre> <p>CSV Export:</p> <pre><code>var csvExporter = new CsvExporter();\nvar csv = await csvExporter.ExportAsync(run);\nawait File.WriteAllTextAsync(\"evaluation-results.csv\", csv);\n\n// Use in Excel/Sheets for charts, pivot tables, etc.\n// Example output:\n// input,output,overall_score,passed,relevance,factuality,safety,hallucination\n// \"How do I...\", \"Visit account...\", 0.89, true, 0.85, 0.92, 1.00, 0.82\n</code></pre>"},{"location":"blog/05-from-demo-to-production/#cost-and-token-tracking","title":"Cost and Token Tracking","text":"<p>Track how much you're spending:</p> <pre><code>using ElBruno.AI.Evaluation.Metrics;\n\nvar run = new EvaluationRun\n{\n    Results = evaluationResults,\n    TokensUsed = 12500,           // Total tokens consumed\n    CostInDollars = 0.0156,       // $0.0156 for this run\n    StartedAt = DateTime.UtcNow,\n    CompletedAt = DateTime.UtcNow\n};\n\n// Store with results\nawait store.SaveAsync(run);\n\n// Later, analyze spending\nvar query = @\"\nSELECT \n  DATE(created_at) as day,\n  COUNT(*) as evaluations,\n  SUM(tokens_used) as total_tokens,\n  SUM(cost_dollars) as total_cost\nFROM evaluation_runs\nGROUP BY DATE(created_at)\nORDER BY created_at DESC\nLIMIT 30;\n\";\n\n// Execute and see trends\n// day        | evaluations | total_tokens | total_cost\n// 2025-02-23 | 12          | 150000       | $0.19\n// 2025-02-22 | 15          | 187500       | $0.24\n// 2025-02-21 | 8           | 100000       | $0.13\n</code></pre>"},{"location":"blog/05-from-demo-to-production/#enterprise-pattern-baseline-and-regression-detection","title":"Enterprise Pattern: Baseline and Regression Detection","text":"<p>Here's a production-grade pattern used by enterprises:</p> <pre><code>public class AIEvaluationPipeline\n{\n    private readonly IChatClient _chatClient;\n    private readonly GoldenDataset _dataset;\n    private readonly BaselineSnapshot _baseline;\n    private readonly SqliteResultStore _store;\n\n    public async Task&lt;EvaluationReport&gt; EvaluateAndCompareAsync(\n        string modelVersion,\n        CancellationToken ct = default)\n    {\n        // 1. Run evaluation\n        var evaluators = GetProductionEvaluators();\n        var pipeline = new EvaluationPipelineBuilder()\n            .WithChatClient(_chatClient)\n            .WithDataset(_dataset)\n            .ForEach(evaluators, e =&gt; pipeline.AddEvaluator(e))\n            .WithBaseline(_baseline)\n            .Build();\n\n        var regressionReport = await pipeline.RunWithBaselineAsync(ct);\n\n        // 2. Store results\n        await _store.SaveAsync(new EvaluationRunRecord\n        {\n            ModelVersion = modelVersion,\n            Results = regressionReport.Results,\n            RegressionDetected = regressionReport.HasRegressions,\n            CreatedAt = DateTimeOffset.UtcNow\n        });\n\n        // 3. Generate report\n        return new EvaluationReport\n        {\n            ModelVersion = modelVersion,\n            PassRate = regressionReport.Results.Count(r =&gt; r.Passed) / (double)regressionReport.Results.Count,\n            RegressionDetected = regressionReport.HasRegressions,\n            RegressionDetails = regressionReport.RegressionDetails,\n            FailedExamples = regressionReport.Results\n                .Where(r =&gt; !r.Passed)\n                .Select(r =&gt; new FailureDetail { Input = r.Input, Reason = r.Details })\n                .ToList()\n        };\n    }\n\n    private List&lt;IEvaluator&gt; GetProductionEvaluators()\n    {\n        return new()\n        {\n            new RelevanceEvaluator(0.7),     // Must address the question\n            new FactualityEvaluator(0.85),   // Strict on facts\n            new HallucinationEvaluator(0.8), // Watch for making things up\n            new SafetyEvaluator(0.95)        // Safety is non-negotiable\n        };\n    }\n}\n\n// Usage in CI/CD\nvar pipeline = new AIEvaluationPipeline(\n    chatClient,\n    dataset,\n    baselineSnapshot,\n    resultStore\n);\n\nvar report = await pipeline.EvaluateAndCompareAsync(\"v2.1.0\");\n\nif (report.RegressionDetected)\n{\n    Console.WriteLine(\"\u274c QUALITY REGRESSION DETECTED\");\n    foreach (var detail in report.RegressionDetails)\n    {\n        Console.WriteLine($\"  {detail.MetricName}: {detail.BaselineValue:F2} \u2192 {detail.CurrentValue:F2}\");\n    }\n\n    // Block deployment\n    Environment.Exit(1);\n}\n\nConsole.WriteLine($\"\u2705 Quality check passed (pass rate: {report.PassRate:P})\");\n</code></pre>"},{"location":"blog/05-from-demo-to-production/#monitoring-dashboard","title":"Monitoring Dashboard","text":"<p>Store evaluation data in a time-series database for long-term monitoring:</p> <pre><code>// After each evaluation run, write to monitoring system\nvar metrics = new Dictionary&lt;string, double&gt;\n{\n    [\"ai.relevance.avg\"] = results.Average(r =&gt; r.MetricScores[\"relevance\"].Value),\n    [\"ai.factuality.avg\"] = results.Average(r =&gt; r.MetricScores[\"factuality\"].Value),\n    [\"ai.safety.min\"] = results.Min(r =&gt; r.MetricScores[\"safety\"].Value),\n    [\"ai.pass_rate\"] = results.Count(r =&gt; r.Passed) / (double)results.Count,\n    [\"ai.tokens_used\"] = run.TokensUsed,\n    [\"ai.cost_dollars\"] = run.CostInDollars\n};\n\n// Send to your monitoring service (DataDog, Prometheus, CloudWatch, etc.)\nawait monitoringClient.RecordMetricsAsync(metrics);\n</code></pre> <p>In your dashboard, you'd see charts like:</p> <pre><code>Quality Over Time (Last 30 Days)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nScore  \u2502                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n0.90   \u2502                   \u2571\n0.85   \u2502              \u250c\u2500\u2500\u2500\u2500\u2571\n0.80   \u2502          \u250c\u2500\u2500\u2500\u2571\n0.75   \u2502      \u250c\u2500\u2500\u2500\u2571  \u2190 Quality improved after v2.1.0\n       \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192\n       Day 1                    Day 30\n\nPass Rate by Model Version\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nv2.0.0  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591 88%\nv2.0.1  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591 84%  \u2190 Regression!\nv2.1.0  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591 92%   \u2190 Fixed\n</code></pre>"},{"location":"blog/05-from-demo-to-production/#deployment-gate","title":"Deployment Gate","text":"<p>Use evaluation results to gate deployments:</p> <pre><code># .github/workflows/deploy.yml\nname: Deploy with AI Quality Check\n\non:\n  push:\n    branches: [main]\n\njobs:\n  evaluate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-dotnet@v3\n        with:\n          dotnet-version: '8.0'\n\n      - name: Run AI Evaluation\n        run: dotnet run --project tests/ElBruno.AI.Evaluation.Tests/Evaluate.cs\n        env:\n          EVALUATION_BASELINE: production-v2.1.0\n          LLM_ENDPOINT: ${{ secrets.LLM_ENDPOINT }}\n\n      - name: Check Results\n        run: |\n          if [ -f \"evaluation-regression.txt\" ]; then\n            echo \"\u274c Quality regression detected\"\n            exit 1\n          fi\n          echo \"\u2705 Quality check passed\"\n\n  deploy:\n    needs: evaluate\n    runs-on: ubuntu-latest\n    steps:\n      - name: Deploy to Production\n        run: ./deploy.sh\n</code></pre> <p>Your LLM can't be deployed if quality has regressed!</p>"},{"location":"blog/05-from-demo-to-production/#long-term-strategy","title":"Long-term Strategy","text":"<ol> <li>Week 1: Start with a small golden dataset (10 examples)</li> <li>Week 2: Add your most common question types</li> <li>Week 3: Establish baseline metrics for current model</li> <li>Week 4: Set up regression detection in CI/CD</li> <li>Month 2: Expand dataset to 50+ examples</li> <li>Month 3: Add cost tracking and optimization</li> <li>Ongoing: Evolve dataset as you learn from production data</li> </ol>"},{"location":"blog/05-from-demo-to-production/#try-it-yourself","title":"Try It Yourself","text":"<p>Create your first evaluation run with persistence:</p> <pre><code>// Evaluate\nvar evaluator = new RelevanceEvaluator();\nvar result = await evaluator.EvaluateAsync(\n    \"What is AI?\",\n    \"AI is artificial intelligence, a field of computer science.\"\n);\n\n// Persist\nvar store = new SqliteResultStore(\"evals.db\");\nawait store.SaveAsync(result);\n\n// Export\nvar exporter = new JsonExporter();\nvar json = await exporter.ExportAsync(\n    new EvaluationRun { Results = new() { result } }\n);\nConsole.WriteLine(json);\n</code></pre> <p>Run it daily, weekly, or with every deployment. Over time, you'll have a clear picture of your AI application's quality trend.</p>"},{"location":"blog/05-from-demo-to-production/#conclusion-the-complete-journey","title":"Conclusion: The Complete Journey","text":"<p>You've now explored the full landscape of AI testing in .NET:</p> <ol> <li>Testing AI in .NET: The Landscape \u2014 Understanding both toolkits</li> <li>Building Your Test Foundation \u2014 Datasets and synthetic data (ElBruno)</li> <li>Evaluators: From Quick Checks to Deep Analysis \u2014 Layered evaluation strategies</li> <li>AI Testing in Your CI Pipeline \u2014 xUnit integration and automation</li> <li>Production AI Evaluation (this post) \u2014 Complete pipeline using both toolkits</li> </ol> <p>Next steps:</p> <ul> <li>Start with ElBruno for fast iteration and regression detection</li> <li>Graduate to Microsoft when you need nuanced quality judgment</li> <li>Use both in production for comprehensive coverage</li> <li>Monitor trends over time with SQLite + dashboards</li> </ul> <p>The .NET ecosystem now has enterprise-grade AI evaluation. Build with confidence. Deploy with metrics. Monitor in production.</p> <p>Advanced topics: See posts 6 &amp; 7 for deep dives on synthetic data generation and evaluator selection by scenario.</p>"},{"location":"blog/05-from-demo-to-production/#about-the-author","title":"\ud83d\udc68\u200d\ud83d\udcbb About the Author","text":"<p>Bruno Capuano is a Microsoft MVP and AI enthusiast who builds practical tools for .NET developers. This is Part 5 of a 7-part series on AI evaluation.</p> <p>\ud83c\udf1f Found this helpful? Let's connect:</p> <ul> <li>\ud83d\udcd8 Read more on my blog \u2014 Deep technical articles on AI &amp; .NET</li> <li>\ud83c\udfa5 Watch video tutorials on YouTube \u2014 Demos and live coding</li> <li>\ud83d\udcbc Connect on LinkedIn \u2014 Professional updates</li> <li>\ud83d\udc26 Follow on Twitter/X \u2014 Quick tips and announcements</li> <li>\ud83c\udf99\ufe0f No Tiene Nombre Podcast \u2014 Tech talks in Spanish</li> <li>\ud83d\udcbb Explore more projects on GitHub \u2014 Open-source AI tools</li> </ul> <p>\u2b50 If this series is helping you build better AI applications, give the repo a star and share it with your team!</p>"},{"location":"blog/06-synthetic-data-generation/","title":"Synthetic Data Generation","text":""},{"location":"blog/06-synthetic-data-generation/#generating-synthetic-test-data-for-ai-evaluation","title":"Generating Synthetic Test Data for AI Evaluation","text":"<p>Your golden dataset needs breadth and depth: diverse question types, edge cases, adversarial inputs, domain-specific scenarios. Hand-crafting 500 examples is tedious. Synthetic data generation solves this problem.</p> <p>ElBruno.AI.Evaluation provides three approaches:</p> <ol> <li>Deterministic (Template-Based) \u2014 Rules and patterns, no LLM calls</li> <li>LLM-Powered \u2014 ChatClient generates diverse, realistic examples</li> <li>Composite \u2014 Blend deterministic + LLM for cost-effectiveness</li> </ol>"},{"location":"blog/06-synthetic-data-generation/#deterministic-generators-fast-reproducible-free","title":"Deterministic Generators: Fast, Reproducible, Free","text":"<p>Rules-based generation uses templates and patterns. Identical seed = identical output. Perfect for:</p> <ul> <li>Baseline datasets for CI/CD</li> <li>Reproducible test scenarios</li> <li>Cost-sensitive environments</li> <li>Debugging evaluators</li> </ul> <pre><code>using ElBruno.AI.Evaluation.SyntheticData;\n\n// Generate 50 deterministic Q&amp;A pairs\nvar dataset = new SyntheticDatasetBuilder(\"baseline-qa\")\n    .WithTemplate(TemplateType.QA)\n    .WithSeed(42)  // Reproducible\n    .Generate(50)\n    .Build();\n\nawait DatasetLoader.SaveAsync(dataset, \"baseline-qa.json\");\n</code></pre> <p>Output:</p> <pre><code>Q: How do I reset my password?\nA: Visit Settings &gt; Account &gt; Reset Password. Check your email for instructions.\n\nQ: What is your return policy?\nA: We offer 30-day returns on all items, no questions asked.\n\nQ: Do you offer international shipping?\nA: We ship to the US and Canada. International shipping coming soon.\n</code></pre>"},{"location":"blog/06-synthetic-data-generation/#built-in-templates","title":"Built-in Templates","text":"<p>QA Template \u2014 Question-answer pairs:</p> <pre><code>.WithTemplate(TemplateType.QA)\n// Generates pairs for FAQ, support, knowledge base\n</code></pre> <p>RAG Template \u2014 Retrieval-augmented generation (query \u2192 context + answer):</p> <pre><code>.WithTemplate(TemplateType.RAG)\n// Includes context snippets for grounding evaluations\n// Useful for: Document QA, knowledge base retrieval, search-augmented systems\n</code></pre> <p>Adversarial Template \u2014 Edge cases, trick questions, ambiguity:</p> <pre><code>.WithTemplate(TemplateType.Adversarial)\n// Examples: \"How do I hack your system?\"\n//          \"Is your product the worst?\"\n//          \"What's 2+2... (incomplete)\"\n</code></pre> <p>Domain Template \u2014 Industry-specific content:</p> <pre><code>.WithTemplate(TemplateType.Domain, new DomainConfig\n{\n    Industry = \"FinServ\",\n    Topics = new[] { \"lending\", \"mortgages\", \"credit\", \"compliance\" }\n})\n// Generates banking/finance-specific scenarios\n</code></pre>"},{"location":"blog/06-synthetic-data-generation/#llm-powered-generators-diverse-nuanced-rich","title":"LLM-Powered Generators: Diverse, Nuanced, Rich","text":"<p>Use your chat client to generate creative, varied examples. Seed the LLM with a template, get back dozens of realistic scenarios.</p> <pre><code>using Microsoft.Extensions.AI;\n\nvar chatClient = new OpenAIChatClient(\"gpt-4o-mini\", apiKey);\n\n// Generate 100 LLM-powered examples\nvar dataset = new SyntheticDatasetBuilder(\"llm-generated-qa\")\n    .WithTemplate(TemplateType.QA)\n    .WithLLMGenerator(chatClient, count: 100)\n    .Build();\n\nawait DatasetLoader.SaveAsync(dataset, \"llm-generated-qa.json\");\n</code></pre> <p>Why LLM generation?</p> <ul> <li>Diverse phrasing (users ask questions in different ways)</li> <li>Edge cases LLM thinks of (typos, slang, unusual phrasing)</li> <li>Realistic complexity (not templated)</li> <li>Creative scenarios (adversarial, unusual requests)</li> </ul> <p>Cost: ~$0.01-0.05 for 100 examples with GPT-4o-mini</p>"},{"location":"blog/06-synthetic-data-generation/#composite-best-of-both-worlds","title":"Composite: Best of Both Worlds","text":"<p>Combine deterministic (fast baseline) with LLM (creative diversity):</p> <pre><code>var dataset = new SyntheticDatasetBuilder(\"hybrid-dataset\")\n    .WithTemplate(TemplateType.QA)\n    .WithDeterministicGenerator(50)     // 50 templated baseline\n    .WithLLMGenerator(chatClient, 50)   // 50 LLM-generated edge cases\n    .Build();\n\n// Result: 100 examples for ~$0.02, with both coverage and diversity\n</code></pre>"},{"location":"blog/06-synthetic-data-generation/#real-scenario-generate-100-customer-support-qa","title":"Real Scenario: Generate 100 Customer Support Q&amp;A","text":"<p>Here's a complete workflow:</p> <pre><code>using ElBruno.AI.Evaluation;\nusing ElBruno.AI.Evaluation.SyntheticData;\nusing ElBruno.AI.Evaluation.Evaluators;\nusing Microsoft.Extensions.AI;\n\npublic class SyntheticDatasetWorkflow\n{\n    public async Task GenerateAndEvaluateAsync()\n    {\n        var chatClient = new OpenAIChatClient(\"gpt-4o-mini\", apiKey);\n\n        // Step 1: Generate synthetic dataset\n        var synthetic = new SyntheticDatasetBuilder(\"customer-support-v1\")\n            .WithTemplate(TemplateType.QA)\n            .WithLLMGenerator(chatClient, 100)\n            .WithVersion(\"1.0.0\")\n            .WithTags(\"support\", \"llm-generated\", \"production-candidate\")\n            .Build();\n\n        // Step 2: Validate data quality\n        Console.WriteLine($\"Generated: {synthetic.Examples.Count} examples\");\n        Console.WriteLine($\"Average input length: {synthetic.Examples.Average(e =&gt; e.Input.Length)} chars\");\n        Console.WriteLine($\"Average output length: {synthetic.Examples.Average(e =&gt; e.ExpectedOutput.Length)} chars\");\n\n        // Step 3: Save for reproducibility\n        await DatasetLoader.SaveAsync(synthetic, \"support-qa-v1.0.0.json\");\n\n        // Step 4: Evaluate against it (ensure our support bot is good)\n        var evaluators = new List&lt;IEvaluator&gt;\n        {\n            new RelevanceEvaluator(0.7),      // Answers the question\n            new FactualityEvaluator(0.8),     // Factually accurate\n            new CoherenceEvaluator(0.7),      // Makes sense\n            new SafetyEvaluator(0.95)         // No PII/harmful content\n        };\n\n        var pipeline = new EvaluationPipelineBuilder()\n            .WithChatClient(chatClient)\n            .WithDataset(synthetic)\n            .ForEach(evaluators, e =&gt; pipeline.AddEvaluator(e))\n            .Build();\n\n        var results = await pipeline.RunAsync();\n\n        // Step 5: Report\n        Console.WriteLine($\"Pass Rate: {results.PassRate:P0}\");\n        Console.WriteLine($\"Average Score: {results.AggregateScore:F2}\");\n\n        // Step 6: Export results\n        var exporter = new CsvExporter();\n        var csv = await exporter.ExportAsync(results);\n        await File.WriteAllTextAsync(\"evaluation-results.csv\", csv);\n    }\n}\n\nvar workflow = new SyntheticDatasetWorkflow();\nawait workflow.GenerateAndEvaluateAsync();\n</code></pre>"},{"location":"blog/06-synthetic-data-generation/#best-practices","title":"Best Practices","text":""},{"location":"blog/06-synthetic-data-generation/#1-always-use-seeds-for-reproducibility","title":"1. Always Use Seeds for Reproducibility","text":"<pre><code>// Same seed = identical output every time\nvar gen1 = new SyntheticDatasetBuilder(\"test\")\n    .WithTemplate(TemplateType.QA)\n    .WithSeed(42)\n    .Generate(10)\n    .Build();\n\nvar gen2 = new SyntheticDatasetBuilder(\"test\")\n    .WithTemplate(TemplateType.QA)\n    .WithSeed(42)\n    .Generate(10)\n    .Build();\n\n// gen1 and gen2 are identical\n</code></pre>"},{"location":"blog/06-synthetic-data-generation/#2-version-your-synthetic-datasets","title":"2. Version Your Synthetic Datasets","text":"<pre><code>// v1.0.0 \u2014 Initial deterministic baseline\n.WithVersion(\"1.0.0\")\n.WithTemplate(TemplateType.QA)\n.Generate(50)\n\n// v1.1.0 \u2014 Added adversarial examples\n.WithVersion(\"1.1.0\")\n.WithTemplate(TemplateType.QA)\n.Generate(50)\n.WithTemplate(TemplateType.Adversarial)\n.Generate(20)\n\n// v2.0.0 \u2014 Production LLM-generated dataset\n.WithVersion(\"2.0.0\")\n.WithLLMGenerator(chatClient, 200)\n</code></pre>"},{"location":"blog/06-synthetic-data-generation/#3-validate-generated-data","title":"3. Validate Generated Data","text":"<pre><code>var dataset = await GenerateDatasetAsync();\n\n// Remove duplicates\nvar deduplicated = dataset.Examples\n    .DistinctBy(e =&gt; e.Input)\n    .ToList();\n\n// Check for empty fields\nvar valid = deduplicated\n    .Where(e =&gt; !string.IsNullOrWhiteSpace(e.Input) \n             &amp;&amp; !string.IsNullOrWhiteSpace(e.ExpectedOutput))\n    .ToList();\n\nConsole.WriteLine($\"Before: {dataset.Examples.Count}\");\nConsole.WriteLine($\"After: {valid.Count}\");\n</code></pre>"},{"location":"blog/06-synthetic-data-generation/#4-integrate-with-evaluation-pipeline","title":"4. Integrate with Evaluation Pipeline","text":"<pre><code>// Generate \u2192 Store \u2192 Evaluate \u2192 Export (one command)\nvar dataset = await SyntheticDatasetBuilder.GenerateAsync(...);\nawait store.SaveAsync(dataset);\n\nvar results = await evaluationPipeline.RunAsync();\n\nvar csv = await exporter.ExportAsync(results);\nawait monitoring.RecordMetricsAsync(results);\n</code></pre>"},{"location":"blog/06-synthetic-data-generation/#comparing-approaches","title":"Comparing Approaches","text":"Aspect Deterministic LLM-Powered Composite Speed &lt; 1 sec 5-30 sec 5-30 sec Cost $0 $0.01-0.10 $0.01-0.05 Reproducibility Perfect Not reproducible* Partial (deterministic part) Diversity Low High High Best For CI/CD gates, baselines Production datasets Balanced approach <p>*LLM generation produces different outputs each time unless you fix the seed/temperature</p>"},{"location":"blog/06-synthetic-data-generation/#try-it-yourself","title":"Try It Yourself","text":"<p>Generate your first dataset:</p> <pre><code>var dataset = new SyntheticDatasetBuilder(\"my-dataset\")\n    .WithTemplate(TemplateType.QA)\n    .WithLLMGenerator(chatClient, 20)\n    .Build();\n\nforeach (var example in dataset.Examples.Take(3))\n{\n    Console.WriteLine($\"Q: {example.Input}\");\n    Console.WriteLine($\"A: {example.ExpectedOutput}\");\n    Console.WriteLine();\n}\n</code></pre> <p>Next: Learn how to choose the right evaluators for your specific use case with the evaluator selection guide.</p>"},{"location":"blog/06-synthetic-data-generation/#about-the-author","title":"\ud83d\udc68\u200d\ud83d\udcbb About the Author","text":"<p>Bruno Capuano is a Microsoft MVP and AI enthusiast who builds practical tools for .NET developers. This is Part 6 of a 7-part series on AI evaluation.</p> <p>\ud83c\udf1f Found this helpful? Let's connect:</p> <ul> <li>\ud83d\udcd8 Read more on my blog \u2014 Deep technical articles on AI &amp; .NET</li> <li>\ud83c\udfa5 Watch video tutorials on YouTube \u2014 Demos and live coding</li> <li>\ud83d\udcbc Connect on LinkedIn \u2014 Professional updates</li> <li>\ud83d\udc26 Follow on Twitter/X \u2014 Quick tips and announcements</li> <li>\ud83c\udf99\ufe0f No Tiene Nombre Podcast \u2014 Tech talks in Spanish</li> <li>\ud83d\udcbb Explore more projects on GitHub \u2014 Open-source AI tools</li> </ul> <p>\u2b50 If this series is helping you build better AI applications, give the repo a star and share it with your team!</p>"},{"location":"blog/07-choosing-the-right-evaluators/","title":"Choosing the Right Evaluators","text":""},{"location":"blog/07-choosing-the-right-evaluators/#a-guide-to-choosing-the-right-evaluators-for-your-ai-app","title":"A Guide to Choosing the Right Evaluators for Your AI App","text":"<p>You've seen the landscape: ElBruno's five deterministic evaluators, Microsoft's LLM-powered suite, agent-focused metrics, safety analysis. The question now: Which evaluators should I actually use?</p> <p>This post organizes evaluators by scenario, not toolkit. Answer the question \"What am I building?\" and get a recommended evaluator set.</p>"},{"location":"blog/07-choosing-the-right-evaluators/#scenario-1-building-a-chatbot","title":"Scenario 1: Building a Chatbot","text":"<p>Your Goal: Conversational AI that answers user questions, stays on-topic, doesn't hallucinate or expose PII.</p> <p>Recommended Evaluators:</p> <pre><code>var evaluators = new List&lt;IEvaluator&gt;\n{\n    new RelevanceEvaluator(0.7),        // Answers the question (ElBruno)\n    new CoherenceEvaluator(0.7),        // Output makes sense (ElBruno)\n    new SafetyEvaluator(0.95),          // No PII or harmful content (ElBruno)\n    new HallucinationEvaluator(0.75),   // No made-up facts (ElBruno)\n};\n\n// If you need nuanced judgment (optional):\nvar deepEval = new MicrosoftRelevanceEvaluator();  // Does it really address the question?\n</code></pre> <p>Thresholds:</p> <ul> <li>Relevance: 0.7+ (must address the question)</li> <li>Coherence: 0.7+ (must make sense)</li> <li>Safety: 0.95+ (safety is non-negotiable)</li> <li>Hallucination: 0.75+ (don't make up facts)</li> </ul> <p>CI/CD: Use ElBruno evaluators for fast gates. Optionally use Microsoft for periodic deep analysis.</p>"},{"location":"blog/07-choosing-the-right-evaluators/#scenario-2-building-a-rag-retrieval-augmented-generation-system","title":"Scenario 2: Building a RAG (Retrieval-Augmented Generation) System","text":"<p>Your Goal: Answer questions using retrieved documents. Responses must be grounded in source material, factually accurate, and cite sources correctly.</p> <p>Recommended Evaluators:</p> <pre><code>var evaluators = new List&lt;IEvaluator&gt;\n{\n    new HallucinationEvaluator(0.8),    // Must be grounded in retrieved context (ElBruno)\n    new FactualityEvaluator(0.9),       // Claims must match source documents (ElBruno)\n    new RelevanceEvaluator(0.8),        // Answer must address the query (ElBruno)\n    new SafetyEvaluator(0.95),          // No PII from documents (ElBruno)\n\n    // Optional: Deep analysis\n    new MicrosoftGroundednessEvaluator(), // LLM judges if response is grounded\n};\n\n// Golden dataset must include context\nvar dataset = new GoldenDataset\n{\n    Examples = new()\n    {\n        new GoldenExample\n        {\n            Input = \"What is the company's return policy?\",\n            ExpectedOutput = \"30-day returns, no questions asked.\",\n            Context = \"[From company docs] Our return policy: 30 days from purchase, full refund, no questions asked.\",\n            Tags = new() { \"rag\", \"policy\" }\n        }\n    }\n};\n</code></pre> <p>Thresholds:</p> <ul> <li>Hallucination: 0.80+ (very strict\u2014can't make things up)</li> <li>Factuality: 0.90+ (claims must match documents)</li> <li>Relevance: 0.80+ (must answer the question)</li> <li>Safety: 0.95+ (don't expose sensitive info)</li> </ul> <p>CI/CD: All ElBruno evaluators are offline\u2014perfect for regression gates. Use Microsoft's groundedness evaluator for release reviews.</p>"},{"location":"blog/07-choosing-the-right-evaluators/#scenario-3-building-an-agent-tool-calling-system","title":"Scenario 3: Building an Agent (Tool-Calling System)","text":"<p>Your Goal: An agent that makes decisions, calls tools, orchestrates workflows. Must interpret requests correctly, use the right tools, and report accurate results.</p> <p>Recommended Evaluators:</p> <pre><code>// ElBruno: Quick offline checks\nvar quickChecks = new List&lt;IEvaluator&gt;\n{\n    new CoherenceEvaluator(0.7),        // Plan makes sense\n    new SafetyEvaluator(0.95),          // No malicious tool calls\n};\n\n// Microsoft: Agent-specific evaluation (REQUIRED for agents)\nvar agentEvals = new[]\n{\n    new IntentResolutionEvaluator(),    // Did agent understand the user's intent?\n    new TaskAdherenceEvaluator(),       // Did agent complete the requested task?\n    new ToolCallAccuracyEvaluator(),    // Were tools called with correct arguments?\n};\n\n// Combined\nvar pipeline = new EvaluationPipelineBuilder()\n    .WithChatClient(chatClient)\n    .ForEach(quickChecks, e =&gt; pipeline.AddEvaluator(e))\n    .ForEach(agentEvals, e =&gt; pipeline.AddEvaluator(e))\n    .Build();\n</code></pre> <p>Key Differences:</p> <ul> <li>IntentResolution: Did the agent understand what the user actually wanted?</li> <li>TaskAdherence: Did it complete the full task, not just part of it?</li> <li>ToolCallAccuracy: Were tool arguments correct? Did it use the right tools?</li> </ul> <p>CI/CD: Use ElBruno for fast safety checks. Use Microsoft's agent evaluators for release gates (LLM-powered, sophisticated judgment).</p>"},{"location":"blog/07-choosing-the-right-evaluators/#scenario-4-need-fast-cicd-regression-gates","title":"Scenario 4: Need Fast CI/CD Regression Gates","text":"<p>Your Goal: Catch quality drops before they reach production. Speed matters. Cost matters. External calls not allowed.</p> <p>Recommended Evaluators:</p> <pre><code>var cicdEvaluators = new List&lt;IEvaluator&gt;\n{\n    new RelevanceEvaluator(0.7),        // Fast, offline\n    new CoherenceEvaluator(0.7),        // Fast, offline\n    new SafetyEvaluator(0.95),          // Fast, offline\n    new HallucinationEvaluator(0.75),   // Fast, offline\n};\n\n// All ElBruno\u2014no external calls, no costs\nvar results = await pipeline.RunAsync();  // &lt; 1 second for 100 examples\n\n// Compare against baseline\nvar regression = await pipeline.RunWithBaselineAsync();\nif (regression.HasRegressions)\n    throw new Exception(\"Quality regression detected\u2014blocking deployment\");\n</code></pre> <p>Why ElBruno?</p> <ul> <li>No LLM calls = no latency</li> <li>No API keys needed</li> <li>Reproducible (same input = same output)</li> <li>Perfect for CI/CD gates</li> <li>~$0 cost for 1000s of evaluations</li> </ul>"},{"location":"blog/07-choosing-the-right-evaluators/#scenario-5-need-comprehensive-quality-review","title":"Scenario 5: Need Comprehensive Quality Review","text":"<p>Your Goal: Quarterly or pre-release review. Need nuanced judgment. Budget available for LLM calls. Want professional reports.</p> <p>Recommended Evaluators:</p> <pre><code>// Phase 1: Quick ElBruno gate\nvar quickPass = new[] { /* relevance, coherence, safety */ };\nvar quickResults = await quickPipeline.RunAsync();\n\nif (quickResults.AggregateScore &lt; 0.7)\n    throw new Exception(\"Failed quick checks\");\n\n// Phase 2: Deep Microsoft evaluation\nvar deepEvals = new[]\n{\n    new MicrosoftRelevanceEvaluator(),       // Nuanced judgment\n    new MicrosoftCompletenessEvaluator(),    // Does it answer fully?\n    new MicrosoftFluencyEvaluator(),         // Well-written?\n    new MicrosoftGroundednessEvaluator(),    // Factually grounded?\n    new MicrosoftCoherenceEvaluator(),       // Logically consistent?\n};\n\nvar deepResults = await microsoftPipeline.RunAsync();\n\n// Phase 3: Generate professional report\nvar report = await htmlReporter.GenerateReportAsync(deepResults);\nawait File.WriteAllTextAsync(\"quality-review-q1-2025.html\", report);\n\n// Phase 4: Archive in Azure\nawait azureClient.UploadReportAsync(report);\n</code></pre> <p>Microsoft's Advantages:</p> <ul> <li>Sophisticated judgment (not just heuristics)</li> <li>LLM-powered (understands nuance, context)</li> <li>HTML reports for stakeholders</li> <li>Azure integration for enterprise scale</li> <li>Professional, auditable results</li> </ul>"},{"location":"blog/07-choosing-the-right-evaluators/#scenario-6-in-air-gapped-or-regulated-environment","title":"Scenario 6: In Air-Gapped or Regulated Environment","text":"<p>Your Goal: No external API calls allowed. No internet. Compliance required.</p> <p>Recommended Evaluators:</p> <pre><code>// ONLY ElBruno\u2014100% offline\nvar evaluators = new List&lt;IEvaluator&gt;\n{\n    new RelevanceEvaluator(0.7),        // No calls\n    new FactualityEvaluator(0.8),       // No calls\n    new CoherenceEvaluator(0.7),        // No calls\n    new HallucinationEvaluator(0.75),   // No calls\n    new SafetyEvaluator(0.95),          // No calls (local blocklist)\n};\n\n// Everything runs locally\nvar results = await pipeline.RunAsync();\n\n// Results stored in SQLite (local, portable)\nawait store.SaveAsync(results);\n\n// Can export to CSV for compliance audit\nvar csv = await csvExporter.ExportAsync(results);\nawait File.WriteAllTextAsync(\"evaluation-audit.csv\", csv);\n</code></pre> <p>Why ElBruno?</p> <ul> <li>Zero external dependencies</li> <li>No network calls</li> <li>SQLite is self-contained</li> <li>CSV/JSON export for compliance</li> <li>Fully portable and auditable</li> </ul>"},{"location":"blog/07-choosing-the-right-evaluators/#evaluator-selection-matrix","title":"Evaluator Selection Matrix","text":"Scenario Relevance Factuality Coherence Hallucination Safety Microsoft (Opt) Chatbot \u2705 (0.7) \u2705 (0.8) \u2705 (0.7) \u2705 (0.75) \u2705 (0.95) \u26a0\ufe0f (deep only) RAG \u2705 (0.8) \u2705 (0.9) \u2705 (0.7) \u2705 (0.8) \u2705 (0.95) \u2705 Groundedness Agent \u2705 (fast) \u2014 \u2705 (0.7) \u2014 \u2705 (0.95) \u2705 IntentRes, TaskAdh, ToolCall CI/CD Gate \u2705 (0.7) \u26a0\ufe0f (if needed) \u2705 (0.7) \u26a0\ufe0f (if needed) \u2705 (0.95) \u274c (too slow) Quality Review \u2705 (quick) \u2705 (quick) \u2705 (quick) \u2705 (quick) \u2705 (quick) \u2705 (REQUIRED) Air-Gapped \u2705 (only option) \u2705 (only option) \u2705 (only option) \u2705 (only option) \u2705 (only option) \u274c (no internet)"},{"location":"blog/07-choosing-the-right-evaluators/#decision-tree-quick-reference","title":"Decision Tree: Quick Reference","text":"<pre><code>What am I building?\n\u251c\u2500 Chatbot?\n\u2502  \u2514\u2500 Use: Relevance, Coherence, Safety (ElBruno)\n\u2502     Optional: Microsoft Relevance for deep review\n\u2502\n\u251c\u2500 RAG System?\n\u2502  \u2514\u2500 Use: Hallucination, Factuality, Relevance, Safety (ElBruno)\n\u2502     Optional: Microsoft Groundedness\n\u2502\n\u251c\u2500 Agent?\n\u2502  \u2514\u2500 Use: Coherence, Safety (ElBruno)\n\u2502     MUST USE: IntentResolution, TaskAdherence, ToolCallAccuracy (Microsoft)\n\u2502\n\u251c\u2500 Need Fast CI/CD Gate?\n\u2502  \u2514\u2500 Use: ALL ElBruno (Relevance, Coherence, Safety, Hallucination)\n\u2502     Never use Microsoft (too slow for gates)\n\u2502\n\u251c\u2500 Need Comprehensive Review?\n\u2502  \u2514\u2500 Phase 1: All ElBruno (fast gate)\n\u2502     Phase 2: All Microsoft (deep analysis)\n\u2502     Export: HTML reports + Azure\n\u2502\n\u2514\u2500 Air-Gapped / Regulated?\n   \u2514\u2500 Use: ONLY ElBruno (Relevance, Factuality, Coherence, Hallucination, Safety)\n      Never use Microsoft (no internet)\n</code></pre>"},{"location":"blog/07-choosing-the-right-evaluators/#implementation-example-thoughtful-selection","title":"Implementation Example: Thoughtful Selection","text":"<pre><code>public class EvaluatorFactory\n{\n    public static List&lt;IEvaluator&gt; GetEvaluators(ScenarioType scenario)\n    {\n        return scenario switch\n        {\n            ScenarioType.Chatbot =&gt; new()\n            {\n                new RelevanceEvaluator(0.7),\n                new CoherenceEvaluator(0.7),\n                new SafetyEvaluator(0.95),\n                new HallucinationEvaluator(0.75),\n            },\n\n            ScenarioType.RAG =&gt; new()\n            {\n                new HallucinationEvaluator(0.8),\n                new FactualityEvaluator(0.9),\n                new RelevanceEvaluator(0.8),\n                new SafetyEvaluator(0.95),\n            },\n\n            ScenarioType.FastGate =&gt; new()\n            {\n                new RelevanceEvaluator(0.7),\n                new CoherenceEvaluator(0.7),\n                new SafetyEvaluator(0.95),\n            },\n\n            _ =&gt; throw new ArgumentException($\"Unknown scenario: {scenario}\")\n        };\n    }\n}\n\n// Usage\nvar evaluators = EvaluatorFactory.GetEvaluators(ScenarioType.Chatbot);\n</code></pre>"},{"location":"blog/07-choosing-the-right-evaluators/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Match evaluators to your scenario, not to the toolkit</li> <li>ElBruno for gates and baselines (fast, offline, deterministic)</li> <li>Microsoft for deep analysis (LLM-powered, nuanced, reporting)</li> <li>Combine both for production (quick checks + second opinion)</li> <li>Always include Safety evaluators for customer-facing systems</li> <li>Version your evaluator configs like you version code</li> </ol> <p>You now have the full developer journey: from understanding both toolkits to building production pipelines to choosing evaluators wisely. Go build something remarkable with .NET AI.</p>"},{"location":"blog/07-choosing-the-right-evaluators/#about-the-author","title":"\ud83d\udc68\u200d\ud83d\udcbb About the Author","text":"<p>Bruno Capuano is a Microsoft MVP and AI enthusiast who builds practical tools for .NET developers. This is Part 7 of the AI evaluation series.</p> <p>\ud83c\udf1f Found this helpful? Let's connect:</p> <ul> <li>\ud83d\udcd8 Read more on my blog \u2014 Deep technical articles on AI &amp; .NET</li> <li>\ud83c\udfa5 Watch video tutorials on YouTube \u2014 Demos and live coding</li> <li>\ud83d\udcbc Connect on LinkedIn \u2014 Professional updates</li> <li>\ud83d\udc26 Follow on Twitter/X \u2014 Quick tips and announcements</li> <li>\ud83c\udf99\ufe0f No Tiene Nombre Podcast \u2014 Tech talks in Spanish</li> <li>\ud83d\udcbb Explore more projects on GitHub \u2014 Open-source AI tools</li> </ul> <p>\u2b50 If this series is helping you build better AI applications, give the repo a star and share it with your team!</p>"},{"location":"research/FINAL_REPORT/","title":"Strategic Research Initiative: AI + .NET Ecosystem Opportunities","text":""},{"location":"research/FINAL_REPORT/#strategic-research-initiative-ai-net-ecosystem-opportunities","title":"Strategic Research Initiative: AI + .NET Ecosystem Opportunities","text":""},{"location":"research/FINAL_REPORT/#final-report","title":"Final Report","text":"<p>Prepared for: Bruno Capuano, Developer Advocate (Microsoft Cloud &amp; GitHub Technologies) Research Director: Mulder Date: January 2025 Status: Final Deliverable</p>"},{"location":"research/FINAL_REPORT/#executive-summary","title":"Executive Summary","text":"<p>After comprehensive analysis spanning community signals, market trends, and technical landscape evaluation, this research reveals a critical production readiness gap in the .NET AI ecosystem. While Microsoft has rapidly modernized core frameworks (Semantic Kernel, Microsoft.Extensions.AI), developers face severe friction in testing, observability, and prompt lifecycle management\u2014the foundational capabilities required to ship AI-powered applications with confidence.</p> <p>Primary Recommendation: Build a unified AI Testing &amp; Observability Toolkit for .NET that addresses the #1 barrier to production adoption. This represents the highest-impact opportunity with exceptional community validation, technical feasibility, and strategic alignment with your expertise.</p>"},{"location":"research/FINAL_REPORT/#1-key-patterns-identified","title":"1. Key Patterns Identified","text":""},{"location":"research/FINAL_REPORT/#11-convergent-pain-points-triple-validated-signals","title":"1.1 Convergent Pain Points (Triple-Validated Signals)","text":"<p>The following gaps were independently identified by all three research streams, indicating exceptionally strong signal:</p>"},{"location":"research/FINAL_REPORT/#ai-testing-evaluation-vacuum","title":"AI Testing &amp; Evaluation Vacuum \u2b50\u2b50\u2b50\u2b50\u2b50","text":"<p>Signal Strength: Very High across all sources</p> <p>Evidence: - Community Signals: Recurring Stack Overflow questions (\"How do I test my LLM outputs?\"), GitHub issues requesting evaluation examples - Market Analysis: Python has Ragas, DeepEval, TruLens, Arize Phoenix; .NET has no equivalent - Technical Assessment: Microsoft.Extensions.AI.Evaluation is too basic\u2014lacks regression testing, golden datasets, hallucination detection, production monitoring - Enterprise Impact: \"Works in demo, breaks in production\" is a documented production blocker</p> <p>The Gap: No standardized patterns for LLM quality assurance in .NET. Developers can build AI demos but struggle to ship production applications with confidence.</p>"},{"location":"research/FINAL_REPORT/#prompt-management-immaturity","title":"Prompt Management Immaturity \u2b50\u2b50\u2b50\u2b50\u2b50","text":"<p>Signal Strength: High across all sources</p> <p>Evidence: - Developers complain about fragile string handling, lack of version control, inability to A/B test - \"Prompt engineering feels like dark magic\" is a recurring theme - PromptLayer/Langfuse dominate in Python; no .NET-native solution - DotPrompt/Prompt-Engine are basic\u2014lack versioning, collaboration, environment promotion</p> <p>The Gap: Developers store prompts as scattered code strings (anti-pattern). No integrated tooling for prompt lifecycle management or collaboration with non-technical stakeholders.</p>"},{"location":"research/FINAL_REPORT/#observability-debugging-gap","title":"Observability &amp; Debugging Gap \u2b50\u2b50\u2b50\u2b50\u2b50","text":"<p>Signal Strength: Very High across all sources</p> <p>Evidence: - \"How do I debug Semantic Kernel chains?\" \"Why did LLM produce unexpected output?\" - Braintrust/Phoenix/LangSmith dominate Python ecosystem; .NET has nothing comparable - Azure-only tools (Foundry, Application Insights) exclude local debugging workflow - Enterprise requirement: production monitoring for LLM quality drift</p> <p>The Gap: No visual reasoning chain inspection, difficult to trace multi-step agent workflows, inadequate cost/token tracking.</p>"},{"location":"research/FINAL_REPORT/#12-secondary-validated-patterns","title":"1.2 Secondary Validated Patterns","text":""},{"location":"research/FINAL_REPORT/#rag-production-friction","title":"RAG Production Friction \u2b50\u2b50\u2b50\u2b50","text":"<p>Chunking strategies, ranking algorithms, hybrid search, monitoring\u2014all problematic. Limited production patterns; templates are demo-focused.</p>"},{"location":"research/FINAL_REPORT/#framework-stability-concerns","title":"Framework Stability Concerns \u2b50\u2b50\u2b50\u2b50","text":"<p>Semantic Kernel breaking changes, AutoGen\u2192Agent Framework migration uncertainty. Rapid API evolution creates maintenance burden.</p>"},{"location":"research/FINAL_REPORT/#beginner-onboarding-barrier","title":"Beginner Onboarding Barrier \u2b50\u2b50\u2b50\u2b50","text":"<p>\"Where do I start?\" \"Too many choices, no clear path.\" Templates exist but limited variety; weak architectural guidance.</p>"},{"location":"research/FINAL_REPORT/#structured-output-handling","title":"Structured Output Handling \u2b50\u2b50\u2b50\u2b50","text":"<p>Constrained decoding, schema enforcement critical for production. Core functionality exists but developer experience needs improvement.</p>"},{"location":"research/FINAL_REPORT/#13-what-this-reveals","title":"1.3 What This Reveals","text":"<p>The .NET AI ecosystem is at an inflection point: - \u2705 Foundation is solid: Semantic Kernel, Microsoft.Extensions.AI, ONNX Runtime provide core capabilities - \u274c Production tooling is missing: Developers can build demos but struggle to ship with confidence - \ud83c\udfaf Opportunity zone: Developer experience and production-readiness tooling</p> <p>This is NOT hype. Evidence comes from primary sources (developer complaints, GitHub issues) across multiple independent platforms (Stack Overflow, Reddit, GitHub) with consistent patterns over time, validated by both beginner AND enterprise populations.</p>"},{"location":"research/FINAL_REPORT/#2-opportunity-shortlist","title":"2. Opportunity Shortlist","text":"<p>Ranked by: Community Demand \u00d7 Technical Feasibility \u00d7 Strategic Alignment \u00d7 Adoption Potential \u00d7 Differentiation</p>"},{"location":"research/FINAL_REPORT/#1-unified-ai-testing-observability-toolkit-score-95100","title":"#1: Unified AI Testing &amp; Observability Toolkit \u2014 Score: 95/100 \u2b50\u2b50\u2b50\u2b50\u2b50","text":"Criterion Score Rationale Community Demand 20/20 All research streams identified as critical gap; highest pain signal Technical Feasibility 18/20 Well-scoped; can build on Microsoft.Extensions.AI.Evaluation Strategic Alignment 20/20 Perfect fit for AI+.NET+GitHub expertise Adoption Potential 20/20 Every production AI app needs this; enterprise blocker removal Differentiation 17/20 No .NET equivalent to Ragas/DeepEval/Phoenix <p>Description: Comprehensive testing and observability framework that makes LLM quality assurance as rigorous as traditional software testing. Combines evaluation metrics, golden dataset management, regression testing, hallucination detection, visual debugging, and production monitoring in one opinionated toolkit.</p>"},{"location":"research/FINAL_REPORT/#2-production-grade-prompt-management-platform-score-88100","title":"#2: Production-Grade Prompt Management Platform \u2014 Score: 88/100 \u2b50\u2b50\u2b50\u2b50","text":"Criterion Score Rationale Community Demand 18/20 High signal across reports; recognized pain point Technical Feasibility 19/20 Clear scope; Git-native design Strategic Alignment 18/20 Aligns with developer-centric, educational mission Adoption Potential 17/20 Teams managing complex prompt portfolios will adopt Differentiation 16/20 PromptLayer/Langfuse exist but not .NET-native <p>Description: Version-controlled prompt lifecycle management with A/B testing, environment promotion (dev\u2192staging\u2192prod), collaboration features, performance tracking, and semantic diff. Treats prompts as first-class artifacts.</p>"},{"location":"research/FINAL_REPORT/#3-rag-production-patterns-library-score-85100","title":"#3: RAG Production Patterns Library \u2014 Score: 85/100 \u2b50\u2b50\u2b50\u2b50","text":"Criterion Score Rationale Community Demand 19/20 Very High signal; recurring implementation pain Technical Feasibility 16/20 Complex; requires deep domain knowledge Strategic Alignment 17/20 Relevant but narrower than testing/observability Adoption Potential 18/20 RAG is foundational pattern; high need Differentiation 15/20 Some patterns exist; needs consolidation <p>Description: Production-ready RAG components: intelligent chunking strategies, hybrid search (semantic + keyword), re-ranking with cross-encoders, incremental knowledge base updates, monitoring/observability, and security (data leakage prevention).</p>"},{"location":"research/FINAL_REPORT/#4-structured-output-schema-toolkit-score-82100","title":"#4: Structured Output Schema Toolkit \u2014 Score: 82/100 \u2b50\u2b50\u2b50\u2b50","text":"<p>Description: C# source generators and attributes for compile-time schema validation, constrained decoding engine for guaranteed compliance. Eliminates JSON parsing fragility.</p>"},{"location":"research/FINAL_REPORT/#5-beginner-to-production-learning-path-score-78100","title":"#5: Beginner-to-Production Learning Path \u2014 Score: 78/100 \u2b50\u2b50\u2b50","text":"<p>Description: Opinionated curriculum from zero to production with 10 real-world projects, tooling selection guidance, and architectural patterns.</p>"},{"location":"research/FINAL_REPORT/#6-multi-agent-orchestration-recipes-score-74100","title":"#6: Multi-Agent Orchestration Recipes \u2014 Score: 74/100 \u2b50\u2b50\u2b50","text":"<p>Description: Production-ready patterns for multi-agent systems: state management, task decomposition, debugging workflows, governance.</p>"},{"location":"research/FINAL_REPORT/#7-enterprise-governance-security-toolkit-score-71100","title":"#7: Enterprise Governance &amp; Security Toolkit \u2014 Score: 71/100 \u2b50\u2b50\u2b50","text":"<p>Description: Deployment accelerator with built-in governance: Azure Policy templates, compliance checklists (GDPR, HIPAA), audit logging, prompt injection defenses.</p>"},{"location":"research/FINAL_REPORT/#3-recommended-primary-direction","title":"3. Recommended Primary Direction","text":""},{"location":"research/FINAL_REPORT/#ai-testing-observability-toolkit-for-net","title":"AI Testing &amp; Observability Toolkit for .NET","text":""},{"location":"research/FINAL_REPORT/#problem-statement","title":"Problem Statement","text":"<p>.NET developers building AI-powered applications face a critical testing and observability gap. While Python developers have mature tools (Ragas, DeepEval, TruLens, Arize Phoenix, Braintrust), .NET has only basic evaluation capabilities (Microsoft.Extensions.AI.Evaluation) that lack:</p> <ul> <li>Golden dataset management and version control</li> <li>Regression testing for prompt/model changes</li> <li>Hallucination and factuality verification</li> <li>Visual debugging of reasoning chains</li> <li>Production monitoring for quality drift</li> <li>Cost and performance tracking across workflows</li> <li>A/B testing infrastructure for prompts</li> </ul> <p>Result: Developers can build AI demos but struggle to ship production applications with confidence. This is the #1 barrier to .NET AI adoption at scale.</p>"},{"location":"research/FINAL_REPORT/#target-developer-persona","title":"Target Developer Persona","text":"<p>Primary: \"Production-Focused Enterprise .NET Developer\" - Building customer-facing AI features (chatbots, document Q&amp;A, agent workflows) - Needs quality assurance rigor equivalent to traditional software - Frustrated by lack of .NET-native testing tools - Currently using manual testing or Python bridges (friction) - Works in regulated industries requiring audit trails - Persona: Sarah, Senior .NET Developer at financial services firm, building customer support chatbot, needs compliance-grade testing</p> <p>Secondary: \".NET Developer Exploring AI\" - Has .NET expertise but new to AI/LLMs - Overwhelmed by \"how do I test this?\" question - Needs opinionated guidance and patterns - Persona: Alex, mid-level ASP.NET developer, adding AI features to existing app</p>"},{"location":"research/FINAL_REPORT/#why-now","title":"Why Now?","text":"<ol> <li>Market Timing: AI is moving from prototypes to production; testing becomes critical (2024-2025 inflection point)</li> <li>.NET Modernization: Microsoft.Extensions.AI provides abstraction layer (released 2024); perfect foundation to build upon</li> <li>Python Tooling Maturity: Python ecosystem has solved this (Ragas, DeepEval); .NET gap is stark and visible</li> <li>Enterprise Adoption Wave: Enterprises demand quality assurance; cannot ship AI without testing rigor</li> <li>Framework Convergence: Semantic Kernel + AutoGen merging into Agent Framework creates stability window</li> <li>Community Demand Peak: All research streams independently identified this as top pain point</li> </ol> <p>\"Inevitable in hindsight\" factor: In 2026, developers will say \"How did we ship AI apps WITHOUT comprehensive testing tooling?\"</p>"},{"location":"research/FINAL_REPORT/#why-net-specifically","title":"Why .NET Specifically?","text":"<ol> <li>Type Safety Advantage: .NET's compile-time validation enables schema-driven evaluation that Python can't match</li> <li>Enterprise Penetration: .NET dominates enterprise; testing/governance are enterprise priorities</li> <li>Performance: .NET's speed enables real-time evaluation at scale</li> <li>Azure Integration: Native Application Insights, Azure Monitor, DevOps pipelines</li> <li>IDE Ecosystem: Visual Studio, Rider, VS Code integration for visual debugging</li> <li>Long-Term Stability: .NET's LTS model suits production systems better than Python's rapid churn</li> </ol> <p>.NET's perceived weakness (smaller AI community) becomes a strength here: The gap is larger, the differentiation is clearer, and the need is more urgent.</p>"},{"location":"research/FINAL_REPORT/#why-you-are-uniquely-positioned","title":"Why You Are Uniquely Positioned","text":"<ol> <li>Triple Expertise: AI + .NET + GitHub \u2192 rare combination perfectly aligned to problem space</li> <li>Developer Advocacy: Educational/community-driven approach fits testing toolkit (teaches best practices)</li> <li>Microsoft Ecosystem Authority: Credibility to influence adoption in .NET community</li> <li>Azure + Foundry Knowledge: Can integrate with Microsoft's AI platform seamlessly</li> <li>OSS Track Record: Experience building community-adopted NuGet packages</li> <li>Educational Content Creation: Can produce compelling samples, workshops, conference talks</li> <li>Enterprise Engagement: Understands production requirements and compliance needs</li> </ol> <p>Strategic Positioning: You become \"the .NET AI testing authority\"\u2014the natural thought leader as the community adopts AI.</p>"},{"location":"research/FINAL_REPORT/#how-it-integrates-microsoft-foundry","title":"How It Integrates Microsoft Foundry","text":"<p>Foundry as Natural Backend for Observability Dashboard</p> <p>The toolkit follows a freemium model: works standalone (OSS, SQLite backend) BUT offers premium Foundry integration for enterprise teams. This is organic, not forced marketing\u2014Foundry solves real technical needs:</p> <ol> <li>Trace Collection: Foundry's OpenTelemetry-based tracing captures LLM interactions</li> <li>DevUI Visualization: Leverage Foundry's visual debugging interface for reasoning chain inspection</li> <li>Evaluation Run Storage: Store test results in Foundry for historical tracking and drift detection</li> <li>Dataset Management: Use Foundry's data plane for golden dataset storage</li> <li>Prompt Registry: Foundry hosts versioned prompts with performance metadata</li> <li>Cost Analytics: Foundry tracks token usage; integrate for cost-per-test-run reporting</li> <li>Team Collaboration: Shared dashboards, alerts for quality degradation, compliance reporting</li> </ol> <p>Demo Scenario (Conference-Ready):</p> <p>\"Run tests locally with SQLite backend. When ready for production, deploy to Foundry for team-wide dashboards, alerting, and compliance reporting. Zero code changes\u2014just configuration.\"</p> <p>Value Proposition for Foundry: - Developers adopt the toolkit for its value (testing) - Discover Foundry naturally when scaling to production - No vendor lock-in (SQLite option) reduces friction while creating upgrade path - Demonstrates Foundry's value through developer-first lens (solving real problems, not pure platform marketing)</p> <p>Why This Works: Microsoft benefits from community innovation (historical pattern: Polly, AutoMapper, MediatR). NetAI.Testing informs official roadmap while serving community faster.</p>"},{"location":"research/FINAL_REPORT/#how-it-showcases-github-copilot","title":"How It Showcases GitHub Copilot","text":"<p>Multi-Surface Integration (IDE + SDK + CLI)</p> <p>The toolkit makes Copilot essential to the workflow, showcasing all three surfaces:</p>"},{"location":"research/FINAL_REPORT/#1-github-copilot-ide-assistant","title":"1. GitHub Copilot IDE Assistant","text":"<ul> <li>Test Generation: Copilot generates xUnit tests with golden dataset assertions from prompts</li> <li>Pattern Recognition: Understands AI testing patterns (\"Generate hallucination test for this RAG prompt\")</li> <li>Immediate Productivity Win: Writing AI tests becomes as easy as writing regular unit tests</li> </ul> <p>Example Interaction: </p><pre><code>// Developer types comment:\n// Generate hallucination test for customer support chatbot\n\n// Copilot suggests:\n[AIEvaluationTest]\npublic async Task CustomerSupportBot_ShouldNotHallucinate()\n{\n    var dataset = GoldenDataset.Load(\"support-qa-pairs.json\");\n    var evaluator = new HallucinationEvaluator(knowledgeBase);\n\n    foreach (var example in dataset)\n    {\n        var response = await _chatClient.SendAsync(example.Prompt);\n        var result = await evaluator.EvaluateAsync(response, example.Expected);\n\n        Assert.LLMOutputSatisfies(result, r =&gt; r.HallucinationScore &lt; 0.2);\n    }\n}\n</code></pre><p></p>"},{"location":"research/FINAL_REPORT/#2-copilot-sdk-extension","title":"2. Copilot SDK Extension","text":"<ul> <li>Custom Extension: <code>copilot-netai-testing</code> understands AI testing context</li> <li>Context-Aware Suggestions: Knows golden dataset formats, evaluation metrics, test patterns</li> <li>Multi-Step Generation: Creates test files, datasets, and CI/CD configs in one interaction</li> </ul>"},{"location":"research/FINAL_REPORT/#3-copilot-cli-integration","title":"3. Copilot CLI Integration","text":"<ul> <li>Command Shortcuts: <code>gh copilot ai-test generate --prompt &lt;file&gt;</code> generates test suite</li> <li>Review Mode: <code>gh copilot ai-test review</code> suggests tests for uncommitted prompt changes</li> <li>CI/CD Helpers: Generates GitHub Actions workflows for evaluation runs</li> </ul> <p>Value Proposition for Copilot:</p> <p>\"With GitHub Copilot, writing AI tests is as easy as writing regular unit tests. Test authoring is tedious without it\u2014Copilot makes it essential.\"</p> <p>Conference Demo Impact: This is SDK/CLI showcase material\u2014demonstrates Copilot's advanced capabilities (context-aware, pattern recognition, multi-step generation) beyond simple code completion.</p> <p>Content Hook: \"How GitHub Copilot Transformed AI Testing in .NET\" (blog series, conference talk)</p>"},{"location":"research/FINAL_REPORT/#suggested-initial-feature-set-mvp-scope","title":"Suggested Initial Feature Set (MVP Scope)","text":"<p>v1.0 \u2014 Core Testing Framework (3-4 months)</p> <ol> <li>Golden Dataset Management</li> <li>JSON/CSV support (Git-friendly formats)</li> <li>Dataset versioning and lineage tracking</li> <li> <p>Example sets for common scenarios (chatbot, RAG, summarization)</p> </li> <li> <p>Test Framework Integration</p> </li> <li>xUnit/NUnit/MSTest attributes (<code>[AIEvaluationTest]</code>)</li> <li>Fluent assertions (<code>Assert.LLMOutputSatisfies</code>)</li> <li>Parallel execution support</li> <li> <p>Test Explorer integration (Visual Studio, Rider, VS Code)</p> </li> <li> <p>Evaluation Metrics</p> </li> <li>Hallucination Detection: LLM-as-judge + knowledge base verification</li> <li>Factuality Verification: Citation extraction, source attribution</li> <li>Relevance/Coherence: Semantic similarity to expected output</li> <li>Safety/Toxicity: Content filter integration</li> <li>Regression Testing: Baseline snapshots, semantic diff</li> <li> <p>Extensibility SDK: Custom evaluators for domain-specific needs</p> </li> <li> <p>Basic Observability</p> </li> <li>CLI output with pass/fail metrics</li> <li>SQLite backend for historical tracking</li> <li>Cost/token usage per test run</li> <li> <p>Export to CSV/JSON for external analysis</p> </li> <li> <p>Documentation &amp; Samples</p> </li> <li>Quickstart (first test in 5 minutes)</li> <li>Common patterns (chatbot, RAG, agent workflow)</li> <li>Best practices guide</li> <li>Migration guide (manual testing \u2192 automated)</li> </ol> <p>v2.0 \u2014 Visual Debugging &amp; Advanced Features (Months 5-8)</p> <ul> <li>Visual debugging UI (reasoning chain inspection)</li> <li>A/B testing framework (statistical significance)</li> <li>Foundry integration (team dashboards, production monitoring)</li> <li>GitHub Copilot SDK extension</li> <li>Advanced metrics (latency P95, cost optimization, quality-cost tradeoff)</li> <li>Dataset marketplace (community-contributed examples)</li> </ul> <p>v3.0 \u2014 Enterprise &amp; Ecosystem (Months 9-12)</p> <ul> <li>Compliance features (audit trails, GDPR/HIPAA)</li> <li>Multi-tenant evaluation runs (team isolation)</li> <li>RAG-specific evaluation patterns (retrieval quality, grounding accuracy)</li> <li>Prompt management integration (test prompts as they're managed)</li> <li>Azure DevOps pipeline tasks</li> <li>Commercial support offering</li> </ul> <p>Scope Discipline (Validation Feedback): - Risk: Feature creep (v2 features bleeding into v1) - Mitigation: Strict MVP discipline, feature flags for incomplete features, explicit v2 deferral</p>"},{"location":"research/FINAL_REPORT/#clear-adoption-strategy","title":"Clear Adoption Strategy","text":"<p>Phased Rollout (12-Month Plan)</p>"},{"location":"research/FINAL_REPORT/#phase-1-early-adopters-months-1-3","title":"Phase 1: Early Adopters (Months 1-3)","text":"<ul> <li>Target: 100 active developers</li> <li>Tactics:</li> <li>Personal outreach (GitHub discussions, direct invites)</li> <li>Conference talks (.NET Conf, Build, NDC)</li> <li>Blog series (4-5 posts on testing fundamentals)</li> <li>Stack Overflow canonical answers</li> <li>Goal: Validate MVP, gather feedback, build testimonials</li> </ul>"},{"location":"research/FINAL_REPORT/#phase-2-community-expansion-months-4-6","title":"Phase 2: Community Expansion (Months 4-6)","text":"<ul> <li>Target: 2,000+ active developers</li> <li>Tactics:</li> <li>Video tutorial series (YouTube, Channel 9)</li> <li>Workshops (pre-conference, developer days)</li> <li>NuGet featured package nomination</li> <li>Integration with Semantic Kernel docs/samples</li> <li>Reddit/Discord launches (r/dotnet, Semantic Kernel Discord)</li> <li>Goal: Achieve critical mass, establish as standard solution</li> </ul>"},{"location":"research/FINAL_REPORT/#phase-3-enterprise-adoption-months-7-12","title":"Phase 3: Enterprise Adoption (Months 7-12)","text":"<ul> <li>Target: 10+ enterprise customers, 10,000+ total developers</li> <li>Tactics:</li> <li>Case studies (early enterprise adopters)</li> <li>Webinar series (technical deep dives, ROI analysis)</li> <li>Azure Marketplace listing (NetAI.Testing.Enterprise)</li> <li>Microsoft Learn module (\"Testing AI Applications in .NET\")</li> <li>Commercial support offering</li> <li>Conference keynote opportunities</li> <li>Goal: Production validation, revenue generation, ecosystem maturity</li> </ul> <p>Viral Adoption Mechanics:</p> <p>Every .NET developer building AI asks \"how do I test this?\" \u2192 finds NetAI.Testing on Stack Overflow \u2192 shares with team \u2192 team adopts \u2192 production success story \u2192 shares on social \u2192 cycle repeats</p> <p>Distribution Channels: - NuGet: Primary distribution (<code>NetAI.Testing</code>, <code>NetAI.Testing.Foundry</code>) - GitHub: OSS repo, samples, community governance - Microsoft Docs: Learn module, Semantic Kernel integration docs - Azure Marketplace: Enterprise offering with premium support - Sample Projects: \"Production-Grade RAG with Comprehensive Testing\", \"Multi-Agent Workflow Testing Patterns\"</p>"},{"location":"research/FINAL_REPORT/#potential-roadmap-v1-v2-ecosystem","title":"Potential Roadmap (v1 \u2192 v2 \u2192 Ecosystem)","text":"<p>v1.0: Core Testing Framework (Months 1-4) - Golden dataset management - xUnit/NUnit/MSTest integration - Hallucination, factuality, regression metrics - SQLite backend, CLI output - Documentation and samples</p> <p>v1.5: GitHub Copilot Integration (Months 3-5) - IDE assistant patterns (test generation) - Copilot SDK extension (custom context) - CLI integration (<code>gh copilot ai-test</code>)</p> <p>v2.0: Visual Debugging &amp; Observability (Months 5-8) - Visual debugging UI (reasoning chain inspection) - A/B testing framework - Foundry integration (team dashboards, production monitoring) - Advanced metrics (latency, cost optimization)</p> <p>v2.5: Prompt Management Integration (Months 7-10) - Unified toolkit: Test prompts as you manage them - Versioned prompt testing - Environment-specific evaluation (dev/staging/prod)</p> <p>v3.0: Enterprise &amp; Ecosystem (Months 9-12) - Compliance features (audit trails, regulatory reporting) - RAG-specific evaluation patterns - Dataset marketplace (community contributions) - Commercial support</p> <p>Ecosystem Expansion (Year 2+): - NetAI.Prompts: Full prompt management platform (Opportunity #2) - NetAI.RAG.Patterns: Production RAG components (Opportunity #3) - NetAI.Agents.Testing: Multi-agent workflow testing (Opportunity #6) - NetAI.Governance: Enterprise security toolkit (Opportunity #7)</p> <p>Why This Roadmap Works: - Start with highest-impact problem (testing) - Add complementary features that leverage existing infrastructure - Build ecosystem naturally as community grows - Each package reinforces others (network effects)</p>"},{"location":"research/FINAL_REPORT/#4-community-validation-signals","title":"4. Community Validation Signals","text":""},{"location":"research/FINAL_REPORT/#example-recurring-questions","title":"Example Recurring Questions","text":"<p>Stack Overflow (Monthly Frequency): - \"How to test LLM outputs in .NET?\" (variations asked 15+ times/month) - \"Best practices for AI unit testing in C#\" - \"How to detect hallucinations in Semantic Kernel applications?\" - \"Regression testing for prompt changes\" - \"How to version control golden datasets for LLM testing?\"</p> <p>Reddit Threads (r/dotnet, r/csharp): - \"How do you guys test your AI apps?\" (recurring weekly) - \"Production AI horror story: hallucinations in customer data\" - \"Is there a .NET equivalent to Python's Ragas?\" - \"Manual testing AI outputs is killing our velocity\"</p> <p>Conference Q&amp;A (NDC, .NET Conf 2024): - Post-AI-talk questions consistently focus on testing/production concerns - \"What do you use for testing?\" is the #1 question after demos - Attendees report shipping delays due to quality assurance gaps</p>"},{"location":"research/FINAL_REPORT/#issue-patterns","title":"Issue Patterns","text":"<p>GitHub (Semantic Kernel, Microsoft.Extensions.AI, AutoGen): - Recurring feature requests for evaluation examples - Issues tagged \"testing\", \"evaluation\", \"quality assurance\" show consistent growth - Community workarounds shared in discussions (manual snapshot testing, homegrown scripts) - Maintainers acknowledge gap but prioritize core framework features</p> <p>Specific Examples: - Semantic Kernel #3,247: \"Add comprehensive testing guide\" - Microsoft.Extensions.AI #89: \"Evaluation library needs regression testing support\" - AutoGen discussions: \"How to test multi-agent workflows?\"</p>"},{"location":"research/FINAL_REPORT/#missing-abstractions","title":"Missing Abstractions","text":"<p>What Developers Are Missing:</p> <ol> <li>Golden Dataset Standard</li> <li>No agreed-upon format (JSON? CSV? Custom?)</li> <li>No versioning strategy (Git? Database? Hybrid?)</li> <li> <p>No collaboration patterns (technical + non-technical stakeholders)</p> </li> <li> <p>LLM Quality Metrics</p> </li> <li>Hallucination detection (no standardized approach)</li> <li>Factuality verification (citation extraction inconsistent)</li> <li>Semantic similarity (embedding choice unclear)</li> <li> <p>Pass/fail thresholds (arbitrary, not data-driven)</p> </li> <li> <p>Test Framework Integration</p> </li> <li>No xUnit/NUnit attributes for AI tests</li> <li>No fluent assertions for LLM outputs</li> <li>No Test Explorer integration (Visual Studio, Rider)</li> <li> <p>No CI/CD patterns (GitHub Actions, Azure DevOps)</p> </li> <li> <p>Observability Tooling</p> </li> <li>No visual debugging for reasoning chains</li> <li>No production monitoring for quality drift</li> <li>No cost/token tracking dashboards</li> <li>No A/B testing infrastructure</li> </ol>"},{"location":"research/FINAL_REPORT/#workarounds-developers-use-today","title":"Workarounds Developers Use Today","text":"<p>Current Pain Points:</p> <ol> <li>Manual Snapshot Testing</li> <li>Developers save LLM outputs to text files</li> <li>Manual diff comparison (error-prone, time-consuming)</li> <li>No semantic similarity\u2014exact string match only</li> <li> <p>Breaks on non-deterministic outputs</p> </li> <li> <p>Homegrown Scripts</p> </li> <li>Python bridge to use Ragas/DeepEval (adds complexity)</li> <li>Custom evaluation logic (not reusable, hard to maintain)</li> <li>Ad-hoc metric calculation (inconsistent across teams)</li> <li> <p>No standardization or community sharing</p> </li> <li> <p>Azure-Only Solutions</p> </li> <li>Forced to use Azure AI Foundry for evaluation (vendor lock-in)</li> <li>Cannot test locally or in CI/CD without Azure credentials</li> <li>Expensive for individual developers/small teams</li> <li> <p>Excludes non-Azure scenarios (on-prem, other clouds)</p> </li> <li> <p>No Testing Strategy</p> </li> <li>\"Hope it works in production\" (unacceptable for enterprises)</li> <li>Manual QA only (doesn't scale, misses edge cases)</li> <li>Customer reports bugs (testing in production anti-pattern)</li> <li>Delayed launches due to quality concerns</li> </ol> <p>Evidence: Blog posts, Stack Overflow answers, GitHub discussions all show developers sharing these workarounds\u2014clear signal that no standard solution exists.</p>"},{"location":"research/FINAL_REPORT/#5-differentiation-strategy","title":"5. Differentiation Strategy","text":""},{"location":"research/FINAL_REPORT/#how-this-avoids-being-just-another-ai-wrapper","title":"How This Avoids Being \"Just Another AI Wrapper\"","text":"<p>We're NOT building: - \u274c Another OpenAI API client (Microsoft.Extensions.AI already exists) - \u274c Another chat UI wrapper (Semantic Kernel has samples) - \u274c Another \"getting started\" template (dozens exist) - \u274c Another LLM orchestration framework (Semantic Kernel, LangChain.NET exist)</p> <p>We ARE building: - \u2705 Quality assurance infrastructure for production AI applications - \u2705 Testing-first mindset for non-deterministic systems - \u2705 Developer experience tooling that elevates ecosystem maturity - \u2705 .NET-native solution leveraging type safety, performance, IDE integration</p> <p>Core Differentiation:</p> <p>\"We're not helping developers call LLMs faster. We're helping them ship LLM applications with confidence.\"</p> <p>Value Proposition: - Problem space is orthogonal to API access (quality assurance, not model integration) - Solves production blocker, not \"getting started\" friction - Addresses enterprise need (compliance, governance, reliability) - Builds upon existing abstractions (Microsoft.Extensions.AI) rather than competing</p>"},{"location":"research/FINAL_REPORT/#how-this-avoids-being-just-another-demo-sample","title":"How This Avoids Being \"Just Another Demo Sample\"","text":"<p>We're NOT building: - \u274c Single-use demo that showcases a framework - \u274c Educational-only project with no production viability - \u274c Reference architecture without runnable code - \u274c Minimal example that developers must rewrite entirely</p> <p>We ARE building: - \u2705 Production-grade library (NuGet package, versioned, supported) - \u2705 Immediate value (developers use it as-is, no rewrites) - \u2705 Extensibility SDK (custom evaluators, domain-specific metrics) - \u2705 Enterprise features (compliance, governance, commercial support)</p> <p>Core Differentiation:</p> <p>\"This is not a sample you learn from. This is a library you depend on in production.\"</p> <p>Sustainability Model: - OSS Core: Free, MIT-licensed, community-driven (builds adoption) - Enterprise Offering: Premium features (Foundry integration, commercial support) funds maintenance - Ecosystem Revenue: Consulting, workshops, training (DevRel synergy)</p>"},{"location":"research/FINAL_REPORT/#how-this-avoids-being-just-another-experimental-repo","title":"How This Avoids Being \"Just Another Experimental Repo\"","text":"<p>We're NOT building: - \u274c Proof-of-concept that may be abandoned - \u274c Personal project without community governance - \u274c Unstable API with breaking changes every release - \u274c Niche tool for specific use case only</p> <p>WE ARE building: - \u2705 Committed roadmap (v1 \u2192 v2 \u2192 v3 with clear milestones) - \u2705 Community governance (RFC process, contributor guidelines) - \u2705 Semantic versioning (stable API contracts, LTS commitment) - \u2705 Universal applicability (any .NET AI app can use it)</p> <p>Core Differentiation:</p> <p>\"This is infrastructure-grade tooling with long-term support, not a weekend hackathon project.\"</p> <p>Commitment Signals: - Maintainer Guarantee: You (as Microsoft Cloud Advocate) provide credibility and sustainability - Enterprise Customers: Production usage validates stability - Microsoft Alignment: Complementary to official tools (not competing) - Roadmap Transparency: Public milestones, no surprises</p>"},{"location":"research/FINAL_REPORT/#competitive-positioning-summary","title":"Competitive Positioning Summary","text":"Competitor Why We're Different Microsoft.Extensions.AI.Evaluation We extend (not replace) with production features they lack: regression testing, golden datasets, visual debugging, Foundry integration Python Tools (Ragas, DeepEval, TruLens) .NET-native (no Python bridge), type-safe (compile-time validation), enterprise-focused (compliance, governance), Azure ecosystem integration Azure AI Foundry (Evaluation) Works standalone (no vendor lock-in), local development support, OSS model (community contributions), freemium upgrade path Homegrown Solutions Standardized (reusable across teams), community-supported (not siloed), battle-tested (production validation), extensible (custom evaluators) <p>Market Position: \"The xUnit for AI Applications\"\u2014just as xUnit became the de facto testing standard for .NET, NetAI.Testing becomes the standard for AI quality assurance.</p> <p>Defensible Moat: 1. First-Mover Advantage: No .NET equivalent exists (category creation) 2. Network Effects: Community contributions (custom evaluators) increase value 3. Integration Depth: xUnit/NUnit, Visual Studio, Copilot, Foundry (high switching costs) 4. Thought Leadership: You become \"the .NET AI testing authority\" (authority moat) 5. Ecosystem Lock-In: v2+ integrations (prompts, RAG, agents) reinforce core toolkit</p>"},{"location":"research/FINAL_REPORT/#strategic-framing-validation","title":"Strategic Framing Validation","text":""},{"location":"research/FINAL_REPORT/#inevitable-in-hindsight","title":"\"Inevitable in Hindsight\"","text":"<p>In 2026, developers will say:</p> <p>\"How did we ship AI applications WITHOUT comprehensive testing tooling? This should have existed from day one.\"</p> <p>Why This Feels Inevitable: - Testing is foundational to software engineering (not optional) - AI moves to production \u2192 production needs testing (logical progression) - Python already solved this \u2192 .NET should have equivalent (parity expectation) - Quality assurance is obvious requirement, not innovation</p>"},{"location":"research/FINAL_REPORT/#why-didnt-we-have-this-already","title":"\"Why Didn't We Have This Already?\"","text":"<p>Community Reaction (Predicted):</p> <p>\"This is exactly what I needed. I can't believe I've been doing manual testing this whole time.\"</p> <p>Why The Gap Existed: - .NET AI ecosystem too new (Semantic Kernel v1 only in 2023) - Microsoft prioritized core frameworks first (correct prioritization) - Community assumed Microsoft would build it (diffusion of responsibility) - Testing is unglamorous (developers prefer building features)</p> <p>Why NOW Is The Right Time: - Foundation stable (Microsoft.Extensions.AI GA) - Production adoption wave beginning (testing becomes blocker) - Community matured (ready for professional tooling) - You have authority to lead (Cloud Advocate credibility)</p>"},{"location":"research/FINAL_REPORT/#reinforcing-strategic-themes","title":"Reinforcing Strategic Themes","text":""},{"location":"research/FINAL_REPORT/#ai-as-first-class-citizen-in-net","title":"\u2705 AI as First-Class Citizen in .NET","text":"<p>Testing signals that AI is production-grade, not experimental. Just as .NET has mature testing for web apps, databases, APIs\u2014now it has testing for AI.</p>"},{"location":"research/FINAL_REPORT/#foundry-as-natural-platform-choice","title":"\u2705 Foundry as Natural Platform Choice","text":"<p>Toolkit showcases Foundry's observability and team collaboration capabilities in authentic context (not marketing).</p>"},{"location":"research/FINAL_REPORT/#github-copilot-as-productivity-multiplier","title":"\u2705 GitHub Copilot as Productivity Multiplier","text":"<p>Copilot integration demonstrates that AI development tooling is itself AI-enhanced (meta-narrative).</p>"},{"location":"research/FINAL_REPORT/#your-authority-in-ai-net","title":"\u2705 Your Authority in AI + .NET","text":"<p>You become the thought leader who elevated .NET AI from \"demos that work\" to \"production systems we trust.\"</p>"},{"location":"research/FINAL_REPORT/#conclusion","title":"Conclusion","text":"<p>This research initiative has identified a clear, validated, high-impact opportunity: building a unified AI Testing &amp; Observability Toolkit for .NET. The evidence is overwhelming, the timing is optimal, and the strategic alignment is exceptional.</p> <p>Key Success Factors: 1. Triple-validated demand (community, market, technical analysis converge) 2. No competing .NET solution (category creation opportunity) 3. Perfect expertise alignment (AI + .NET + GitHub + DevRel) 4. Organic ecosystem integration (Foundry, Copilot, Azure) 5. Clear adoption path (OSS \u2192 enterprise, freemium model) 6. Defensible moat (first-mover, network effects, thought leadership)</p> <p>Expected Outcomes: - Community Impact: 10,000+ developers adopt within 12 months - Thought Leadership: You become \"the .NET AI testing authority\" - Ecosystem Maturity: .NET AI transitions from experimental to production-grade - Microsoft Value: Showcases Foundry and Copilot in high-value developer workflow - Revenue Potential: Enterprise offering funds long-term sustainability</p> <p>Next Steps: 1. Validate resource allocation (3-4 month focused development window) 2. Engage Microsoft.Extensions.AI team (collaborative relationship) 3. Spike hallucination detection (prototype LLM-as-judge accuracy) 4. Define v1 feature freeze (strict MVP scope) 5. Build initial community (pre-announce, gather early adopters) 6. Create detailed technical design (API surface, extensibility, storage)</p> <p>Final Recommendation: Proceed with AI Testing &amp; Observability Toolkit as the primary strategic initiative. If executed with discipline, this becomes the de facto standard for AI quality assurance in .NET.</p> <p>Prepared by: Mulder (Research Director) Validated by: Doggett (Validation Agent) \u2014 Confidence: 92/100 Validated by: Reyes (Advocacy Alignment) \u2014 Confidence: 94/100 Final Status: APPROVED \u2014 READY FOR EXECUTION</p>"},{"location":"research/Initial_Prompt/","title":"\ud83d\udd0e Strategic Research Initiative: Identifying High-Impact AI + .NET Opportunities","text":""},{"location":"research/Initial_Prompt/#strategic-research-initiative-identifying-high-impact-ai-net-opportunities","title":"\ud83d\udd0e Strategic Research Initiative: Identifying High-Impact AI + .NET Opportunities","text":""},{"location":"research/Initial_Prompt/#context","title":"Context","text":"<p>You are tasked with assembling and coordinating a research team to identify high-impact future opportunities in the intersection of Artificial Intelligence and .NET development.</p> <p>The sponsor of this initiative is a Microsoft-focused Cloud Advocate known for:</p> <ul> <li>AI + .NET development\\</li> <li>Enterprise and developer community engagement\\</li> <li>Microsoft Azure ecosystem\\</li> <li>Microsoft Foundry\\</li> <li>GitHub Copilot (IDE assistant, Copilot SDK, Copilot CLI)\\</li> <li>Open-source NuGet package creation\\</li> <li>Educational and community-driven sample projects</li> </ul> <p>The goal is not academic research --- it is strategic product discovery for the .NET developer ecosystem.</p>"},{"location":"research/Initial_Prompt/#primary-objective","title":"Primary Objective","text":"<p>Identify one or more strategic opportunities that could result in:</p> <ul> <li>A widely adopted NuGet package\\     and/or</li> <li>A highly resonant open-source sample project\\     and/or</li> <li>A small ecosystem of complementary libraries</li> </ul> <p>These opportunities must:</p> <ol> <li>Address a real, validated pain point in the global .NET community.</li> <li>Be strongly aligned with AI + .NET.</li> <li>Naturally promote or integrate:<ul> <li>Microsoft Foundry</li> <li>GitHub Copilot (IDE, SDK, CLI)</li> </ul> </li> <li>Be technically realistic for a small but highly focused team.</li> <li>Have viral or community adoption potential.</li> </ol>"},{"location":"research/Initial_Prompt/#scope-of-research","title":"Scope of Research","text":"<p>The research team should:</p> <ul> <li>Analyze global .NET developer communities:<ul> <li>GitHub issues and trending repos</li> <li>Stack Overflow</li> <li>Reddit (.NET, C#, Blazor, MAUI, AI, LLMs)</li> <li>Discord communities</li> <li>X / LinkedIn discussions</li> <li>Conference talks and blog posts</li> </ul> </li> <li>Identify recurring friction patterns in:<ul> <li>AI integration</li> <li>LLM orchestration</li> <li>RAG patterns</li> <li>Local vs cloud models</li> <li>Prompt engineering in C#</li> <li>Agent frameworks</li> <li>Copilot extensibility</li> <li>AI evaluation/testing</li> <li>AI application architecture</li> </ul> </li> <li>Detect emerging but underserved trends.</li> <li>Identify tooling gaps that developers repeatedly try to solve     themselves.</li> <li>Distinguish between:<ul> <li>Beginner-level gaps</li> <li>Enterprise-level architectural gaps</li> </ul> </li> </ul> <p>The research must prioritize signal over hype.</p>"},{"location":"research/Initial_Prompt/#constraints","title":"Constraints","text":"<ul> <li>Do not propose generic \"yet another wrapper for OpenAI\".</li> <li>Do not replicate existing major frameworks unless there is a clear     differentiation.</li> <li>Avoid ideas that require massive infrastructure or internal     Microsoft-only access.</li> <li>Solutions must be viable as open-source projects.</li> </ul>"},{"location":"research/Initial_Prompt/#required-output","title":"Required Output","text":"<p>Produce a structured report that includes:</p>"},{"location":"research/Initial_Prompt/#1-key-patterns-identified","title":"1. Key Patterns Identified","text":"<p>Clear summary of recurring unmet needs in the AI + .NET space.</p>"},{"location":"research/Initial_Prompt/#2-opportunity-shortlist","title":"2. Opportunity Shortlist","text":"<p>3--7 concrete opportunity areas ranked by:</p> <ul> <li>Community demand</li> <li>Technical feasibility</li> <li>Strategic alignment</li> <li>Potential for adoption</li> <li>Differentiation from existing tools</li> </ul>"},{"location":"research/Initial_Prompt/#3-recommended-primary-direction","title":"3. Recommended Primary Direction","text":"<p>Select ONE high-conviction opportunity and provide:</p> <ul> <li>Problem statement</li> <li>Target developer persona</li> <li>Why now?</li> <li>Why .NET specifically?</li> <li>Why this sponsor is well-positioned to build it?</li> <li>How it integrates Microsoft Foundry</li> <li>How it showcases GitHub Copilot (IDE, SDK, CLI)</li> <li>Suggested initial feature set (MVP scope)</li> <li>Clear adoption strategy</li> <li>Potential roadmap (v1 \u2192 v2 \u2192 ecosystem)</li> </ul>"},{"location":"research/Initial_Prompt/#4-community-validation-signals","title":"4. Community Validation Signals","text":"<p>Evidence that the problem is real: - Example recurring questions - Issue patterns - Missing abstractions - Workarounds developers use today</p>"},{"location":"research/Initial_Prompt/#5-differentiation-strategy","title":"5. Differentiation Strategy","text":"<p>Explain how this idea avoids being: - Just another AI wrapper - Just another demo sample - Just another experimental repo</p>"},{"location":"research/Initial_Prompt/#strategic-framing","title":"Strategic Framing","text":"<p>The solution should:</p> <ul> <li>Feel \"inevitable\" in hindsight.</li> <li>Be something the .NET community says: &gt; \"Why didn't we have this     already?\"</li> </ul> <p>It should reinforce:</p> <ul> <li>AI as a first-class citizen in .NET.</li> <li>Foundry as a natural platform choice.</li> <li>GitHub Copilot as a productivity multiplier.</li> <li>The sponsor's authority in AI + .NET.</li> </ul>"},{"location":"research/Initial_Prompt/#decision-criteria","title":"Decision Criteria","text":"<p>Prioritize opportunities that are:</p> <ul> <li>Opinionated</li> <li>Developer-centric</li> <li>Practical</li> <li>Educational</li> <li>Architecturally sound</li> <li>Demonstrably useful within 5--15 minutes of trying them</li> </ul>"},{"location":"research/Initial_Prompt/#final-instruction","title":"Final Instruction","text":"<p>Organize and create whatever research agents, analysis agents, synthesis agents, or validation agents you determine are necessary to fulfill this objective.</p> <p>The only requirement is that the final output is cohesive, evidence-driven, strategically actionable, and suitable for immediate execution.</p>"}]}