<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Production-grade AI testing and evaluation toolkit for .NET applications — xUnit for AI.">
      
      
        <meta name="author" content="Bruno Capuano">
      
      
      
        <link rel="prev" href="../best-practices/">
      
      
        <link rel="next" href="../publishing/">
      
      
        
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.3">
    
    
      
        <title>Comparison with Microsoft - ElBruno.AI.Evaluation — AI Testing for .NET</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  <link href="../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#elbrunoaievaluation-vs-microsoftextensionsaievaluation" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="ElBruno.AI.Evaluation — AI Testing for .NET" class="md-header__button md-logo" aria-label="ElBruno.AI.Evaluation — AI Testing for .NET" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ElBruno.AI.Evaluation — AI Testing for .NET
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Comparison with Microsoft
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 2c-1.82 0-3.53.5-5 1.35C8 5.08 10 8.3 10 12s-2 6.92-5 8.65C6.47 21.5 8.18 22 10 22a10 10 0 0 0 10-10A10 10 0 0 0 10 2"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/elbruno/netai-nextwin" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    elbruno/netai-nextwin
  </div>
</a>
      </div>
    
  </nav>
  
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="ElBruno.AI.Evaluation — AI Testing for .NET" class="md-nav__button md-logo" aria-label="ElBruno.AI.Evaluation — AI Testing for .NET" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    ElBruno.AI.Evaluation — AI Testing for .NET
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/elbruno/netai-nextwin" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    elbruno/netai-nextwin
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2">
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Blog Series
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Blog Series
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blog/01-introducing-elbruno-ai-evaluation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Introducing ElBruno.AI.Evaluation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blog/02-golden-datasets-for-ai-testing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Golden Datasets for AI Testing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blog/03-ai-evaluators-deep-dive/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    AI Evaluators Deep Dive
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blog/04-ai-testing-with-xunit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    AI Testing with xUnit
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blog/05-from-demo-to-production/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    From Demo to Production
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blog/06-synthetic-data-generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Synthetic Data Generation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blog/07-choosing-the-right-evaluators/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Choosing the Right Evaluators
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Library Docs
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Library Docs
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../quickstart/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Quickstart
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../golden-datasets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Golden Datasets
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../evaluation-metrics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Evaluation Metrics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../synthetic-data/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Synthetic Data
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../best-practices/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Best Practices
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Comparison with Microsoft
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Comparison with Microsoft
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#executive-summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Executive Summary
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#architecture-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      
        Architecture Comparison
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Architecture Comparison">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ievaluator-interface" class="md-nav__link">
    <span class="md-ellipsis">
      
        IEvaluator Interface
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#result-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Result Models
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pipeline-orchestration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pipeline &amp; Orchestration
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#feature-matrix" class="md-nav__link">
    <span class="md-ellipsis">
      
        Feature Matrix
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#overlapping-areas" class="md-nav__link">
    <span class="md-ellipsis">
      
        Overlapping Areas
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Overlapping Areas">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#quality-evaluation-relevance-coherence-fluency" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quality Evaluation (Relevance, Coherence, Fluency)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#safety-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Safety Evaluation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#unique-to-microsoft" class="md-nav__link">
    <span class="md-ellipsis">
      
        Unique to Microsoft
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#unique-to-elbruno" class="md-nav__link">
    <span class="md-ellipsis">
      
        Unique to ElBruno
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gap-analysis-scenarios-not-covered-by-official-libraries" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gap Analysis: Scenarios NOT Covered by Official Libraries
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Gap Analysis: Scenarios NOT Covered by Official Libraries">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-synthetic-test-data-generation-critical-gap" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Synthetic Test Data Generation (Critical Gap)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-offline-air-gapped-evaluation-critical-gap" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Offline / Air-Gapped Evaluation (Critical Gap)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-golden-dataset-lifecycle-versioning-medium-gap" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Golden Dataset Lifecycle &amp; Versioning (Medium Gap)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-regression-detection-in-cicd-medium-gap" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Regression Detection in CI/CD (Medium Gap)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-xunit-native-test-assertions-medium-gap" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. xUnit-Native Test Assertions (Medium Gap)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-cost-token-tracking-across-runs-small-gap" class="md-nav__link">
    <span class="md-ellipsis">
      
        6. Cost &amp; Token Tracking Across Runs (Small Gap)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7-deterministic-debuggable-evaluation-small-gap" class="md-nav__link">
    <span class="md-ellipsis">
      
        7. Deterministic, Debuggable Evaluation (Small Gap)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#when-to-use-which" class="md-nav__link">
    <span class="md-ellipsis">
      
        When to Use Which
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="When to Use Which">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#use-microsoftextensionsaievaluation-if" class="md-nav__link">
    <span class="md-ellipsis">
      
        Use Microsoft.Extensions.AI.Evaluation if:
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#use-elbrunoaievaluation-if" class="md-nav__link">
    <span class="md-ellipsis">
      
        Use ElBruno.AI.Evaluation if:
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#decision-tree" class="md-nav__link">
    <span class="md-ellipsis">
      
        Decision Tree
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#complementary-usage" class="md-nav__link">
    <span class="md-ellipsis">
      
        Complementary Usage
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Complementary Usage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pattern-1-hybrid-evaluation-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pattern 1: Hybrid Evaluation Pipeline
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pattern-2-regression-testing-with-baseline-snapshots" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pattern 2: Regression Testing with Baseline Snapshots
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pattern-3-golden-dataset-microsoft-llm-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pattern 3: Golden Dataset + Microsoft LLM Evaluation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pattern-4-test-automation-enterprise-reporting" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pattern 4: Test Automation + Enterprise Reporting
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary-table" class="md-nav__link">
    <span class="md-ellipsis">
      
        Summary Table
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      
        Conclusion
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../publishing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Publishing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#executive-summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Executive Summary
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#architecture-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      
        Architecture Comparison
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Architecture Comparison">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ievaluator-interface" class="md-nav__link">
    <span class="md-ellipsis">
      
        IEvaluator Interface
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#result-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Result Models
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pipeline-orchestration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pipeline &amp; Orchestration
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#feature-matrix" class="md-nav__link">
    <span class="md-ellipsis">
      
        Feature Matrix
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#overlapping-areas" class="md-nav__link">
    <span class="md-ellipsis">
      
        Overlapping Areas
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Overlapping Areas">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#quality-evaluation-relevance-coherence-fluency" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quality Evaluation (Relevance, Coherence, Fluency)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#safety-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Safety Evaluation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#unique-to-microsoft" class="md-nav__link">
    <span class="md-ellipsis">
      
        Unique to Microsoft
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#unique-to-elbruno" class="md-nav__link">
    <span class="md-ellipsis">
      
        Unique to ElBruno
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gap-analysis-scenarios-not-covered-by-official-libraries" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gap Analysis: Scenarios NOT Covered by Official Libraries
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Gap Analysis: Scenarios NOT Covered by Official Libraries">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-synthetic-test-data-generation-critical-gap" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Synthetic Test Data Generation (Critical Gap)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-offline-air-gapped-evaluation-critical-gap" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Offline / Air-Gapped Evaluation (Critical Gap)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-golden-dataset-lifecycle-versioning-medium-gap" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Golden Dataset Lifecycle &amp; Versioning (Medium Gap)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-regression-detection-in-cicd-medium-gap" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Regression Detection in CI/CD (Medium Gap)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-xunit-native-test-assertions-medium-gap" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. xUnit-Native Test Assertions (Medium Gap)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-cost-token-tracking-across-runs-small-gap" class="md-nav__link">
    <span class="md-ellipsis">
      
        6. Cost &amp; Token Tracking Across Runs (Small Gap)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7-deterministic-debuggable-evaluation-small-gap" class="md-nav__link">
    <span class="md-ellipsis">
      
        7. Deterministic, Debuggable Evaluation (Small Gap)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#when-to-use-which" class="md-nav__link">
    <span class="md-ellipsis">
      
        When to Use Which
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="When to Use Which">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#use-microsoftextensionsaievaluation-if" class="md-nav__link">
    <span class="md-ellipsis">
      
        Use Microsoft.Extensions.AI.Evaluation if:
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#use-elbrunoaievaluation-if" class="md-nav__link">
    <span class="md-ellipsis">
      
        Use ElBruno.AI.Evaluation if:
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#decision-tree" class="md-nav__link">
    <span class="md-ellipsis">
      
        Decision Tree
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#complementary-usage" class="md-nav__link">
    <span class="md-ellipsis">
      
        Complementary Usage
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Complementary Usage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pattern-1-hybrid-evaluation-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pattern 1: Hybrid Evaluation Pipeline
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pattern-2-regression-testing-with-baseline-snapshots" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pattern 2: Regression Testing with Baseline Snapshots
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pattern-3-golden-dataset-microsoft-llm-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pattern 3: Golden Dataset + Microsoft LLM Evaluation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pattern-4-test-automation-enterprise-reporting" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pattern 4: Test Automation + Enterprise Reporting
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary-table" class="md-nav__link">
    <span class="md-ellipsis">
      
        Summary Table
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      
        Conclusion
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="elbrunoaievaluation-vs-microsoftextensionsaievaluation">ElBruno.AI.Evaluation vs Microsoft.Extensions.AI.Evaluation<a class="headerlink" href="#elbrunoaievaluation-vs-microsoftextensionsaievaluation" title="Permanent link">¶</a></h1>
<h2 id="executive-summary">Executive Summary<a class="headerlink" href="#executive-summary" title="Permanent link">¶</a></h2>
<p><strong>Microsoft.Extensions.AI.Evaluation</strong> (Official Libraries) is a comprehensive, enterprise-grade framework from Microsoft for evaluating AI applications at scale. It provides LLM-based quality evaluators (Relevance, Completeness, Fluency, etc.), traditional NLP metrics (BLEU, GLEU, F1), Azure AI Foundry safety evaluators, response caching, and professional HTML reporting—all integrated with Microsoft's broader .NET AI ecosystem.</p>
<p><strong>ElBruno.AI.Evaluation</strong> (Our Toolkit) is a lightweight, deterministic evaluation framework designed for offline scenarios, golden dataset management, and xUnit-native test integration. It provides fast, zero-external-dependency evaluators (Hallucination, Factuality, Relevance, Coherence, Safety), synthetic data generation, regression detection, and SQLite-based persistence. Both toolkits fill different niches in the evaluation pipeline: Microsoft excels at LLM-powered quality and safety analysis, while ElBruno excels at baseline management, local regression detection, and test automation.</p>
<hr>
<h2 id="architecture-comparison">Architecture Comparison<a class="headerlink" href="#architecture-comparison" title="Permanent link">¶</a></h2>
<h3 id="ievaluator-interface">IEvaluator Interface<a class="headerlink" href="#ievaluator-interface" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Microsoft</th>
<th>ElBruno</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Method Signature</strong></td>
<td><code>EvaluateAsync(IEnumerable&lt;ChatMessage&gt;, ChatResponse, ChatConfiguration?, IEnumerable&lt;EvaluationContext&gt;?, CancellationToken)</code></td>
<td><code>EvaluateAsync(string input, string output, string? expectedOutput, CancellationToken)</code></td>
</tr>
<tr>
<td><strong>Scope</strong></td>
<td>Full conversation context (message history)</td>
<td>Single input/output pair</td>
</tr>
<tr>
<td><strong>Configuration</strong></td>
<td>ChatConfiguration object for LLM settings</td>
<td>Evaluator-specific configurable thresholds</td>
</tr>
<tr>
<td><strong>Context Support</strong></td>
<td>EvaluationContext for metadata</td>
<td>Golden dataset context field + metadata</td>
</tr>
</tbody>
</table>
<p><strong>Design Philosophy:</strong>
- <strong>Microsoft</strong>: Conversation-aware, LLM-centric. Evaluators receive full chat history and output ChatResponse objects from the LLM.
- <strong>ElBruno</strong>: Input/output-centric, deterministic-first. Evaluators analyze self-contained text pairs without external calls.</p>
<h3 id="result-models">Result Models<a class="headerlink" href="#result-models" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Property</th>
<th>Microsoft (EvaluationResult)</th>
<th>ElBruno (EvaluationResult)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Core Score</strong></td>
<td><code>double</code> (0-1)</td>
<td><code>double</code> (0-1)</td>
</tr>
<tr>
<td><strong>Pass/Fail</strong></td>
<td><code>bool Passed</code></td>
<td><code>bool Passed</code></td>
</tr>
<tr>
<td><strong>Details</strong></td>
<td><code>string Message</code></td>
<td><code>string Details</code></td>
</tr>
<tr>
<td><strong>Metric Breakdown</strong></td>
<td>Built into result</td>
<td><code>Dictionary&lt;string, MetricScore&gt;</code> with weights, thresholds, timestamps</td>
</tr>
<tr>
<td><strong>Severity</strong></td>
<td><code>Severity</code> enum (Info/Warning/Error)</td>
<td>N/A</td>
</tr>
</tbody>
</table>
<p><strong>Key Difference:</strong> ElBruno's <code>MetricScore</code> objects are first-class, enabling fine-grained metric tracking, weighted aggregation, per-metric thresholds, and temporal analysis. Microsoft bundles metrics within the result.</p>
<h3 id="pipeline-orchestration">Pipeline &amp; Orchestration<a class="headerlink" href="#pipeline-orchestration" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Microsoft</th>
<th>ElBruno</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Execution Model</strong></td>
<td>ScenarioRun + ExecutionName for comparing runs</td>
<td>EvaluationRun with timestamp, token tracking, cost estimation</td>
</tr>
<tr>
<td><strong>Configuration Binding</strong></td>
<td>ChatConfiguration per scenario</td>
<td>Evaluator thresholds + AggregateScorer strategies</td>
</tr>
<tr>
<td><strong>Builder Pattern</strong></td>
<td>N/A (constructor injection)</td>
<td>EvaluationPipelineBuilder (fluent)</td>
</tr>
<tr>
<td><strong>Baseline Comparison</strong></td>
<td>Manual or via CLI (<code>dotnet aieval</code>)</td>
<td>BaselineSnapshot + RegressionDetector in code</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="feature-matrix">Feature Matrix<a class="headerlink" href="#feature-matrix" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Microsoft</th>
<th>ElBruno</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Core Evaluators</strong></td>
<td>✅ LLM-based (Relevance, Completeness, Fluency, Coherence, Groundedness, etc.)</td>
<td>✅ Deterministic (Hallucination, Factuality, Relevance, Coherence, Safety)</td>
<td>Microsoft uses LLM calls; ElBruno uses heuristics.</td>
</tr>
<tr>
<td><strong>NLP Metrics</strong></td>
<td>✅ BLEU, GLEU, F1 (via .NLP package)</td>
<td>❌ Not provided</td>
<td>Microsoft has traditional linguistics metrics.</td>
</tr>
<tr>
<td><strong>Agent-Focused Evaluators</strong></td>
<td>✅ IntentResolution, TaskAdherence, ToolCallAccuracy</td>
<td>❌ Not provided</td>
<td>Specific to agentic systems.</td>
</tr>
<tr>
<td><strong>Safety Evaluation</strong></td>
<td>✅ Azure AI Foundry (GroundednessPro, ProtectedMaterial, HateAndUnfairness, Violence, Sexual, etc.)</td>
<td>✅ Basic blocklist + PII detection</td>
<td>Microsoft's safety suite is comprehensive; ElBruno's is local/fast.</td>
</tr>
<tr>
<td><strong>Response Caching</strong></td>
<td>✅ Avoid re-calling LLM</td>
<td>❌ N/A (no LLM calls in v1)</td>
<td>Microsoft optimizes token usage.</td>
</tr>
<tr>
<td><strong>Conversation Context</strong></td>
<td>✅ Full message history support</td>
<td>❌ Single I/O pairs</td>
<td>Microsoft understands dialogue flow.</td>
</tr>
<tr>
<td><strong>HTML Report Generation</strong></td>
<td>✅ Via <code>dotnet aieval</code> CLI</td>
<td>❌ Not provided</td>
<td>Microsoft includes visualization tools.</td>
</tr>
<tr>
<td><strong>Azure Storage Integration</strong></td>
<td>✅ Cloud-based result persistence</td>
<td>❌ Not provided</td>
<td>Microsoft supports enterprise scale.</td>
</tr>
<tr>
<td><strong>SQLite Persistence</strong></td>
<td>❌ Not provided</td>
<td>✅ SqliteResultStore</td>
<td>ElBruno offers local, portable storage.</td>
</tr>
<tr>
<td><strong>Golden Dataset Management</strong></td>
<td>❌ Not provided</td>
<td>✅ Versioning, diffing, subsetting, loader/exporter</td>
<td>ElBruno's core strength.</td>
</tr>
<tr>
<td><strong>Regression Detection</strong></td>
<td>❌ Manual baseline comparison</td>
<td>✅ RegressionDetector with tolerance thresholds</td>
<td>ElBruno detects perf degradation automatically.</td>
</tr>
<tr>
<td><strong>Baseline Snapshots</strong></td>
<td>❌ Manual tracking</td>
<td>✅ Automatic snapshot creation + comparison</td>
<td>ElBruno streamlines baseline workflows.</td>
</tr>
<tr>
<td><strong>Synthetic Data Generation</strong></td>
<td>❌ Not provided</td>
<td>✅ SyntheticDatasetBuilder with deterministic/LLM/composite generators, templates (QA, RAG, Adversarial, Domain)</td>
<td><strong>Unique to ElBruno.</strong></td>
</tr>
<tr>
<td><strong>xUnit Test Integration</strong></td>
<td>❌ Not provided</td>
<td>✅ AIEvaluationTest attribute, AIAssert methods</td>
<td><strong>Unique to ElBruno.</strong></td>
</tr>
<tr>
<td><strong>CSV/JSON Export</strong></td>
<td>❌ Not provided</td>
<td>✅ JsonExporter, CsvExporter</td>
<td>ElBruno offers flexible data export.</td>
</tr>
<tr>
<td><strong>Console Reporter</strong></td>
<td>✅ Via CLI</td>
<td>✅ ConsoleReporter in-process</td>
<td>Both support console output.</td>
</tr>
<tr>
<td><strong>Offline Capability</strong></td>
<td>⚠️ Requires Azure (Safety)</td>
<td>✅ Fully offline, no external APIs</td>
<td>ElBruno is air-gapped by design.</td>
</tr>
<tr>
<td><strong>Cost Tracking</strong></td>
<td>✅ Token counting built-in</td>
<td>✅ Optional EstimatedCost on EvaluationRun</td>
<td>Both support cost analysis.</td>
</tr>
<tr>
<td><strong>Weighted Metrics</strong></td>
<td>❌ Implicit (via result aggregation)</td>
<td>✅ Explicit weights per MetricScore</td>
<td>ElBruno enables fine-tuned aggregation.</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="overlapping-areas">Overlapping Areas<a class="headerlink" href="#overlapping-areas" title="Permanent link">¶</a></h2>
<h3 id="quality-evaluation-relevance-coherence-fluency">Quality Evaluation (Relevance, Coherence, Fluency)<a class="headerlink" href="#quality-evaluation-relevance-coherence-fluency" title="Permanent link">¶</a></h3>
<p>Both toolkits provide relevance and coherence evaluators, but use different approaches:</p>
<p><strong>Why ElBruno's version exists:</strong>
1. <strong>Offline-first:</strong> No LLM calls = no latency, no cost, no Azure dependency. Useful for:
   - Local development and CI/CD pipelines
   - Air-gapped or regulated environments
   - Real-time evaluation within tight SLAs
   - Early-stage prototyping when LLM calls are expensive</p>
<ol>
<li>
<p><strong>Simpler semantics:</strong> Single <code>(input, output, expectedOutput?)</code> signature vs. full conversation context. Easier to integrate into test frameworks.</p>
</li>
<li>
<p><strong>Metric transparency:</strong> Heuristic-based metrics are debuggable (tokenization, overlap %, cosine similarity). LLM-based metrics are black boxes.</p>
</li>
</ol>
<p><strong>When to use which:</strong>
- Use <strong>Microsoft</strong> for: Nuanced quality judgment, conversation-aware evaluation, edge cases requiring LLM reasoning.
- Use <strong>ElBruno</strong> for: Fast iteration loops, cost control, local environments, regression testing.</p>
<h3 id="safety-evaluation">Safety Evaluation<a class="headerlink" href="#safety-evaluation" title="Permanent link">¶</a></h3>
<p><strong>Microsoft:</strong> Azure AI Foundry integration with specialized safety classifiers (hate, violence, sexual content, etc.).<br>
<strong>ElBruno:</strong> Local blocklist + regex-based PII detection (email, SSN, phone).</p>
<p><strong>Why ElBruno's version exists:</strong>
- Works offline (no Azure calls)
- Suitable for PII and basic content filtering
- Fast feedback in CI/CD</p>
<p><strong>When to use which:</strong>
- Use <strong>Microsoft</strong> for: Sophisticated safety analysis, compliance-grade reporting, diverse content categories.
- Use <strong>ElBruno</strong> for: Local PII checks, quick safety gates, cost-sensitive workflows.</p>
<hr>
<h2 id="unique-to-microsoft">Unique to Microsoft<a class="headerlink" href="#unique-to-microsoft" title="Permanent link">¶</a></h2>
<ol>
<li><strong>LLM-Based Quality Evaluators</strong> — Relevance, Completeness, Fluency, Coherence, Groundedness, Equivalence, RelevanceTruthAndCompleteness</li>
<li><strong>Agent-Focused Evaluators</strong> — IntentResolution, TaskAdherence, ToolCallAccuracy</li>
<li><strong>Traditional NLP Metrics</strong> — BLEU, GLEU, F1 scores (via Microsoft.Extensions.AI.Evaluation.NLP)</li>
<li><strong>Azure AI Foundry Safety Evaluation</strong> — Comprehensive safety classification (GroundednessPro, ProtectedMaterial, UngroundedAttributes, HateAndUnfairness, SelfHarm, Violence, Sexual, CodeVulnerability, IndirectAttack)</li>
<li><strong>Response Caching</strong> — Avoid redundant LLM calls during repeated evaluations</li>
<li><strong>Conversation-Aware Context</strong> — Full ChatMessage history in evaluator signatures</li>
<li><strong>HTML Report Generation</strong> — Professional visualization via <code>dotnet aieval</code> CLI</li>
<li><strong>Azure Storage Integration</strong> — Cloud-based persistence for enterprise scale</li>
<li><strong>dotnet aieval CLI Tool</strong> — Full-featured command-line for report management and data exploration</li>
</ol>
<hr>
<h2 id="unique-to-elbruno">Unique to ElBruno<a class="headerlink" href="#unique-to-elbruno" title="Permanent link">¶</a></h2>
<ol>
<li><strong>Synthetic Data Generation</strong> — SyntheticDatasetBuilder with multiple strategies:</li>
<li>Deterministic generators (templates, rules-based)</li>
<li>LLM generators (single-call, batch, creative)</li>
<li>Composite generators (blend multiple strategies)</li>
<li>
<p>Pre-built templates: QA, RAG, Adversarial, Domain-specific</p>
</li>
<li>
<p><strong>Golden Dataset Lifecycle Management</strong> — Full versioning and diffing:</p>
</li>
<li>Version tracking (semantic versioning)</li>
<li>DatasetVersion + DatasetDiff for identifying changes</li>
<li>Subsetting capabilities (GetByTag, GetSubset)</li>
<li>
<p>Loader/exporter support (JSON, CSV)</p>
</li>
<li>
<p><strong>Regression Detection</strong> — Automatic baseline comparison:</p>
</li>
<li>BaselineSnapshot stores per-metric baselines</li>
<li>RegressionDetector with configurable tolerance</li>
<li>RegressionReport identifies regressions vs. improvements</li>
<li>
<p>Easy conversion from EvaluationRun → BaselineSnapshot</p>
</li>
<li>
<p><strong>xUnit-Native Test Integration</strong> — Evaluation as unit tests:</p>
</li>
<li>AIEvaluationTest attribute</li>
<li>AIAssert methods (AssertPassed, AssertScore, AssertMetric, etc.)</li>
<li>AITestRunner for orchestration</li>
<li>
<p>Familiar test lifecycle for .NET developers</p>
</li>
<li>
<p><strong>Deterministic Evaluators</strong> — No external dependencies:</p>
</li>
<li>Hallucination (token overlap)</li>
<li>Factuality (sentence extraction + keyword matching)</li>
<li>Relevance (cosine similarity of term vectors)</li>
<li>Coherence (sentence completion, contradiction, repetition checks)</li>
<li>
<p>Safety (local blocklist + PII regex)</p>
</li>
<li>
<p><strong>SQLite Persistence</strong> — Lightweight, portable result storage:</p>
</li>
<li>SqliteResultStore for durability</li>
<li>Query-friendly for post-analysis</li>
<li>
<p>No cloud dependencies</p>
</li>
<li>
<p><strong>Flexible Export Formats</strong> — JsonExporter, CsvExporter for data portability</p>
</li>
</ol>
<hr>
<h2 id="gap-analysis-scenarios-not-covered-by-official-libraries">Gap Analysis: Scenarios NOT Covered by Official Libraries<a class="headerlink" href="#gap-analysis-scenarios-not-covered-by-official-libraries" title="Permanent link">¶</a></h2>
<h3 id="1-synthetic-test-data-generation-critical-gap">1. Synthetic Test Data Generation (Critical Gap)<a class="headerlink" href="#1-synthetic-test-data-generation-critical-gap" title="Permanent link">¶</a></h3>
<p><strong>Problem:</strong> Developers building LLM applications need realistic test data fast, but creating golden datasets manually is laborious. Microsoft's libraries assume you <em>already have</em> a golden dataset.</p>
<p><strong>Solution (ElBruno):</strong>
- SyntheticDatasetBuilder generates QA, RAG, adversarial, or domain-specific examples
- Composite strategy: blend deterministic rules + LLM generation
- Versioned, exportable datasets</p>
<p><strong>Actionable Example:</strong>
</p><div class="highlight"><pre><span></span><code><span class="kt">var</span><span class="w"> </span><span class="n">dataset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">SyntheticDatasetBuilder</span><span class="p">(</span><span class="s">"customer-support-qa"</span><span class="p">)</span>
<span class="w">    </span><span class="p">.</span><span class="n">WithTemplate</span><span class="p">(</span><span class="n">TemplateType</span><span class="p">.</span><span class="n">QA</span><span class="p">)</span>
<span class="w">    </span><span class="p">.</span><span class="n">WithLLMGenerator</span><span class="p">(</span><span class="n">chatClient</span><span class="p">,</span><span class="w"> </span><span class="mi">100</span><span class="p">)</span><span class="w">  </span><span class="c1">// Generate 100 examples</span>
<span class="w">    </span><span class="p">.</span><span class="n">Build</span><span class="p">();</span>
</code></pre></div><p></p>
<h3 id="2-offline-air-gapped-evaluation-critical-gap">2. Offline / Air-Gapped Evaluation (Critical Gap)<a class="headerlink" href="#2-offline-air-gapped-evaluation-critical-gap" title="Permanent link">¶</a></h3>
<p><strong>Problem:</strong> Developers in regulated industries, private clouds, or with tight budgets cannot rely on external LLM calls for evaluation. Microsoft's quality evaluators require LLM calls; safety evaluators require Azure.</p>
<p><strong>Solution (ElBruno):</strong>
- All evaluators work offline (heuristic-based)
- No external dependencies, fully local
- SQLite storage is self-contained</p>
<p><strong>Actionable Example:</strong> Use ElBruno in CI/CD pipelines without Azure credentials or network calls.</p>
<h3 id="3-golden-dataset-lifecycle-versioning-medium-gap">3. Golden Dataset Lifecycle &amp; Versioning (Medium Gap)<a class="headerlink" href="#3-golden-dataset-lifecycle-versioning-medium-gap" title="Permanent link">¶</a></h3>
<p><strong>Problem:</strong> Teams need to track dataset evolution (add examples, remove noisy ones, A/B test different versions). Microsoft provides no dataset versioning, diffing, or management tools.</p>
<p><strong>Solution (ElBruno):</strong>
- GoldenDataset with semantic versioning
- DatasetDiff to identify Added/Removed/Modified examples
- Subsetting by tags for targeted evaluation
- CSV/JSON import/export for data science workflows</p>
<p><strong>Actionable Example:</strong>
</p><div class="highlight"><pre><span></span><code><span class="kt">var</span><span class="w"> </span><span class="n">v1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">await</span><span class="w"> </span><span class="n">DatasetLoader</span><span class="p">.</span><span class="n">LoadAsync</span><span class="p">(</span><span class="s">"dataset-v1.0.0.json"</span><span class="p">);</span>
<span class="kt">var</span><span class="w"> </span><span class="n">v2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">await</span><span class="w"> </span><span class="n">DatasetLoader</span><span class="p">.</span><span class="n">LoadAsync</span><span class="p">(</span><span class="s">"dataset-v1.1.0.json"</span><span class="p">);</span>
<span class="kt">var</span><span class="w"> </span><span class="n">diff</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DatasetDiff</span><span class="p">.</span><span class="n">Diff</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span><span class="w"> </span><span class="n">v2</span><span class="p">);</span>
<span class="n">Console</span><span class="p">.</span><span class="n">WriteLine</span><span class="p">(</span><span class="s">$"Added: {diff.Added.Count}, Removed: {diff.Removed.Count}"</span><span class="p">);</span>
</code></pre></div><p></p>
<h3 id="4-regression-detection-in-cicd-medium-gap">4. Regression Detection in CI/CD (Medium Gap)<a class="headerlink" href="#4-regression-detection-in-cicd-medium-gap" title="Permanent link">¶</a></h3>
<p><strong>Problem:</strong> Teams want to automatically detect performance regressions across evaluation runs and fail CI if thresholds are crossed. Microsoft requires manual baseline management.</p>
<p><strong>Solution (ElBruno):</strong>
- RegressionDetector with configurable tolerance
- Automatic snapshot creation from EvaluationRun
- Per-metric regression reporting</p>
<p><strong>Actionable Example:</strong>
</p><div class="highlight"><pre><span></span><code><span class="kt">var</span><span class="w"> </span><span class="n">baseline</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">BaselineSnapshot</span><span class="p">.</span><span class="n">Load</span><span class="p">(</span><span class="s">"baseline-v1.0.0.json"</span><span class="p">);</span>
<span class="kt">var</span><span class="w"> </span><span class="n">run</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">await</span><span class="w"> </span><span class="n">pipeline</span><span class="p">.</span><span class="n">RunAsync</span><span class="p">();</span>
<span class="kt">var</span><span class="w"> </span><span class="n">report</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">RegressionDetector</span><span class="p">.</span><span class="n">Compare</span><span class="p">(</span><span class="n">baseline</span><span class="p">,</span><span class="w"> </span><span class="n">run</span><span class="p">,</span><span class="w"> </span><span class="n">tolerance</span><span class="p">:</span><span class="w"> </span><span class="mf">0.05</span><span class="p">);</span>

<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">report</span><span class="p">.</span><span class="n">HasRegressions</span><span class="p">)</span>
<span class="w">    </span><span class="k">throw</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="nf">Exception</span><span class="p">(</span><span class="s">$"Regression detected: {report.Regressed.Keys}"</span><span class="p">);</span>
</code></pre></div><p></p>
<h3 id="5-xunit-native-test-assertions-medium-gap">5. xUnit-Native Test Assertions (Medium Gap)<a class="headerlink" href="#5-xunit-native-test-assertions-medium-gap" title="Permanent link">¶</a></h3>
<p><strong>Problem:</strong> .NET teams want AI evaluation to feel like normal unit testing, not a separate reporting workflow. Microsoft provides no xUnit integration.</p>
<p><strong>Solution (ElBruno):</strong>
- AIEvaluationTest attribute
- AIAssert methods (AssertPassed, AssertScore, AssertMetric)
- Native xUnit/NUnit lifecycle</p>
<p><strong>Actionable Example:</strong>
</p><div class="highlight"><pre><span></span><code><span class="na">[AIEvaluationTest]</span>
<span class="k">public</span><span class="w"> </span><span class="k">async</span><span class="w"> </span><span class="n">Task</span><span class="w"> </span><span class="nf">RelevanceEvaluator_ShouldPassHighQualityResponse</span><span class="p">()</span>
<span class="p">{</span>
<span class="w">    </span><span class="kt">var</span><span class="w"> </span><span class="n">evaluator</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">RelevanceEvaluator</span><span class="p">();</span>
<span class="w">    </span><span class="kt">var</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">await</span><span class="w"> </span><span class="n">evaluator</span><span class="p">.</span><span class="n">EvaluateAsync</span><span class="p">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="n">expectedOutput</span><span class="p">);</span>

<span class="w">    </span><span class="n">AIAssert</span><span class="p">.</span><span class="n">AssertPassed</span><span class="p">(</span><span class="n">result</span><span class="p">);</span>
<span class="w">    </span><span class="n">AIAssert</span><span class="p">.</span><span class="n">AssertScore</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="w"> </span><span class="n">minimumScore</span><span class="p">:</span><span class="w"> </span><span class="mf">0.8</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div><p></p>
<h3 id="6-cost-token-tracking-across-runs-small-gap">6. Cost &amp; Token Tracking Across Runs (Small Gap)<a class="headerlink" href="#6-cost-token-tracking-across-runs-small-gap" title="Permanent link">¶</a></h3>
<p><strong>Problem:</strong> Teams need to aggregate and trend token usage and costs across multiple evaluation runs.</p>
<p><strong>Solution (ElBruno):</strong>
- EvaluationRun includes optional TotalTokens and EstimatedCost
- SqliteResultStore for time-series analysis
- Easy trend detection via SQL queries</p>
<p><strong>Actionable Example:</strong>
</p><div class="highlight"><pre><span></span><code><span class="kt">var</span><span class="w"> </span><span class="n">run1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">await</span><span class="w"> </span><span class="n">pipeline</span><span class="p">.</span><span class="n">RunAsync</span><span class="p">();</span>
<span class="kt">var</span><span class="w"> </span><span class="n">run2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">await</span><span class="w"> </span><span class="n">pipeline</span><span class="p">.</span><span class="n">RunAsync</span><span class="p">();</span>

<span class="n">Console</span><span class="p">.</span><span class="n">WriteLine</span><span class="p">(</span><span class="s">$"Run 1 Cost: ${run1.EstimatedCost:F2}"</span><span class="p">);</span>
<span class="n">Console</span><span class="p">.</span><span class="n">WriteLine</span><span class="p">(</span><span class="s">$"Run 2 Cost: ${run2.EstimatedCost:F2}"</span><span class="p">);</span>
</code></pre></div><p></p>
<h3 id="7-deterministic-debuggable-evaluation-small-gap">7. Deterministic, Debuggable Evaluation (Small Gap)<a class="headerlink" href="#7-deterministic-debuggable-evaluation-small-gap" title="Permanent link">¶</a></h3>
<p><strong>Problem:</strong> Developers want to understand <em>why</em> an evaluation failed. LLM-based evaluators are opaque black boxes.</p>
<p><strong>Solution (ElBruno):</strong>
- All evaluators use transparent heuristics (tokenization, overlap %, similarity)
- MetricScore objects expose intermediate calculations
- Details field explains reasoning</p>
<p><strong>Actionable Example:</strong>
</p><div class="highlight"><pre><span></span><code><span class="kt">var</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">await</span><span class="w"> </span><span class="n">evaluator</span><span class="p">.</span><span class="n">EvaluateAsync</span><span class="p">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">output</span><span class="p">);</span>
<span class="n">Console</span><span class="p">.</span><span class="n">WriteLine</span><span class="p">(</span><span class="s">$"Details: {result.Details}"</span><span class="p">);</span><span class="w">  </span><span class="c1">// Explains the heuristic used</span>
</code></pre></div><p></p>
<hr>
<h2 id="when-to-use-which">When to Use Which<a class="headerlink" href="#when-to-use-which" title="Permanent link">¶</a></h2>
<h3 id="use-microsoftextensionsaievaluation-if">Use Microsoft.Extensions.AI.Evaluation if:<a class="headerlink" href="#use-microsoftextensionsaievaluation-if" title="Permanent link">¶</a></h3>
<ul>
<li>✅ You need sophisticated quality judgment (Relevance, Completeness, Fluency, Groundedness)</li>
<li>✅ You're building agentic systems (need IntentResolution, TaskAdherence, ToolCallAccuracy)</li>
<li>✅ You require comprehensive safety evaluation (Azure AI Foundry classifiers)</li>
<li>✅ You have full conversation history and need context-aware analysis</li>
<li>✅ You can afford LLM calls (cost, latency, external dependency)</li>
<li>✅ You need professional HTML reports and dashboard visualizations</li>
<li>✅ You're operating at enterprise scale (Azure Storage integration)</li>
<li>✅ You want NLP metrics (BLEU, GLEU, F1)</li>
</ul>
<h3 id="use-elbrunoaievaluation-if">Use ElBruno.AI.Evaluation if:<a class="headerlink" href="#use-elbrunoaievaluation-if" title="Permanent link">¶</a></h3>
<ul>
<li>✅ You need fast, offline evaluation (no external LLM/Azure calls)</li>
<li>✅ You're in regulated or air-gapped environments</li>
<li>✅ You want to manage golden datasets with versioning and diffing</li>
<li>✅ You need regression detection in CI/CD pipelines</li>
<li>✅ You prefer xUnit-native test integration</li>
<li>✅ You're cost-conscious and want to avoid LLM call overhead</li>
<li>✅ You need synthetic data generation</li>
<li>✅ You want transparent, debuggable evaluators</li>
<li>✅ You value simplicity over sophistication (single I/O API)</li>
</ul>
<h3 id="decision-tree">Decision Tree<a class="headerlink" href="#decision-tree" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code>START
  ↓
Need LLM-powered quality judgment?
  YES → Use Microsoft (quality evaluators)
  NO  ↓
       Need agentic evaluators (IntentResolution, TaskAdherence)?
         YES → Use Microsoft
         NO  ↓
             Need offline/air-gapped evaluation?
               YES → Use ElBruno (+ Microsoft for non-safety evaluators)
               NO  ↓
                   Need synthetic data or golden dataset versioning?
                     YES → Use ElBruno
                     NO  ↓
                         Need regression detection in CI/CD?
                           YES → Use ElBruno
                           NO  → Can use either; prefer Microsoft for comprehensiveness
</code></pre></div>
<hr>
<h2 id="complementary-usage">Complementary Usage<a class="headerlink" href="#complementary-usage" title="Permanent link">¶</a></h2>
<p>The two toolkits <strong>are not mutually exclusive</strong>—they work well together:</p>
<h3 id="pattern-1-hybrid-evaluation-pipeline">Pattern 1: Hybrid Evaluation Pipeline<a class="headerlink" href="#pattern-1-hybrid-evaluation-pipeline" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1">// Step 1: Generate synthetic golden dataset with ElBruno</span>
<span class="kt">var</span><span class="w"> </span><span class="n">syntheticDataset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">SyntheticDatasetBuilder</span><span class="p">(</span><span class="s">"rag-qa"</span><span class="p">)</span>
<span class="w">    </span><span class="p">.</span><span class="n">WithLLMGenerator</span><span class="p">(</span><span class="n">chatClient</span><span class="p">,</span><span class="w"> </span><span class="mi">50</span><span class="p">)</span>
<span class="w">    </span><span class="p">.</span><span class="n">Build</span><span class="p">();</span>

<span class="c1">// Step 2: Run deterministic evaluators from ElBruno</span>
<span class="kt">var</span><span class="w"> </span><span class="n">evaluators</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">IEvaluator</span><span class="p">[]</span>
<span class="p">{</span>
<span class="w">    </span><span class="k">new</span><span class="w"> </span><span class="nf">HallucinationEvaluator</span><span class="p">(),</span>
<span class="w">    </span><span class="k">new</span><span class="w"> </span><span class="nf">FactualityEvaluator</span><span class="p">(),</span>
<span class="p">};</span>
<span class="kt">var</span><span class="w"> </span><span class="n">detectionResults</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">await</span><span class="w"> </span><span class="n">chatClient</span><span class="p">.</span><span class="n">EvaluateAsync</span><span class="p">(</span><span class="n">syntheticDataset</span><span class="p">,</span><span class="w"> </span><span class="n">evaluators</span><span class="p">);</span>

<span class="c1">// Step 3: For high-confidence failures, run Microsoft's LLM-based evaluators for nuance</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">detectionResults</span><span class="p">.</span><span class="n">AggregateScore</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">0.7</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">    </span><span class="kt">var</span><span class="w"> </span><span class="n">llmEvaluators</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="p">[]</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="k">new</span><span class="w"> </span><span class="nf">MicrosoftRelevanceEvaluator</span><span class="p">(),</span><span class="w">  </span><span class="c1">// LLM-powered</span>
<span class="w">        </span><span class="k">new</span><span class="w"> </span><span class="nf">MicrosoftCompleteness</span><span class="p">(),</span>
<span class="w">    </span><span class="p">};</span>
<span class="w">    </span><span class="kt">var</span><span class="w"> </span><span class="n">llmResults</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">await</span><span class="w"> </span><span class="n">microsoftPipeline</span><span class="p">.</span><span class="n">EvaluateAsync</span><span class="p">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">response</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div>
<p><strong>Benefit:</strong> Fast first pass with ElBruno, expensive second opinion from Microsoft only when needed.</p>
<h3 id="pattern-2-regression-testing-with-baseline-snapshots">Pattern 2: Regression Testing with Baseline Snapshots<a class="headerlink" href="#pattern-2-regression-testing-with-baseline-snapshots" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1">// Establish baseline with ElBruno deterministic evaluators</span>
<span class="kt">var</span><span class="w"> </span><span class="n">baseline</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">BaselineSnapshot</span><span class="p">.</span><span class="n">Create</span><span class="p">(</span>
<span class="w">    </span><span class="s">"latest-release"</span><span class="p">,</span>
<span class="w">    </span><span class="k">new</span><span class="w"> </span><span class="nf">HallucinationEvaluator</span><span class="p">(),</span>
<span class="w">    </span><span class="k">new</span><span class="w"> </span><span class="nf">FactualityEvaluator</span><span class="p">()</span>
<span class="p">);</span>
<span class="n">baseline</span><span class="p">.</span><span class="n">Save</span><span class="p">(</span><span class="s">"baseline-v1.0.0.json"</span><span class="p">);</span>

<span class="c1">// In CI/CD, detect regressions fast with ElBruno</span>
<span class="kt">var</span><span class="w"> </span><span class="n">newRun</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">await</span><span class="w"> </span><span class="n">evaluationPipeline</span><span class="p">.</span><span class="n">RunWithBaselineAsync</span><span class="p">();</span><span class="w">  </span><span class="c1">// RegressionReport</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">newRun</span><span class="p">.</span><span class="n">HasRegressions</span><span class="p">)</span>
<span class="w">    </span><span class="k">throw</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="nf">Exception</span><span class="p">(</span><span class="s">"Regression detected"</span><span class="p">);</span>

<span class="c1">// For high-impact changes, supplement with Microsoft's deeper analysis</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">newRun</span><span class="p">.</span><span class="n">AggregateScore</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">baseline</span><span class="p">.</span><span class="n">AggregateScore</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">0.95</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">    </span><span class="kt">var</span><span class="w"> </span><span class="n">microsoftReport</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">await</span><span class="w"> </span><span class="n">microsoftPipeline</span><span class="p">.</span><span class="n">EvaluateAsync</span><span class="p">(...);</span>
<span class="p">}</span>
</code></pre></div>
<h3 id="pattern-3-golden-dataset-microsoft-llm-evaluation">Pattern 3: Golden Dataset + Microsoft LLM Evaluation<a class="headerlink" href="#pattern-3-golden-dataset-microsoft-llm-evaluation" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1">// Build golden dataset once with ElBruno (versioned, portable)</span>
<span class="kt">var</span><span class="w"> </span><span class="n">goldenDataset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">SyntheticDatasetBuilder</span><span class="p">(</span><span class="s">"customer-support"</span><span class="p">)</span>
<span class="w">    </span><span class="p">.</span><span class="n">WithVersion</span><span class="p">(</span><span class="s">"1.2.0"</span><span class="p">)</span>
<span class="w">    </span><span class="p">.</span><span class="n">WithTags</span><span class="p">(</span><span class="s">"english"</span><span class="p">,</span><span class="w"> </span><span class="s">"high-quality"</span><span class="p">)</span>
<span class="w">    </span><span class="p">.</span><span class="n">Build</span><span class="p">();</span>
<span class="k">await</span><span class="w"> </span><span class="n">DatasetLoader</span><span class="p">.</span><span class="n">SaveAsync</span><span class="p">(</span><span class="n">goldenDataset</span><span class="p">,</span><span class="w"> </span><span class="s">"golden-dataset-v1.2.0.json"</span><span class="p">);</span>

<span class="c1">// Use with Microsoft's LLM evaluators repeatedly</span>
<span class="kt">var</span><span class="w"> </span><span class="n">microsoftEvaluators</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="p">[]</span>
<span class="p">{</span>
<span class="w">    </span><span class="k">new</span><span class="w"> </span><span class="nf">MicrosoftRelevanceEvaluator</span><span class="p">(</span><span class="n">config</span><span class="p">),</span>
<span class="w">    </span><span class="k">new</span><span class="w"> </span><span class="nf">MicrosoftCoherence</span><span class="p">(</span><span class="n">config</span><span class="p">),</span>
<span class="p">};</span>

<span class="c1">// Run evaluation as needed (Microsoft handles caching)</span>
<span class="kt">var</span><span class="w"> </span><span class="n">scenario1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">await</span><span class="w"> </span><span class="n">microsoftPipeline</span><span class="p">.</span><span class="n">EvaluateAsync</span><span class="p">(</span><span class="n">goldenDataset</span><span class="p">,</span><span class="w"> </span><span class="p">...);</span>
<span class="kt">var</span><span class="w"> </span><span class="n">scenario2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">await</span><span class="w"> </span><span class="n">microsoftPipeline</span><span class="p">.</span><span class="n">EvaluateAsync</span><span class="p">(</span><span class="n">goldenDataset</span><span class="p">,</span><span class="w"> </span><span class="p">...);</span>
</code></pre></div>
<p><strong>Benefit:</strong> Best of both worlds: ElBruno's dataset management, Microsoft's powerful evaluators.</p>
<h3 id="pattern-4-test-automation-enterprise-reporting">Pattern 4: Test Automation + Enterprise Reporting<a class="headerlink" href="#pattern-4-test-automation-enterprise-reporting" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1">// Step 1: Local testing with ElBruno xUnit integration</span>
<span class="na">[AIEvaluationTest]</span>
<span class="k">public</span><span class="w"> </span><span class="k">async</span><span class="w"> </span><span class="n">Task</span><span class="w"> </span><span class="nf">ValidateRAGRelevance</span><span class="p">()</span>
<span class="p">{</span>
<span class="w">    </span><span class="kt">var</span><span class="w"> </span><span class="n">evaluator</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">RelevanceEvaluator</span><span class="p">();</span>
<span class="w">    </span><span class="kt">var</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">await</span><span class="w"> </span><span class="n">evaluator</span><span class="p">.</span><span class="n">EvaluateAsync</span><span class="p">(</span><span class="n">query</span><span class="p">,</span><span class="w"> </span><span class="n">response</span><span class="p">,</span><span class="w"> </span><span class="n">expected</span><span class="p">);</span>
<span class="w">    </span><span class="n">AIAssert</span><span class="p">.</span><span class="n">AssertScore</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="w"> </span><span class="n">minimumScore</span><span class="p">:</span><span class="w"> </span><span class="mf">0.8</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// Step 2: Continuous evaluation with Microsoft for reports</span>
<span class="kt">var</span><span class="w"> </span><span class="n">microsoftRun</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">await</span><span class="w"> </span><span class="n">microsoftPipeline</span><span class="p">.</span><span class="n">EvaluateAsync</span><span class="p">(</span><span class="n">goldenDataset</span><span class="p">);</span>
<span class="c1">// Microsoft generates HTML report + uploads to Azure</span>
<span class="k">await</span><span class="w"> </span><span class="n">reporter</span><span class="p">.</span><span class="n">GenerateHtmlReportAsync</span><span class="p">(</span><span class="n">microsoftRun</span><span class="p">);</span>
</code></pre></div>
<hr>
<h2 id="summary-table">Summary Table<a class="headerlink" href="#summary-table" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Dimension</th>
<th>Microsoft</th>
<th>ElBruno</th>
<th>Recommendation</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>LLM-Powered Evaluation</strong></td>
<td>✅ Comprehensive</td>
<td>❌ Not included</td>
<td>Use Microsoft for quality judgment</td>
</tr>
<tr>
<td><strong>Offline Evaluation</strong></td>
<td>❌ Requires Azure</td>
<td>✅ Fully local</td>
<td>Use ElBruno for air-gapped environments</td>
</tr>
<tr>
<td><strong>Synthetic Data</strong></td>
<td>❌ Not provided</td>
<td>✅ Built-in</td>
<td>Use ElBruno for data generation</td>
</tr>
<tr>
<td><strong>Golden Dataset Versioning</strong></td>
<td>❌ Manual</td>
<td>✅ Automated</td>
<td>Use ElBruno for dataset lifecycle</td>
</tr>
<tr>
<td><strong>Regression Detection</strong></td>
<td>⚠️ Manual baseline</td>
<td>✅ Automated</td>
<td>Use ElBruno for CI/CD gates</td>
</tr>
<tr>
<td><strong>xUnit Integration</strong></td>
<td>❌ Not provided</td>
<td>✅ Native</td>
<td>Use ElBruno for unit tests</td>
</tr>
<tr>
<td><strong>Enterprise Reporting</strong></td>
<td>✅ HTML + Azure</td>
<td>❌ Not included</td>
<td>Use Microsoft for dashboards</td>
</tr>
<tr>
<td><strong>Response Caching</strong></td>
<td>✅ Built-in</td>
<td>❌ N/A</td>
<td>Use Microsoft for cost optimization</td>
</tr>
<tr>
<td><strong>Ease of Learning</strong></td>
<td>⚠️ Steeper (LLM concepts)</td>
<td>✅ Simpler (text heuristics)</td>
<td>Use ElBruno to get started quickly</td>
</tr>
<tr>
<td><strong>Ideal Workflow</strong></td>
<td>High-quality evals + reports</td>
<td>Fast iteration + baselines</td>
<td><strong>Use both in combination</strong></td>
</tr>
</tbody>
</table>
<hr>
<h2 id="conclusion">Conclusion<a class="headerlink" href="#conclusion" title="Permanent link">¶</a></h2>
<p><strong>Microsoft.Extensions.AI.Evaluation</strong> is the go-to choice for sophisticated, LLM-powered quality and safety evaluation at scale. Its strength lies in leveraging Azure AI Foundry and advanced language understanding.</p>
<p><strong>ElBruno.AI.Evaluation</strong> fills critical gaps in offline evaluation, synthetic data generation, golden dataset management, and test automation—scenarios not addressed by Microsoft's libraries. Its strength lies in simplicity, transparency, and local-first evaluation.</p>
<p><strong>Best Practice:</strong> Use both. Start with ElBruno for fast iteration, golden dataset versioning, and regression detection in CI/CD. Graduate to Microsoft when you need nuanced quality judgment, agentic evaluation, or enterprise safety analysis. The two toolkits complement each other seamlessly.</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer">
        
          
          <a href="../best-practices/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Best Practices">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Best Practices
              </div>
            </div>
          </a>
        
        
          
          <a href="../publishing/" class="md-footer__link md-footer__link--next" aria-label="Next: Publishing">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Publishing
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "..", "features": ["navigation.instant", "navigation.footer", "content.code.copy", "navigation.tabs.sticky", "content.tabs.link"], "search": "../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.79ae519e.min.js"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>